{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "yelp_model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JiaweiYu1/fasttext/blob/master/yelp_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ0MJoc3pWXz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import codecs\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "import csv\n",
        "import html\n",
        "\n",
        "from tensorflow.contrib import slim\n",
        "\n",
        "import argparse\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "import pprint\n",
        "import easydict\n",
        "import random\n",
        "\n",
        "\n",
        "#tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hzf2nE99pZTQ",
        "colab_type": "code",
        "outputId": "2fd17e9f-6310-4cba-a8cf-15dd591d65f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/Colab_Notebooks/fasttext\")\n",
        "cwd = os.getcwd()\n",
        "print(cwd)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "/content/drive/My Drive/Colab_Notebooks/fasttext\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzwfd1P0pDT-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class fastTextModel():\n",
        "    \"\"\"\n",
        "    A simple implementation of fasttext for text classification\n",
        "    \"\"\"\n",
        "    def __init__(self, sequence_length, num_classes, vocab_size,\n",
        "                 embedding_size, learning_rate, l2_reg_lambda, epoch, \n",
        "                 decay_steps, decay_rate, is_training=True,\n",
        "                 initializer=tf.random_normal_initializer(stddev=0.1)):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.num_classes = num_classes\n",
        "        self.sequence_length = sequence_length\n",
        "        self.learning_rate = learning_rate\n",
        "        self.decay_steps = decay_steps\n",
        "        self.decay_rate = decay_rate\n",
        "        self.epoch = epoch\n",
        "        self.is_training = is_training\n",
        "        self.l2_reg_lambda = l2_reg_lambda\n",
        "        self.initializer = initializer\n",
        " \n",
        "        self.input_x = tf.placeholder(tf.int32, [None, self.sequence_length], name='input_x')\n",
        "        self.input_y = tf.placeholder(tf.int32, [None, self.num_classes], name='input_y')\n",
        " \n",
        "        self.global_step = tf.Variable(0, trainable=False, name='global_step')\n",
        "        self.instantiate_weight()\n",
        "        self.logits = self.inference()\n",
        "        self.loss_val = self.loss()\n",
        "        self.train_op = self.train()\n",
        " \n",
        "        self.predictions = tf.argmax(self.logits, axis=1, name='predictions')\n",
        "        correct_prediction = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'), name='accuracy')\n",
        " \n",
        "    def instantiate_weight(self):\n",
        "        with tf.variable_scope('weights', reuse=tf.AUTO_REUSE):\n",
        "            self.Embedding = tf.get_variable('Embedding', shape=[self.vocab_size, self.embedding_size],\n",
        "                                             initializer=self.initializer)\n",
        "            self.W_projection = tf.get_variable('W_projection', shape=[self.embedding_size, self.num_classes],\n",
        "                                                initializer=self.initializer)\n",
        "            self.b_projection = tf.get_variable('b_projection', shape=[self.num_classes])\n",
        " \n",
        " \n",
        "    def inference(self):\n",
        "        with tf.name_scope('embedding'):\n",
        "            words_embedding = tf.nn.embedding_lookup(self.Embedding, self.input_x)\n",
        "            self.average_embedding = tf.reduce_mean(words_embedding, axis=1)\n",
        " \n",
        "        logits = tf.matmul(self.average_embedding, self.W_projection) +self.b_projection\n",
        " \n",
        "        return logits\n",
        " \n",
        " \n",
        "    def loss(self):\n",
        "        # loss\n",
        "        with tf.name_scope('loss'):\n",
        "            losses = tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.input_y, logits=self.logits)\n",
        "            data_loss = tf.reduce_mean(losses)\n",
        "            #l2_loss = tf.add_n([tf.nn.l2_loss(cand_var) for cand_var in tf.trainable_variables()\n",
        "            #                    if 'bias' not in cand_var.name]) * self.l2_reg_lambda\n",
        "            #data_loss += l2_loss * self.l2_reg_lambda\n",
        "            return data_loss\n",
        " \n",
        "    def train(self):\n",
        "        with tf.name_scope('train'):\n",
        "            learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step,\n",
        "                                                       self.decay_steps, self.decay_rate,\n",
        "                                                       staircase=True)\n",
        " \n",
        "            train_op = tf.contrib.layers.optimize_loss(self.loss_val, global_step=self.global_step,\n",
        "                                                      learning_rate=learning_rate, optimizer='Adam')\n",
        " \n",
        "        return train_op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1aWQqGNpLEv",
        "colab_type": "code",
        "outputId": "286459d4-5529-4c42-b48d-a61cc253704b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "\n",
        "class_size=2\n",
        "learning_rate=0.1\n",
        "batch_size=32\n",
        "decay_steps=1000\n",
        "decay_rate=0.9\n",
        "sequence_length=467\n",
        "vocab_size = 4939525\n",
        "embedding_dim = 10\n",
        "is_training=True\n",
        "dropout_keep_prob=1\n",
        "epoch = 5\n",
        "batch_size = 4096\n",
        "l2_reg_lambda = 0.01\n",
        "\n",
        "\n",
        "fasttext = fastTextModel(sequence_length,\n",
        "                      class_size,\n",
        "                      vocab_size,\n",
        "                      embedding_dim,\n",
        "                      learning_rate,\n",
        "                      l2_reg_lambda,\n",
        "                      epoch,\n",
        "                      decay_steps,\n",
        "                      decay_rate,\n",
        "                      is_training=True,\n",
        "                    )"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trXP-7j6pLBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_iter(x, y, batch_size=1, shuffle=True):\n",
        "  data_x = np.array(x)\n",
        "  data_y = np.array(y)\n",
        "  data_len = len(x)\n",
        "  num_batch = int((data_len-1)/batch_size)+1\n",
        "    \n",
        "  if shuffle:\n",
        "    shuffle_indices = np.random.permutation(np.arange(data_len))\n",
        "    x_shuffle = data_x[shuffle_indices]\n",
        "    y_shuffle= data_y[shuffle_indices]\n",
        "    \n",
        "  else:\n",
        "    x_shuffle=x\n",
        "    y_shuffle = y\n",
        "  for i in range(num_batch):\n",
        "    start_index = i*batch_size\n",
        "    end_index = min((i+1)*batch_size, data_len)\n",
        "    yield (x_shuffle[start_index:end_index], y_shuffle[start_index:end_index])\n",
        "    \n",
        "    \n",
        "    \n",
        "def fit(train_x, train_y, x_dev, y_dev, batch_size, epoch):\n",
        "  if not os.path.exists('yelp_review_polarity_csv/saves/fasttext'): os.makedirs('yelp_review_polarity_csv/saves/fasttext')\n",
        "  if not os.path.exists('yelp_review_polarity_csv/train_logs/fasttext'): os.makedirs('yelp_review_polarity_csv/train_logs/fasttext')\n",
        "\n",
        "  train_steps = 0\n",
        "  best_val_acc = 0\n",
        "    \n",
        "  tf.summary.scalar('val_loss', fasttext.loss_val)\n",
        "  tf.summary.scalar('val_accuracy', fasttext.accuracy)\n",
        "  merged = tf.summary.merge_all()\n",
        "    \n",
        "  sess = tf.Session()\n",
        "    \n",
        "  writer = tf.summary.FileWriter('yelp_review_polarity_csv/train_logs/fasttext', sess.graph)\n",
        "  saver = tf.train.Saver(max_to_keep=10)\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "  for i in range(epoch):\n",
        "    start_time = time.time()\n",
        "    batch_train = batch_iter(train_x, train_y, batch_size)\n",
        "    for batch_x, batch_y in batch_train:\n",
        "      train_steps +=1\n",
        "      feed_dict = {fasttext.input_x:batch_x, fasttext.input_y:batch_y}\n",
        "      _, train_loss, train_acc = sess.run([fasttext.train_op, fasttext.loss_val, fasttext.accuracy], feed_dict = feed_dict)\n",
        "        \n",
        "      if train_steps % 100 ==0:\n",
        "        feed_dict = {fasttext.input_x:x_dev, fasttext.input_y:y_dev}\n",
        "        val_loss, val_acc = sess.run([fasttext.loss_val, fasttext.accuracy],feed_dict=feed_dict)\n",
        "        \n",
        "        summary = sess.run(merged, feed_dict=feed_dict)\n",
        "        writer.add_summary(summary, global_step=train_steps)\n",
        "        \n",
        "        if val_acc >= best_val_acc:\n",
        "          best_val_acc = val_acc\n",
        "          saver.save(sess, \"yelp_review_polarity_csv/saves/fasttext/model.ckpt\", global_step=train_steps)\n",
        "        msg = 'epoch:%d/%d, train_steps:%d, train_loss:%.4f, trina_acc:%.4f, val_loss:%.4f, val_acc:%.4f'\n",
        "        print(msg%(i, epoch, train_steps, train_loss, train_acc, val_loss, val_acc))\n",
        "    print(\"--- %s seconds per epoch ---\" % (time.time() - start_time))   \n",
        "\n",
        "      \n",
        "def predict(x):\n",
        "  sess = tf.Session()\n",
        "    \n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  saver = tf.train.Saver(tf.global_variables())\n",
        "  ckpt = tf.train.get_checkpoint_state('yelp_review_polarity_csv/saves/fasttext/')\n",
        "  saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "  #saver.restore(sess, 'saves/fasttext/checkpoint')\n",
        "    \n",
        "  feed_dict={fasttext.input_x:x}\n",
        "  logits = sess.run(fasttext.logits, feed_dict=feed_dict)\n",
        "  y_pred = np.argmax(logits, 1)\n",
        "    \n",
        "  return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BBx9ZnYA7bUW",
        "colab": {}
      },
      "source": [
        "file_to_read = open('data/yelp_train_x_1.pkl', 'rb')\n",
        "\n",
        "yelp_train_x = pickle.load(file_to_read)\n",
        "\n",
        "file_to_read.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BValQTgy7bUZ",
        "colab": {}
      },
      "source": [
        "file_to_read = open('data/yelp_y_train_1.pkl', 'rb')\n",
        "\n",
        "yelp_y_train = pickle.load(file_to_read)\n",
        "\n",
        "file_to_read.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9xcwEV-aLPZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_to_read = open('data/yelp_test_x.pkl', 'rb')\n",
        "\n",
        "yelp_x_test = pickle.load(file_to_read)\n",
        "\n",
        "file_to_read.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUKOQyAUaLyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_to_read = open('data/yelp_test_y.pkl', 'rb')\n",
        "\n",
        "yelp_y_test = pickle.load(file_to_read)\n",
        "\n",
        "file_to_read.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xlmXn9ea7bUb",
        "colab": {}
      },
      "source": [
        "dev_sample_index = -1 * int(0.1 * float(len(yelp_y_train)))\n",
        "x_train_yelp, x_dev_yelp = yelp_train_x[:dev_sample_index], yelp_train_x[dev_sample_index:]\n",
        "yelp_y_train_1, yelp_y_dev_1 = yelp_y_train[:dev_sample_index], yelp_y_train[dev_sample_index:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "niWPFTRY7bUd",
        "colab": {}
      },
      "source": [
        "yelp_y_dev_2 = np.array(yelp_y_dev_1).astype(np.int32).tolist()\n",
        "yelp_y_train_2 = np.array(yelp_y_train_1).astype(np.int32).tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dug3j8MzR2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del yelp_train_x\n",
        "del yelp_y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "4d2260f6-abb2-4d6d-e2bd-18592c4249cc",
        "id": "nTmIlggm7bUg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "fit(x_train_yelp, yelp_y_train_2, x_dev_yelp, yelp_y_dev_2,batch_size, epoch)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:0/5, train_steps:100, train_loss:0.1333, trina_acc:0.9490, val_loss:0.1347, val_acc:0.9523\n",
            "--- 102.65831303596497 seconds per epoch ---\n",
            "epoch:1/5, train_steps:200, train_loss:0.0450, trina_acc:0.9841, val_loss:0.1430, val_acc:0.9504\n",
            "--- 96.42351388931274 seconds per epoch ---\n",
            "epoch:2/5, train_steps:300, train_loss:0.0137, trina_acc:0.9973, val_loss:0.1503, val_acc:0.9532\n",
            "--- 104.4518985748291 seconds per epoch ---\n",
            "epoch:3/5, train_steps:400, train_loss:0.0082, trina_acc:0.9980, val_loss:0.1704, val_acc:0.9516\n",
            "--- 97.58151984214783 seconds per epoch ---\n",
            "epoch:4/5, train_steps:500, train_loss:0.0031, trina_acc:1.0000, val_loss:0.1835, val_acc:0.9520\n",
            "epoch:4/5, train_steps:600, train_loss:0.0043, trina_acc:0.9990, val_loss:0.1967, val_acc:0.9510\n",
            "--- 102.35480213165283 seconds per epoch ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2GDE1ELe4tU",
        "colab_type": "code",
        "outputId": "80506f2d-f0c1-4378-8fb5-bb50c835c6a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "y_predict = predict(yelp_x_test)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from yelp_review_polarity_csv/saves/fasttext/model.ckpt-300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQVfaF1Re4qz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c_prediction = tf.equal(y_predict, yelp_y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D50PvXn4fWSk",
        "colab_type": "code",
        "outputId": "f3ffb7ca-3757-4370-aad5-728b348b49cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(yelp_y_test, y_predict)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[18141,   859],\n",
              "       [  942, 18058]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3kTUjICfXAf",
        "colab_type": "code",
        "outputId": "6b96d3d6-0473-4f87-d373-6a3145689679",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(yelp_y_test, y_predict)\n",
        "print('The test accuracy is', accuracy)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The test accuracy is 0.9526052631578947\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}