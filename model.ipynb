{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JiaweiYu1/fasttext/blob/master/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ0MJoc3pWXz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import codecs\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "import csv\n",
        "import html\n",
        "\n",
        "from tensorflow.contrib import slim\n",
        "\n",
        "import argparse\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "import pprint\n",
        "import easydict\n",
        "import random\n",
        "\n",
        "\n",
        "#tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hzf2nE99pZTQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "3db15f85-2ba9-45fc-b0c4-8cb8c775e310"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/Colab_Notebooks/fasttext\")\n",
        "cwd = os.getcwd()\n",
        "print(cwd)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "/content/drive/My Drive/Colab_Notebooks/fasttext\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzwfd1P0pDT-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class fastTextModel():\n",
        "    \"\"\"\n",
        "    A simple implementation of fasttext for text classification\n",
        "    \"\"\"\n",
        "    def __init__(self, sequence_length, num_classes, vocab_size,\n",
        "                 embedding_size, learning_rate, l2_reg_lambda, epoch, is_training=True,\n",
        "                 initializer=tf.random_normal_initializer(stddev=0.1)):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.num_classes = num_classes\n",
        "        self.sequence_length = sequence_length\n",
        "        self.learning_rate = learning_rate\n",
        "        #self.decay_steps = decay_steps\n",
        "        #self.decay_rate = decay_rate\n",
        "        self.epoch = epoch\n",
        "        self.is_training = is_training\n",
        "        self.l2_reg_lambda = l2_reg_lambda\n",
        "        self.initializer = initializer\n",
        " \n",
        "        self.input_x = tf.placeholder(tf.int32, [None, self.sequence_length], name='input_x')\n",
        "        self.input_y = tf.placeholder(tf.int32, [None, self.num_classes], name='input_y')\n",
        " \n",
        "        self.global_step = tf.Variable(0, trainable=False, name='global_step')\n",
        "        self.instantiate_weight()\n",
        "        self.logits = self.inference()\n",
        "        self.loss_val = self.loss()\n",
        "        self.train_op = self.train()\n",
        " \n",
        "        self.predictions = tf.argmax(self.logits, axis=1, name='predictions')\n",
        "        correct_prediction = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'), name='accuracy')\n",
        " \n",
        "    def instantiate_weight(self):\n",
        "        with tf.variable_scope('weights', reuse=tf.AUTO_REUSE):\n",
        "            self.Embedding = tf.get_variable('Embedding', shape=[self.vocab_size, self.embedding_size],\n",
        "                                             initializer=self.initializer)\n",
        "            self.W_projection = tf.get_variable('W_projection', shape=[self.embedding_size, self.num_classes],\n",
        "                                                initializer=self.initializer)\n",
        "            self.b_projection = tf.get_variable('b_projection', shape=[self.num_classes])\n",
        " \n",
        " \n",
        "    def inference(self):\n",
        "        with tf.name_scope('embedding'):\n",
        "            words_embedding = tf.nn.embedding_lookup(self.Embedding, self.input_x)\n",
        "            self.average_embedding = tf.reduce_mean(words_embedding, axis=1)\n",
        " \n",
        "        logits = tf.matmul(self.average_embedding, self.W_projection) +self.b_projection\n",
        " \n",
        "        return logits\n",
        " \n",
        " \n",
        "    def loss(self):\n",
        "        # loss\n",
        "        with tf.name_scope('loss'):\n",
        "            losses = tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.input_y, logits=self.logits)\n",
        "            data_loss = tf.reduce_mean(losses)\n",
        "            l2_loss = tf.add_n([tf.nn.l2_loss(cand_var) for cand_var in tf.trainable_variables()\n",
        "                                if 'bias' not in cand_var.name]) * self.l2_reg_lambda\n",
        "            data_loss += l2_loss * self.l2_reg_lambda\n",
        "            return data_loss\n",
        " \n",
        "    def train(self):\n",
        "        with tf.name_scope('train'):\n",
        "            #learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step,\n",
        "            #                                           self.decay_steps, self.decay_rate,\n",
        "            #                                           staircase=True)\n",
        " \n",
        "            train_op = tf.contrib.layers.optimize_loss(self.loss_val, global_step=self.global_step,\n",
        "                                                      learning_rate=learning_rate, optimizer='Adam')\n",
        " \n",
        "        return train_op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1aWQqGNpLEv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "9ad95531-7ba8-4ff6-8813-42289bab1389"
      },
      "source": [
        "\n",
        "class_size=4\n",
        "learning_rate=0.1\n",
        "batch_size=32\n",
        "#decay_steps=1000\n",
        "#decay_rate=0.9\n",
        "sequence_length=467\n",
        "vocab_size = 1439345\n",
        "embedding_dim = 10\n",
        "is_training=True\n",
        "dropout_keep_prob=1\n",
        "epoch = 30\n",
        "batch_size = 4096\n",
        "l2_reg_lambda = 0.01\n",
        "\n",
        "\n",
        "fasttext = fastTextModel(sequence_length,\n",
        "                      class_size,\n",
        "                      vocab_size,\n",
        "                      embedding_dim,\n",
        "                      learning_rate,\n",
        "                      l2_reg_lambda,\n",
        "                      epoch,\n",
        "                      is_training=True,\n",
        "                    )"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trXP-7j6pLBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_iter(x, y, batch_size=1, shuffle=True):\n",
        "  data_x = np.array(x)\n",
        "  data_y = np.array(y)\n",
        "  data_len = len(x)\n",
        "  num_batch = int((data_len-1)/batch_size)+1\n",
        "    \n",
        "  if shuffle:\n",
        "    shuffle_indices = np.random.permutation(np.arange(data_len))\n",
        "    x_shuffle = data_x[shuffle_indices]\n",
        "    y_shuffle= data_y[shuffle_indices]\n",
        "    \n",
        "  else:\n",
        "    x_shuffle=x\n",
        "    y_shuffle = y\n",
        "  for i in range(num_batch):\n",
        "    start_index = i*batch_size\n",
        "    end_index = min((i+1)*batch_size, data_len)\n",
        "    yield (x_shuffle[start_index:end_index], y_shuffle[start_index:end_index])\n",
        "    \n",
        "    \n",
        "    \n",
        "def fit(train_x, train_y, x_dev, y_dev, batch_size, epoch):\n",
        "  if not os.path.exists('saves/fasttext'): os.makedirs('saves/fasttext')\n",
        "  if not os.path.exists('train_logs/fasttext'): os.makedirs('train_logs/fasttext')\n",
        "\n",
        "  train_steps = 0\n",
        "  best_val_acc = 0\n",
        "    \n",
        "  tf.summary.scalar('val_loss', fasttext.loss_val)\n",
        "  tf.summary.scalar('val_accuracy', fasttext.accuracy)\n",
        "  merged = tf.summary.merge_all()\n",
        "    \n",
        "  sess = tf.Session()\n",
        "    \n",
        "  writer = tf.summary.FileWriter('train_logs/fasttext', sess.graph)\n",
        "  saver = tf.train.Saver(max_to_keep=10)\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "  for i in range(epoch):\n",
        "    start_time = time.time()\n",
        "    batch_train = batch_iter(train_x, train_y, batch_size)\n",
        "    for batch_x, batch_y in batch_train:\n",
        "      train_steps +=1\n",
        "      feed_dict = {fasttext.input_x:batch_x, fasttext.input_y:batch_y}\n",
        "      _, train_loss, train_acc = sess.run([fasttext.train_op, fasttext.loss_val, fasttext.accuracy], feed_dict = feed_dict)\n",
        "        \n",
        "      if train_steps % 10 ==0:\n",
        "        feed_dict = {fasttext.input_x:x_dev, fasttext.input_y:y_dev}\n",
        "        val_loss, val_acc = sess.run([fasttext.loss_val, fasttext.accuracy],feed_dict=feed_dict)\n",
        "        \n",
        "        summary = sess.run(merged, feed_dict=feed_dict)\n",
        "        writer.add_summary(summary, global_step=train_steps)\n",
        "        \n",
        "        if val_acc >= best_val_acc:\n",
        "          best_val_acc = val_acc\n",
        "          saver.save(sess, \"saves/fasttext/model.ckpt\", global_step=train_steps)\n",
        "        msg = 'epoch:%d/%d, train_steps:%d, train_loss:%.4f, trina_acc:%.4f, val_loss:%.4f, val_acc:%.4f'\n",
        "        print(msg%(i, epoch, train_steps, train_loss, train_acc, val_loss, val_acc))\n",
        "    \n",
        "    print(\"--- %s seconds per epoch ---\" % (time.time() - start_time))\n",
        "        \n",
        "\n",
        "      \n",
        "def predict(x):\n",
        "  sess = tf.Session()\n",
        "    \n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  saver = tf.train.Saver(tf.global_variables())\n",
        "  ckpt = tf.train.get_checkpoint_state('saves/fasttext/')\n",
        "  saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "  #saver.restore(sess, 'saves/fasttext/checkpoint')\n",
        "    \n",
        "  feed_dict={fasttext.input_x:x}\n",
        "  logits = sess.run(fasttext.logits, feed_dict=feed_dict)\n",
        "  y_pred = np.argmax(logits, 1)\n",
        "    \n",
        "  return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5QPrZHDp0A7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_to_read = open('data/train_x.pkl', 'rb')\n",
        "\n",
        "train_x = pickle.load(file_to_read)\n",
        "\n",
        "file_to_read.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oBpO4pzsdQF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_to_read = open('data/y_train.pkl', 'rb')\n",
        "\n",
        "y_train = pickle.load(file_to_read)\n",
        "\n",
        "file_to_read.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pmy4gPfOpK9X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_sample_index = -1 * int(0.1 * float(len(y_train)))\n",
        "x_train, x_dev = train_x[:dev_sample_index], train_x[dev_sample_index:]\n",
        "y_train_1, y_dev_1 = y_train[:dev_sample_index], y_train[dev_sample_index:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xV_TqZAypzVN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_dev_2 = np.array(y_dev_1).astype(np.int32).tolist()\n",
        "y_train_2 = np.array(y_train_1).astype(np.int32).tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFfYCZaipK5w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1975
        },
        "outputId": "3141aebe-5b2b-45f1-affc-67c986c23768"
      },
      "source": [
        "fit(x_train, y_train_2, x_dev, y_dev_2,batch_size, epoch)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:0/30, train_steps:10, train_loss:2.2766, trina_acc:0.3997, val_loss:2.1146, val_acc:0.3354\n",
            "epoch:0/30, train_steps:20, train_loss:1.6890, trina_acc:0.2522, val_loss:1.6378, val_acc:0.2476\n",
            "--- 12.802305698394775 seconds per epoch ---\n",
            "epoch:1/30, train_steps:30, train_loss:1.4789, trina_acc:0.3557, val_loss:1.4761, val_acc:0.3584\n",
            "epoch:1/30, train_steps:40, train_loss:1.3999, trina_acc:0.4163, val_loss:1.4044, val_acc:0.4193\n",
            "epoch:1/30, train_steps:50, train_loss:1.3509, trina_acc:0.4011, val_loss:1.3588, val_acc:0.4337\n",
            "--- 13.987267255783081 seconds per epoch ---\n",
            "epoch:2/30, train_steps:60, train_loss:1.3340, trina_acc:0.4868, val_loss:1.3250, val_acc:0.5075\n",
            "epoch:2/30, train_steps:70, train_loss:1.2976, trina_acc:0.4731, val_loss:1.3027, val_acc:0.5285\n",
            "epoch:2/30, train_steps:80, train_loss:1.2746, trina_acc:0.6069, val_loss:1.2767, val_acc:0.5982\n",
            "--- 13.517390012741089 seconds per epoch ---\n",
            "epoch:3/30, train_steps:90, train_loss:1.2457, trina_acc:0.6262, val_loss:1.2724, val_acc:0.5669\n",
            "epoch:3/30, train_steps:100, train_loss:1.2196, trina_acc:0.6831, val_loss:1.2584, val_acc:0.6168\n",
            "--- 11.275842189788818 seconds per epoch ---\n",
            "epoch:4/30, train_steps:110, train_loss:1.2327, trina_acc:0.6721, val_loss:1.2443, val_acc:0.6425\n",
            "epoch:4/30, train_steps:120, train_loss:1.1863, trina_acc:0.7283, val_loss:1.2101, val_acc:0.7082\n",
            "epoch:4/30, train_steps:130, train_loss:1.1763, trina_acc:0.7363, val_loss:1.2096, val_acc:0.6962\n",
            "--- 12.270179748535156 seconds per epoch ---\n",
            "epoch:5/30, train_steps:140, train_loss:1.2074, trina_acc:0.6899, val_loss:1.2982, val_acc:0.6228\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "epoch:5/30, train_steps:150, train_loss:1.1969, trina_acc:0.7146, val_loss:1.2020, val_acc:0.7412\n",
            "epoch:5/30, train_steps:160, train_loss:1.1637, trina_acc:0.7620, val_loss:1.1819, val_acc:0.7562\n",
            "--- 12.269407272338867 seconds per epoch ---\n",
            "epoch:6/30, train_steps:170, train_loss:1.1672, trina_acc:0.7659, val_loss:1.1810, val_acc:0.7678\n",
            "epoch:6/30, train_steps:180, train_loss:1.1422, trina_acc:0.7866, val_loss:1.1984, val_acc:0.7367\n",
            "--- 11.075849056243896 seconds per epoch ---\n",
            "epoch:7/30, train_steps:190, train_loss:1.1615, trina_acc:0.7837, val_loss:1.2129, val_acc:0.7659\n",
            "epoch:7/30, train_steps:200, train_loss:1.1676, trina_acc:0.7729, val_loss:1.1781, val_acc:0.7890\n",
            "epoch:7/30, train_steps:210, train_loss:1.1528, trina_acc:0.7939, val_loss:1.1856, val_acc:0.7768\n",
            "--- 11.497074127197266 seconds per epoch ---\n",
            "epoch:8/30, train_steps:220, train_loss:1.1674, trina_acc:0.7947, val_loss:1.1771, val_acc:0.7878\n",
            "epoch:8/30, train_steps:230, train_loss:1.1569, trina_acc:0.7903, val_loss:1.1837, val_acc:0.7723\n",
            "epoch:8/30, train_steps:240, train_loss:1.1543, trina_acc:0.8076, val_loss:1.1946, val_acc:0.7608\n",
            "--- 10.838073492050171 seconds per epoch ---\n",
            "epoch:9/30, train_steps:250, train_loss:1.2044, trina_acc:0.7632, val_loss:1.2421, val_acc:0.7292\n",
            "epoch:9/30, train_steps:260, train_loss:1.1518, trina_acc:0.8137, val_loss:1.2257, val_acc:0.7320\n",
            "epoch:9/30, train_steps:270, train_loss:1.1538, trina_acc:0.8045, val_loss:1.1915, val_acc:0.8012\n",
            "--- 11.134666204452515 seconds per epoch ---\n",
            "epoch:10/30, train_steps:280, train_loss:1.1493, trina_acc:0.8279, val_loss:1.1827, val_acc:0.7869\n",
            "epoch:10/30, train_steps:290, train_loss:1.1478, trina_acc:0.8108, val_loss:1.1857, val_acc:0.7903\n",
            "--- 10.395039796829224 seconds per epoch ---\n",
            "epoch:11/30, train_steps:300, train_loss:1.1726, trina_acc:0.7944, val_loss:1.2010, val_acc:0.7904\n",
            "epoch:11/30, train_steps:310, train_loss:1.1457, trina_acc:0.8237, val_loss:1.1734, val_acc:0.8121\n",
            "epoch:11/30, train_steps:320, train_loss:1.1562, trina_acc:0.8159, val_loss:1.1695, val_acc:0.8130\n",
            "--- 11.802439451217651 seconds per epoch ---\n",
            "epoch:12/30, train_steps:330, train_loss:1.1305, trina_acc:0.8352, val_loss:1.1786, val_acc:0.8058\n",
            "epoch:12/30, train_steps:340, train_loss:1.1691, trina_acc:0.8066, val_loss:1.2038, val_acc:0.7769\n",
            "epoch:12/30, train_steps:350, train_loss:1.1651, trina_acc:0.7866, val_loss:1.1817, val_acc:0.8026\n",
            "--- 11.022351741790771 seconds per epoch ---\n",
            "epoch:13/30, train_steps:360, train_loss:1.1399, trina_acc:0.8340, val_loss:1.1914, val_acc:0.7947\n",
            "epoch:13/30, train_steps:370, train_loss:1.1530, trina_acc:0.8306, val_loss:1.2224, val_acc:0.7513\n",
            "--- 10.003244161605835 seconds per epoch ---\n",
            "epoch:14/30, train_steps:380, train_loss:1.1642, trina_acc:0.8267, val_loss:1.2061, val_acc:0.8088\n",
            "epoch:14/30, train_steps:390, train_loss:1.1368, trina_acc:0.8291, val_loss:1.1703, val_acc:0.8214\n",
            "epoch:14/30, train_steps:400, train_loss:1.1502, trina_acc:0.8254, val_loss:1.1702, val_acc:0.8138\n",
            "--- 11.216931581497192 seconds per epoch ---\n",
            "epoch:15/30, train_steps:410, train_loss:1.1505, trina_acc:0.8218, val_loss:1.1862, val_acc:0.8005\n",
            "epoch:15/30, train_steps:420, train_loss:1.1626, trina_acc:0.8081, val_loss:1.1714, val_acc:0.8184\n",
            "epoch:15/30, train_steps:430, train_loss:1.1384, trina_acc:0.8301, val_loss:1.1836, val_acc:0.8038\n",
            "--- 10.807466506958008 seconds per epoch ---\n",
            "epoch:16/30, train_steps:440, train_loss:1.1444, trina_acc:0.8350, val_loss:1.1832, val_acc:0.7962\n",
            "epoch:16/30, train_steps:450, train_loss:1.1513, trina_acc:0.8201, val_loss:1.1791, val_acc:0.8127\n",
            "--- 10.028529644012451 seconds per epoch ---\n",
            "epoch:17/30, train_steps:460, train_loss:1.1734, trina_acc:0.8186, val_loss:1.2082, val_acc:0.8054\n",
            "epoch:17/30, train_steps:470, train_loss:1.1724, trina_acc:0.8157, val_loss:1.1942, val_acc:0.7923\n",
            "epoch:17/30, train_steps:480, train_loss:1.1448, trina_acc:0.8296, val_loss:1.1905, val_acc:0.7893\n",
            "--- 10.678374290466309 seconds per epoch ---\n",
            "epoch:18/30, train_steps:490, train_loss:1.1500, trina_acc:0.8357, val_loss:1.1938, val_acc:0.7894\n",
            "epoch:18/30, train_steps:500, train_loss:1.1449, trina_acc:0.8252, val_loss:1.1764, val_acc:0.8031\n",
            "epoch:18/30, train_steps:510, train_loss:1.1336, trina_acc:0.8323, val_loss:1.1824, val_acc:0.7997\n",
            "--- 10.593968629837036 seconds per epoch ---\n",
            "epoch:19/30, train_steps:520, train_loss:1.1417, trina_acc:0.8381, val_loss:1.1851, val_acc:0.7997\n",
            "epoch:19/30, train_steps:530, train_loss:1.1491, trina_acc:0.8191, val_loss:1.1731, val_acc:0.8068\n",
            "epoch:19/30, train_steps:540, train_loss:1.1302, trina_acc:0.8391, val_loss:1.1920, val_acc:0.8053\n",
            "--- 10.624638080596924 seconds per epoch ---\n",
            "epoch:20/30, train_steps:550, train_loss:1.1440, trina_acc:0.8347, val_loss:1.1797, val_acc:0.8043\n",
            "epoch:20/30, train_steps:560, train_loss:1.1362, trina_acc:0.8271, val_loss:1.1773, val_acc:0.8018\n",
            "--- 10.058294773101807 seconds per epoch ---\n",
            "epoch:21/30, train_steps:570, train_loss:1.1544, trina_acc:0.8364, val_loss:1.1869, val_acc:0.8074\n",
            "epoch:21/30, train_steps:580, train_loss:1.1343, trina_acc:0.8342, val_loss:1.1805, val_acc:0.7967\n",
            "epoch:21/30, train_steps:590, train_loss:1.1521, trina_acc:0.8159, val_loss:1.1701, val_acc:0.8152\n",
            "--- 10.680254459381104 seconds per epoch ---\n",
            "epoch:22/30, train_steps:600, train_loss:1.1335, trina_acc:0.8359, val_loss:1.1854, val_acc:0.8041\n",
            "epoch:22/30, train_steps:610, train_loss:1.1200, trina_acc:0.8469, val_loss:1.1630, val_acc:0.8217\n",
            "epoch:22/30, train_steps:620, train_loss:1.1365, trina_acc:0.8318, val_loss:1.1609, val_acc:0.8270\n",
            "--- 11.737980127334595 seconds per epoch ---\n",
            "epoch:23/30, train_steps:630, train_loss:1.1336, trina_acc:0.8394, val_loss:1.1749, val_acc:0.8069\n",
            "epoch:23/30, train_steps:640, train_loss:1.1364, trina_acc:0.8328, val_loss:1.1649, val_acc:0.8148\n",
            "--- 10.406413555145264 seconds per epoch ---\n",
            "epoch:24/30, train_steps:650, train_loss:1.1713, trina_acc:0.8210, val_loss:1.1983, val_acc:0.8067\n",
            "epoch:24/30, train_steps:660, train_loss:1.1391, trina_acc:0.8301, val_loss:1.1664, val_acc:0.8189\n",
            "epoch:24/30, train_steps:670, train_loss:1.1448, trina_acc:0.8125, val_loss:1.1803, val_acc:0.8010\n",
            "--- 10.5931077003479 seconds per epoch ---\n",
            "epoch:25/30, train_steps:680, train_loss:1.1376, trina_acc:0.8293, val_loss:1.1919, val_acc:0.8037\n",
            "epoch:25/30, train_steps:690, train_loss:1.1395, trina_acc:0.8298, val_loss:1.1875, val_acc:0.7930\n",
            "epoch:25/30, train_steps:700, train_loss:1.1320, trina_acc:0.8372, val_loss:1.1749, val_acc:0.8052\n",
            "--- 10.662586688995361 seconds per epoch ---\n",
            "epoch:26/30, train_steps:710, train_loss:1.1429, trina_acc:0.8225, val_loss:1.2128, val_acc:0.7733\n",
            "epoch:26/30, train_steps:720, train_loss:1.1288, trina_acc:0.8271, val_loss:1.1766, val_acc:0.7955\n",
            "--- 9.988904237747192 seconds per epoch ---\n",
            "epoch:27/30, train_steps:730, train_loss:1.1385, trina_acc:0.8389, val_loss:1.1956, val_acc:0.8027\n",
            "epoch:27/30, train_steps:740, train_loss:1.1344, trina_acc:0.8257, val_loss:1.1659, val_acc:0.8138\n",
            "epoch:27/30, train_steps:750, train_loss:1.1338, trina_acc:0.8335, val_loss:1.1713, val_acc:0.7962\n",
            "--- 10.600111246109009 seconds per epoch ---\n",
            "epoch:28/30, train_steps:760, train_loss:1.1350, trina_acc:0.8367, val_loss:1.1717, val_acc:0.8117\n",
            "epoch:28/30, train_steps:770, train_loss:1.1468, trina_acc:0.8201, val_loss:1.1604, val_acc:0.8230\n",
            "epoch:28/30, train_steps:780, train_loss:1.1280, trina_acc:0.8330, val_loss:1.1771, val_acc:0.8050\n",
            "--- 10.656855583190918 seconds per epoch ---\n",
            "epoch:29/30, train_steps:790, train_loss:1.1391, trina_acc:0.8359, val_loss:1.1841, val_acc:0.8037\n",
            "epoch:29/30, train_steps:800, train_loss:1.1386, trina_acc:0.8306, val_loss:1.1792, val_acc:0.8052\n",
            "epoch:29/30, train_steps:810, train_loss:1.1253, trina_acc:0.8424, val_loss:1.1811, val_acc:0.8199\n",
            "--- 10.60330080986023 seconds per epoch ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh43tN04v6Js",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_to_read = open('data/test_x.pkl', 'rb')\n",
        "\n",
        "test_x = pickle.load(file_to_read)\n",
        "\n",
        "file_to_read.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDqYCa4LwzKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_to_read = open('data/test_y.pkl', 'rb')\n",
        "\n",
        "test_y = pickle.load(file_to_read)\n",
        "\n",
        "file_to_read.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlY7h3zHv5Yj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e85bdf1a-17bd-415a-8bcd-767cf3f5eef7"
      },
      "source": [
        "y_predict = predict(test_x)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from saves/fasttext/model.ckpt-620\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLKlvnwF7XiH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c_prediction = tf.equal(y_predict, test_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXsdGomlITh-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "6a37c3fa-e42b-406c-96b0-11533bb63979"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(test_y, y_predict)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1595,   91,  107,  107],\n",
              "       [  76, 1717,   45,   62],\n",
              "       [ 103,   64, 1466,  267],\n",
              "       [  96,   68,  191, 1545]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK2V6lEaI2_E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cab814cf-979a-4c0e-8f96-5ab906dffb3d"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(test_y, y_predict)\n",
        "print('The test accuracy is', accuracy)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The test accuracy is 0.8319736842105263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-XvA4R37XfW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}