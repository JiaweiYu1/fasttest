{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JiaweiYu1/fasttext/blob/master/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ0MJoc3pWXz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import codecs\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "import csv\n",
        "import html\n",
        "\n",
        "from tensorflow.contrib import slim\n",
        "\n",
        "import argparse\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "import pprint\n",
        "import easydict\n",
        "import random\n",
        "\n",
        "\n",
        "#tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hzf2nE99pZTQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "outputId": "52b98fe2-df2a-4b2c-90b2-7bc8d9107b69"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/Colab_Notebooks/fasttext\")\n",
        "cwd = os.getcwd()\n",
        "print(cwd)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n",
            "/content/drive/My Drive/Colab_Notebooks/fasttext\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzwfd1P0pDT-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class fastTextModel():\n",
        "    \"\"\"\n",
        "    A simple implementation of fasttext for text classification\n",
        "    \"\"\"\n",
        "    def __init__(self, sequence_length, num_classes, vocab_size,\n",
        "                 embedding_size, learning_rate, l2_reg_lambda, epoch, is_training=True,\n",
        "                 initializer=tf.random_normal_initializer(stddev=0.1)):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.num_classes = num_classes\n",
        "        self.sequence_length = sequence_length\n",
        "        self.learning_rate = learning_rate\n",
        "        #self.decay_steps = decay_steps\n",
        "        #self.decay_rate = decay_rate\n",
        "        self.epoch = epoch\n",
        "        self.is_training = is_training\n",
        "        self.l2_reg_lambda = l2_reg_lambda\n",
        "        self.initializer = initializer\n",
        " \n",
        "        self.input_x = tf.placeholder(tf.int32, [None, self.sequence_length], name='input_x')\n",
        "        self.input_y = tf.placeholder(tf.int32, [None, self.num_classes], name='input_y')\n",
        " \n",
        "        self.global_step = tf.Variable(0, trainable=False, name='global_step')\n",
        "        self.instantiate_weight()\n",
        "        self.logits = self.inference()\n",
        "        self.loss_val = self.loss()\n",
        "        self.train_op = self.train()\n",
        " \n",
        "        self.predictions = tf.argmax(self.logits, axis=1, name='predictions')\n",
        "        correct_prediction = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'), name='accuracy')\n",
        " \n",
        "    def instantiate_weight(self):\n",
        "        with tf.variable_scope('weights', reuse=tf.AUTO_REUSE):\n",
        "            self.Embedding = tf.get_variable('Embedding', shape=[self.vocab_size, self.embedding_size],\n",
        "                                             initializer=self.initializer)\n",
        "            self.W_projection = tf.get_variable('W_projection', shape=[self.embedding_size, self.num_classes],\n",
        "                                                initializer=self.initializer)\n",
        "            self.b_projection = tf.get_variable('b_projection', shape=[self.num_classes])\n",
        " \n",
        " \n",
        "    def inference(self):\n",
        "        with tf.name_scope('embedding'):\n",
        "            words_embedding = tf.nn.embedding_lookup(self.Embedding, self.input_x)\n",
        "            self.average_embedding = tf.reduce_mean(words_embedding, axis=1)\n",
        " \n",
        "        logits = tf.matmul(self.average_embedding, self.W_projection) +self.b_projection\n",
        " \n",
        "        return logits\n",
        " \n",
        " \n",
        "    def loss(self):\n",
        "        # loss\n",
        "        with tf.name_scope('loss'):\n",
        "            losses = tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.input_y, logits=self.logits)\n",
        "            data_loss = tf.reduce_mean(losses)\n",
        "            #l2_loss = tf.add_n([tf.nn.l2_loss(cand_var) for cand_var in tf.trainable_variables()\n",
        "            #                    if 'bias' not in cand_var.name]) * self.l2_reg_lambda\n",
        "            #data_loss += l2_loss * self.l2_reg_lambda\n",
        "            return data_loss\n",
        " \n",
        "    def train(self):\n",
        "        with tf.name_scope('train'):\n",
        "            #learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step,\n",
        "            #                                           self.decay_steps, self.decay_rate,\n",
        "            #                                           staircase=True)\n",
        " \n",
        "            train_op = tf.contrib.layers.optimize_loss(self.loss_val, global_step=self.global_step,\n",
        "                                                      learning_rate=learning_rate, optimizer='Adam')\n",
        " \n",
        "        return train_op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1aWQqGNpLEv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "1970cfa0-73f9-4885-f7c0-6294448f15a9"
      },
      "source": [
        "\n",
        "class_size=4\n",
        "learning_rate=0.1\n",
        "batch_size=32\n",
        "#decay_steps=1000\n",
        "#decay_rate=0.9\n",
        "sequence_length=467\n",
        "vocab_size = 1439345\n",
        "embedding_dim = 10\n",
        "is_training=True\n",
        "dropout_keep_prob=1\n",
        "epoch = 30\n",
        "batch_size = 4096\n",
        "l2_reg_lambda = 0.01\n",
        "\n",
        "\n",
        "fasttext = fastTextModel(sequence_length,\n",
        "                      class_size,\n",
        "                      vocab_size,\n",
        "                      embedding_dim,\n",
        "                      learning_rate,\n",
        "                      l2_reg_lambda,\n",
        "                      epoch,\n",
        "                      is_training=True,\n",
        "                    )"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trXP-7j6pLBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_iter(x, y, batch_size=1, shuffle=True):\n",
        "  data_x = np.array(x)\n",
        "  data_y = np.array(y)\n",
        "  data_len = len(x)\n",
        "  num_batch = int((data_len-1)/batch_size)+1\n",
        "    \n",
        "  if shuffle:\n",
        "    shuffle_indices = np.random.permutation(np.arange(data_len))\n",
        "    x_shuffle = data_x[shuffle_indices]\n",
        "    y_shuffle= data_y[shuffle_indices]\n",
        "    \n",
        "  else:\n",
        "    x_shuffle=x\n",
        "    y_shuffle = y\n",
        "  for i in range(num_batch):\n",
        "    start_index = i*batch_size\n",
        "    end_index = min((i+1)*batch_size, data_len)\n",
        "    yield (x_shuffle[start_index:end_index], y_shuffle[start_index:end_index])\n",
        "    \n",
        "    \n",
        "    \n",
        "def fit(train_x, train_y, x_dev, y_dev, batch_size, epoch):\n",
        "  if not os.path.exists('saves/fasttext'): os.makedirs('saves/fasttext')\n",
        "  if not os.path.exists('train_logs/fasttext'): os.makedirs('train_logs/fasttext')\n",
        "\n",
        "  train_steps = 0\n",
        "  best_val_acc = 0\n",
        "    \n",
        "  tf.summary.scalar('val_loss', fasttext.loss_val)\n",
        "  tf.summary.scalar('val_accuracy', fasttext.accuracy)\n",
        "  merged = tf.summary.merge_all()\n",
        "    \n",
        "  sess = tf.Session()\n",
        "    \n",
        "  writer = tf.summary.FileWriter('train_logs/fasttext', sess.graph)\n",
        "  saver = tf.train.Saver(max_to_keep=10)\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "  for i in range(epoch):\n",
        "    start_time = time.time()\n",
        "    batch_train = batch_iter(train_x, train_y, batch_size)\n",
        "    for batch_x, batch_y in batch_train:\n",
        "      train_steps +=1\n",
        "      feed_dict = {fasttext.input_x:batch_x, fasttext.input_y:batch_y}\n",
        "      _, train_loss, train_acc = sess.run([fasttext.train_op, fasttext.loss_val, fasttext.accuracy], feed_dict = feed_dict)\n",
        "        \n",
        "      if train_steps % 10 ==0:\n",
        "        feed_dict = {fasttext.input_x:x_dev, fasttext.input_y:y_dev}\n",
        "        val_loss, val_acc = sess.run([fasttext.loss_val, fasttext.accuracy],feed_dict=feed_dict)\n",
        "        \n",
        "        summary = sess.run(merged, feed_dict=feed_dict)\n",
        "        writer.add_summary(summary, global_step=train_steps)\n",
        "        \n",
        "        if val_acc >= best_val_acc:\n",
        "          best_val_acc = val_acc\n",
        "          saver.save(sess, \"saves/fasttext/model.ckpt\", global_step=train_steps)\n",
        "        msg = 'epoch:%d/%d, train_steps:%d, train_loss:%.4f, trina_acc:%.4f, val_loss:%.4f, val_acc:%.4f'\n",
        "        print(msg%(i, epoch, train_steps, train_loss, train_acc, val_loss, val_acc))\n",
        "    \n",
        "    print(\"--- %s seconds per epoch ---\" % (time.time() - start_time))\n",
        "        \n",
        "\n",
        "      \n",
        "def predict(x):\n",
        "  sess = tf.Session()\n",
        "    \n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  saver = tf.train.Saver(tf.global_variables())\n",
        "  ckpt = tf.train.get_checkpoint_state('saves/fasttext/')\n",
        "  saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "  #saver.restore(sess, 'saves/fasttext/checkpoint')\n",
        "    \n",
        "  feed_dict={fasttext.input_x:x}\n",
        "  logits = sess.run(fasttext.logits, feed_dict=feed_dict)\n",
        "  y_pred = np.argmax(logits, 1)\n",
        "    \n",
        "  return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5QPrZHDp0A7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_to_read = open('data/train_x.pkl', 'rb')\n",
        "\n",
        "train_x = pickle.load(file_to_read)\n",
        "\n",
        "file_to_read.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oBpO4pzsdQF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_to_read = open('data/y_train.pkl', 'rb')\n",
        "\n",
        "y_train = pickle.load(file_to_read)\n",
        "\n",
        "file_to_read.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pmy4gPfOpK9X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_sample_index = -1 * int(0.1 * float(len(y_train)))\n",
        "x_train, x_dev = train_x[:dev_sample_index], train_x[dev_sample_index:]\n",
        "y_train_1, y_dev_1 = y_train[:dev_sample_index], y_train[dev_sample_index:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xV_TqZAypzVN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_dev_2 = np.array(y_dev_1).astype(np.int32).tolist()\n",
        "y_train_2 = np.array(y_train_1).astype(np.int32).tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFfYCZaipK5w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1975
        },
        "outputId": "cf8331fb-c2da-47dc-ab24-8d1c2d5ee0c9"
      },
      "source": [
        "fit(x_train, y_train_2, x_dev, y_dev_2,batch_size, epoch)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:0/30, train_steps:10, train_loss:1.2861, trina_acc:0.7051, val_loss:1.2719, val_acc:0.5834\n",
            "epoch:0/30, train_steps:20, train_loss:0.8310, trina_acc:0.8733, val_loss:0.8109, val_acc:0.8509\n",
            "--- 10.950680017471313 seconds per epoch ---\n",
            "epoch:1/30, train_steps:30, train_loss:0.3627, trina_acc:0.9155, val_loss:0.4462, val_acc:0.8712\n",
            "epoch:1/30, train_steps:40, train_loss:0.2455, trina_acc:0.9272, val_loss:0.3211, val_acc:0.8977\n",
            "epoch:1/30, train_steps:50, train_loss:0.2158, trina_acc:0.9343, val_loss:0.2811, val_acc:0.9070\n",
            "--- 11.84830117225647 seconds per epoch ---\n",
            "epoch:2/30, train_steps:60, train_loss:0.1086, trina_acc:0.9688, val_loss:0.2667, val_acc:0.9114\n",
            "epoch:2/30, train_steps:70, train_loss:0.0933, trina_acc:0.9724, val_loss:0.2585, val_acc:0.9124\n",
            "epoch:2/30, train_steps:80, train_loss:0.0801, trina_acc:0.9780, val_loss:0.2483, val_acc:0.9172\n",
            "--- 11.604283571243286 seconds per epoch ---\n",
            "epoch:3/30, train_steps:90, train_loss:0.0409, trina_acc:0.9922, val_loss:0.2513, val_acc:0.9156\n",
            "epoch:3/30, train_steps:100, train_loss:0.0406, trina_acc:0.9893, val_loss:0.2454, val_acc:0.9166\n",
            "--- 7.851010799407959 seconds per epoch ---\n",
            "epoch:4/30, train_steps:110, train_loss:0.0268, trina_acc:0.9956, val_loss:0.2448, val_acc:0.9172\n",
            "epoch:4/30, train_steps:120, train_loss:0.0234, trina_acc:0.9956, val_loss:0.2507, val_acc:0.9162\n",
            "epoch:4/30, train_steps:130, train_loss:0.0217, trina_acc:0.9939, val_loss:0.2506, val_acc:0.9175\n",
            "--- 9.185529470443726 seconds per epoch ---\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "epoch:5/30, train_steps:140, train_loss:0.0143, trina_acc:0.9973, val_loss:0.2488, val_acc:0.9178\n",
            "epoch:5/30, train_steps:150, train_loss:0.0124, trina_acc:0.9980, val_loss:0.2478, val_acc:0.9179\n",
            "epoch:5/30, train_steps:160, train_loss:0.0131, trina_acc:0.9973, val_loss:0.2542, val_acc:0.9164\n",
            "--- 10.448561906814575 seconds per epoch ---\n",
            "epoch:6/30, train_steps:170, train_loss:0.0094, trina_acc:0.9990, val_loss:0.2543, val_acc:0.9162\n",
            "epoch:6/30, train_steps:180, train_loss:0.0098, trina_acc:0.9980, val_loss:0.2559, val_acc:0.9163\n",
            "--- 7.654935359954834 seconds per epoch ---\n",
            "epoch:7/30, train_steps:190, train_loss:0.0059, trina_acc:1.0000, val_loss:0.2537, val_acc:0.9176\n",
            "epoch:7/30, train_steps:200, train_loss:0.0075, trina_acc:0.9985, val_loss:0.2566, val_acc:0.9168\n",
            "epoch:7/30, train_steps:210, train_loss:0.0079, trina_acc:0.9978, val_loss:0.2563, val_acc:0.9166\n",
            "--- 7.721396207809448 seconds per epoch ---\n",
            "epoch:8/30, train_steps:220, train_loss:0.0057, trina_acc:0.9995, val_loss:0.2576, val_acc:0.9166\n",
            "epoch:8/30, train_steps:230, train_loss:0.0056, trina_acc:0.9993, val_loss:0.2581, val_acc:0.9172\n",
            "epoch:8/30, train_steps:240, train_loss:0.0056, trina_acc:0.9990, val_loss:0.2595, val_acc:0.9172\n",
            "--- 7.679643869400024 seconds per epoch ---\n",
            "epoch:9/30, train_steps:250, train_loss:0.0039, trina_acc:0.9998, val_loss:0.2616, val_acc:0.9172\n",
            "epoch:9/30, train_steps:260, train_loss:0.0067, trina_acc:0.9980, val_loss:0.2671, val_acc:0.9166\n",
            "epoch:9/30, train_steps:270, train_loss:0.0078, trina_acc:0.9987, val_loss:0.2693, val_acc:0.9155\n",
            "--- 7.926340579986572 seconds per epoch ---\n",
            "epoch:10/30, train_steps:280, train_loss:0.0034, trina_acc:0.9998, val_loss:0.2682, val_acc:0.9158\n",
            "epoch:10/30, train_steps:290, train_loss:0.0046, trina_acc:0.9993, val_loss:0.2669, val_acc:0.9164\n",
            "--- 8.293434858322144 seconds per epoch ---\n",
            "epoch:11/30, train_steps:300, train_loss:0.0030, trina_acc:0.9995, val_loss:0.2684, val_acc:0.9163\n",
            "epoch:11/30, train_steps:310, train_loss:0.0044, trina_acc:0.9988, val_loss:0.2691, val_acc:0.9159\n",
            "epoch:11/30, train_steps:320, train_loss:0.0055, trina_acc:0.9985, val_loss:0.2682, val_acc:0.9161\n",
            "--- 7.679681062698364 seconds per epoch ---\n",
            "epoch:12/30, train_steps:330, train_loss:0.0026, trina_acc:0.9998, val_loss:0.2763, val_acc:0.9151\n",
            "epoch:12/30, train_steps:340, train_loss:0.0033, trina_acc:0.9995, val_loss:0.2711, val_acc:0.9162\n",
            "epoch:12/30, train_steps:350, train_loss:0.0042, trina_acc:0.9983, val_loss:0.2723, val_acc:0.9165\n",
            "--- 7.648068904876709 seconds per epoch ---\n",
            "epoch:13/30, train_steps:360, train_loss:0.0021, trina_acc:0.9998, val_loss:0.2774, val_acc:0.9145\n",
            "epoch:13/30, train_steps:370, train_loss:0.0017, trina_acc:1.0000, val_loss:0.2822, val_acc:0.9147\n",
            "--- 6.973832607269287 seconds per epoch ---\n",
            "epoch:14/30, train_steps:380, train_loss:0.0016, trina_acc:1.0000, val_loss:0.2894, val_acc:0.9132\n",
            "epoch:14/30, train_steps:390, train_loss:0.0025, trina_acc:0.9995, val_loss:0.2860, val_acc:0.9143\n",
            "epoch:14/30, train_steps:400, train_loss:0.0024, trina_acc:0.9998, val_loss:0.2777, val_acc:0.9158\n",
            "--- 7.660906791687012 seconds per epoch ---\n",
            "epoch:15/30, train_steps:410, train_loss:0.0017, trina_acc:1.0000, val_loss:0.2772, val_acc:0.9166\n",
            "epoch:15/30, train_steps:420, train_loss:0.0035, trina_acc:0.9998, val_loss:0.2892, val_acc:0.9138\n",
            "epoch:15/30, train_steps:430, train_loss:0.0026, trina_acc:0.9995, val_loss:0.2895, val_acc:0.9146\n",
            "--- 7.669517993927002 seconds per epoch ---\n",
            "epoch:16/30, train_steps:440, train_loss:0.0021, trina_acc:0.9998, val_loss:0.2849, val_acc:0.9157\n",
            "epoch:16/30, train_steps:450, train_loss:0.0016, trina_acc:0.9998, val_loss:0.2821, val_acc:0.9153\n",
            "--- 7.347015142440796 seconds per epoch ---\n",
            "epoch:17/30, train_steps:460, train_loss:0.0017, trina_acc:0.9998, val_loss:0.2828, val_acc:0.9162\n",
            "epoch:17/30, train_steps:470, train_loss:0.0028, trina_acc:0.9993, val_loss:0.2905, val_acc:0.9145\n",
            "epoch:17/30, train_steps:480, train_loss:0.0021, trina_acc:0.9993, val_loss:0.2828, val_acc:0.9163\n",
            "--- 9.146316051483154 seconds per epoch ---\n",
            "epoch:18/30, train_steps:490, train_loss:0.0017, trina_acc:0.9995, val_loss:0.2880, val_acc:0.9146\n",
            "epoch:18/30, train_steps:500, train_loss:0.0014, trina_acc:1.0000, val_loss:0.2851, val_acc:0.9157\n",
            "epoch:18/30, train_steps:510, train_loss:0.0018, trina_acc:0.9995, val_loss:0.2865, val_acc:0.9154\n",
            "--- 7.67109489440918 seconds per epoch ---\n",
            "epoch:19/30, train_steps:520, train_loss:0.0009, trina_acc:1.0000, val_loss:0.2886, val_acc:0.9148\n",
            "epoch:19/30, train_steps:530, train_loss:0.0032, trina_acc:0.9993, val_loss:0.3007, val_acc:0.9121\n",
            "epoch:19/30, train_steps:540, train_loss:0.0008, trina_acc:1.0000, val_loss:0.3074, val_acc:0.9120\n",
            "--- 7.692709684371948 seconds per epoch ---\n",
            "epoch:20/30, train_steps:550, train_loss:0.0012, trina_acc:1.0000, val_loss:0.2872, val_acc:0.9154\n",
            "epoch:20/30, train_steps:560, train_loss:0.0015, trina_acc:0.9995, val_loss:0.2906, val_acc:0.9143\n",
            "--- 8.028789281845093 seconds per epoch ---\n",
            "epoch:21/30, train_steps:570, train_loss:0.0018, trina_acc:0.9998, val_loss:0.3184, val_acc:0.9094\n",
            "epoch:21/30, train_steps:580, train_loss:0.0015, trina_acc:0.9995, val_loss:0.3210, val_acc:0.9088\n",
            "epoch:21/30, train_steps:590, train_loss:0.0012, trina_acc:1.0000, val_loss:0.3040, val_acc:0.9132\n",
            "--- 8.244978189468384 seconds per epoch ---\n",
            "epoch:22/30, train_steps:600, train_loss:0.0013, trina_acc:1.0000, val_loss:0.2918, val_acc:0.9157\n",
            "epoch:22/30, train_steps:610, train_loss:0.0019, trina_acc:0.9995, val_loss:0.2989, val_acc:0.9148\n",
            "epoch:22/30, train_steps:620, train_loss:0.0028, trina_acc:0.9990, val_loss:0.2989, val_acc:0.9144\n",
            "--- 7.67730712890625 seconds per epoch ---\n",
            "epoch:23/30, train_steps:630, train_loss:0.0007, trina_acc:1.0000, val_loss:0.3135, val_acc:0.9111\n",
            "epoch:23/30, train_steps:640, train_loss:0.0022, trina_acc:0.9995, val_loss:0.2978, val_acc:0.9152\n",
            "--- 7.008640289306641 seconds per epoch ---\n",
            "epoch:24/30, train_steps:650, train_loss:0.0007, trina_acc:1.0000, val_loss:0.3009, val_acc:0.9147\n",
            "epoch:24/30, train_steps:660, train_loss:0.0024, trina_acc:0.9993, val_loss:0.3153, val_acc:0.9119\n",
            "epoch:24/30, train_steps:670, train_loss:0.0011, trina_acc:0.9998, val_loss:0.2971, val_acc:0.9156\n",
            "--- 7.621429681777954 seconds per epoch ---\n",
            "epoch:25/30, train_steps:680, train_loss:0.0005, trina_acc:1.0000, val_loss:0.3020, val_acc:0.9147\n",
            "epoch:25/30, train_steps:690, train_loss:0.0006, trina_acc:1.0000, val_loss:0.3098, val_acc:0.9130\n",
            "epoch:25/30, train_steps:700, train_loss:0.0020, trina_acc:0.9995, val_loss:0.3215, val_acc:0.9114\n",
            "--- 7.6718590259552 seconds per epoch ---\n",
            "epoch:26/30, train_steps:710, train_loss:0.0008, trina_acc:0.9998, val_loss:0.3003, val_acc:0.9153\n",
            "epoch:26/30, train_steps:720, train_loss:0.0007, trina_acc:1.0000, val_loss:0.3166, val_acc:0.9117\n",
            "--- 7.09164834022522 seconds per epoch ---\n",
            "epoch:27/30, train_steps:730, train_loss:0.0005, trina_acc:1.0000, val_loss:0.3136, val_acc:0.9129\n",
            "epoch:27/30, train_steps:740, train_loss:0.0005, trina_acc:1.0000, val_loss:0.3003, val_acc:0.9151\n",
            "epoch:27/30, train_steps:750, train_loss:0.0019, trina_acc:0.9995, val_loss:0.3207, val_acc:0.9110\n",
            "--- 7.668077230453491 seconds per epoch ---\n",
            "epoch:28/30, train_steps:760, train_loss:0.0007, trina_acc:1.0000, val_loss:0.3050, val_acc:0.9155\n",
            "epoch:28/30, train_steps:770, train_loss:0.0013, trina_acc:0.9998, val_loss:0.3027, val_acc:0.9149\n",
            "epoch:28/30, train_steps:780, train_loss:0.0023, trina_acc:0.9995, val_loss:0.3134, val_acc:0.9133\n",
            "--- 7.768030881881714 seconds per epoch ---\n",
            "epoch:29/30, train_steps:790, train_loss:0.0020, trina_acc:0.9990, val_loss:0.3141, val_acc:0.9138\n",
            "epoch:29/30, train_steps:800, train_loss:0.0006, trina_acc:1.0000, val_loss:0.3084, val_acc:0.9149\n",
            "epoch:29/30, train_steps:810, train_loss:0.0042, trina_acc:0.9987, val_loss:0.3154, val_acc:0.9125\n",
            "--- 8.534751892089844 seconds per epoch ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh43tN04v6Js",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_to_read = open('data/test_x.pkl', 'rb')\n",
        "\n",
        "test_x = pickle.load(file_to_read)\n",
        "\n",
        "file_to_read.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDqYCa4LwzKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_to_read = open('data/test_y.pkl', 'rb')\n",
        "\n",
        "test_y = pickle.load(file_to_read)\n",
        "\n",
        "file_to_read.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlY7h3zHv5Yj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "a6e29400-6b51-483c-90be-ca9138e3a88c"
      },
      "source": [
        "y_predict = predict(test_x)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from saves/fasttext/model.ckpt-150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLKlvnwF7XiH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c_prediction = tf.equal(y_predict, test_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXsdGomlITh-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "bb8506e3-2da0-4c56-bc91-dd9b5edbebef"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(test_y, y_predict)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1754,   46,   62,   38],\n",
              "       [  18, 1860,   14,    8],\n",
              "       [  50,   12, 1698,  140],\n",
              "       [  52,   21,  119, 1708]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK2V6lEaI2_E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b0013868-f0a7-4294-cdc8-304ee7cd9f69"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(test_y, y_predict)\n",
        "print('The test accuracy is', accuracy)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The test accuracy is 0.9236842105263158\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-XvA4R37XfW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}