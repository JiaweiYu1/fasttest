{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dbpedia_model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JiaweiYu1/fasttext/blob/master/dbpedia_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ0MJoc3pWXz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import codecs\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "import csv\n",
        "import html\n",
        "\n",
        "from tensorflow.contrib import slim\n",
        "\n",
        "import argparse\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "import pprint\n",
        "import easydict\n",
        "import random\n",
        "\n",
        "\n",
        "#tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hzf2nE99pZTQ",
        "colab_type": "code",
        "outputId": "ffedeaa9-1984-40c6-88be-8bbc9ff4b2b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/Colab_Notebooks/fasttext\")\n",
        "cwd = os.getcwd()\n",
        "print(cwd)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "/content/drive/My Drive/Colab_Notebooks/fasttext\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzwfd1P0pDT-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class fastTextModel():\n",
        "    \"\"\"\n",
        "    A simple implementation of fasttext for text classification\n",
        "    \"\"\"\n",
        "    def __init__(self, sequence_length, num_classes, vocab_size,\n",
        "                 embedding_size, learning_rate, l2_reg_lambda, epoch, \n",
        "                 decay_steps, decay_rate, is_training=True,\n",
        "                 initializer=tf.random_normal_initializer(stddev=0.1)):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.num_classes = num_classes\n",
        "        self.sequence_length = sequence_length\n",
        "        self.learning_rate = learning_rate\n",
        "        self.decay_steps = decay_steps\n",
        "        self.decay_rate = decay_rate\n",
        "        self.epoch = epoch\n",
        "        self.is_training = is_training\n",
        "        self.l2_reg_lambda = l2_reg_lambda\n",
        "        self.initializer = initializer\n",
        " \n",
        "        self.input_x = tf.placeholder(tf.int32, [None, self.sequence_length], name='input_x')\n",
        "        self.input_y = tf.placeholder(tf.int32, [None, self.num_classes], name='input_y')\n",
        " \n",
        "        self.global_step = tf.Variable(0, trainable=False, name='global_step')\n",
        "        self.instantiate_weight()\n",
        "        self.logits = self.inference()\n",
        "        self.loss_val = self.loss()\n",
        "        self.train_op = self.train()\n",
        " \n",
        "        self.predictions = tf.argmax(self.logits, axis=1, name='predictions')\n",
        "        correct_prediction = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'), name='accuracy')\n",
        " \n",
        "    def instantiate_weight(self):\n",
        "        with tf.variable_scope('weights', reuse=tf.AUTO_REUSE):\n",
        "            self.Embedding = tf.get_variable('Embedding', shape=[self.vocab_size, self.embedding_size],\n",
        "                                             initializer=self.initializer)\n",
        "            self.W_projection = tf.get_variable('W_projection', shape=[self.embedding_size, self.num_classes],\n",
        "                                                initializer=self.initializer)\n",
        "            self.b_projection = tf.get_variable('b_projection', shape=[self.num_classes])\n",
        " \n",
        " \n",
        "    def inference(self):\n",
        "        with tf.name_scope('embedding'):\n",
        "            words_embedding = tf.nn.embedding_lookup(self.Embedding, self.input_x)\n",
        "            self.average_embedding = tf.reduce_mean(words_embedding, axis=1)\n",
        " \n",
        "        logits = tf.matmul(self.average_embedding, self.W_projection) +self.b_projection\n",
        " \n",
        "        return logits\n",
        " \n",
        " \n",
        "    def loss(self):\n",
        "        # loss\n",
        "        with tf.name_scope('loss'):\n",
        "            losses = tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.input_y, logits=self.logits)\n",
        "            data_loss = tf.reduce_mean(losses)\n",
        "            #l2_loss = tf.add_n([tf.nn.l2_loss(cand_var) for cand_var in tf.trainable_variables()\n",
        "            #                    if 'bias' not in cand_var.name]) * self.l2_reg_lambda\n",
        "            #data_loss += l2_loss * self.l2_reg_lambda\n",
        "            return data_loss\n",
        " \n",
        "    def train(self):\n",
        "        with tf.name_scope('train'):\n",
        "            learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step,\n",
        "                                                       self.decay_steps, self.decay_rate,\n",
        "                                                       staircase=True)\n",
        " \n",
        "            train_op = tf.contrib.layers.optimize_loss(self.loss_val, global_step=self.global_step,\n",
        "                                                      learning_rate=learning_rate, optimizer='Adam')\n",
        " \n",
        "        return train_op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1aWQqGNpLEv",
        "colab_type": "code",
        "outputId": "7f6a511e-fe8c-4491-c410-28281c8c4892",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "\n",
        "class_size=14\n",
        "learning_rate=0.1\n",
        "batch_size=32\n",
        "decay_steps=1000\n",
        "decay_rate=0.9\n",
        "sequence_length=467\n",
        "vocab_size = 7191861\n",
        "embedding_dim = 10\n",
        "is_training=True\n",
        "dropout_keep_prob=1\n",
        "epoch = 10\n",
        "batch_size = 4096\n",
        "l2_reg_lambda = 0.01\n",
        "\n",
        "\n",
        "fasttext = fastTextModel(sequence_length,\n",
        "                      class_size,\n",
        "                      vocab_size,\n",
        "                      embedding_dim,\n",
        "                      learning_rate,\n",
        "                      l2_reg_lambda,\n",
        "                      epoch,\n",
        "                      decay_steps,\n",
        "                      decay_rate,\n",
        "                      is_training=True,\n",
        "                    )"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trXP-7j6pLBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_iter(x, y, batch_size=1, shuffle=True):\n",
        "  data_x = np.array(x)\n",
        "  data_y = np.array(y)\n",
        "  data_len = len(x)\n",
        "  num_batch = int((data_len-1)/batch_size)+1\n",
        "    \n",
        "  if shuffle:\n",
        "    shuffle_indices = np.random.permutation(np.arange(data_len))\n",
        "    x_shuffle = data_x[shuffle_indices]\n",
        "    y_shuffle= data_y[shuffle_indices]\n",
        "    \n",
        "  else:\n",
        "    x_shuffle=x\n",
        "    y_shuffle = y\n",
        "  for i in range(num_batch):\n",
        "    start_index = i*batch_size\n",
        "    end_index = min((i+1)*batch_size, data_len)\n",
        "    yield (x_shuffle[start_index:end_index], y_shuffle[start_index:end_index])\n",
        "    \n",
        "    \n",
        "    \n",
        "def fit(train_x, train_y, x_dev, y_dev, batch_size, epoch):\n",
        "  if not os.path.exists('dbpedia_csv/saves/fasttext'): os.makedirs('dbpedia_csv/saves/fasttext')\n",
        "  if not os.path.exists('dbpedia_csv/train_logs/fasttext'): os.makedirs('dbpedia_csv/train_logs/fasttext')\n",
        "\n",
        "  train_steps = 0\n",
        "  best_val_acc = 0\n",
        "    \n",
        "  tf.summary.scalar('val_loss', fasttext.loss_val)\n",
        "  tf.summary.scalar('val_accuracy', fasttext.accuracy)\n",
        "  merged = tf.summary.merge_all()\n",
        "    \n",
        "  sess = tf.Session()\n",
        "    \n",
        "  writer = tf.summary.FileWriter('dbpedia_csv/train_logs/fasttext', sess.graph)\n",
        "  saver = tf.train.Saver(max_to_keep=10)\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "  for i in range(epoch):\n",
        "    start_time = time.time()\n",
        "    batch_train = batch_iter(train_x, train_y, batch_size)\n",
        "    for batch_x, batch_y in batch_train:\n",
        "      train_steps +=1\n",
        "      feed_dict = {fasttext.input_x:batch_x, fasttext.input_y:batch_y}\n",
        "      _, train_loss, train_acc = sess.run([fasttext.train_op, fasttext.loss_val, fasttext.accuracy], feed_dict = feed_dict)\n",
        "        \n",
        "      if train_steps % 100 ==0:\n",
        "        feed_dict = {fasttext.input_x:x_dev, fasttext.input_y:y_dev}\n",
        "        val_loss, val_acc = sess.run([fasttext.loss_val, fasttext.accuracy],feed_dict=feed_dict)\n",
        "        \n",
        "        summary = sess.run(merged, feed_dict=feed_dict)\n",
        "        writer.add_summary(summary, global_step=train_steps)\n",
        "        \n",
        "        if val_acc >= best_val_acc:\n",
        "          best_val_acc = val_acc\n",
        "          saver.save(sess, \"dbpedia_csv/saves/fasttext/model.ckpt\", global_step=train_steps)\n",
        "        msg = 'epoch:%d/%d, train_steps:%d, train_loss:%.4f, trina_acc:%.4f, val_loss:%.4f, val_acc:%.4f'\n",
        "        print(msg%(i, epoch, train_steps, train_loss, train_acc, val_loss, val_acc))\n",
        "    print(\"--- %s seconds per epoch ---\" % (time.time() - start_time))   \n",
        "\n",
        "      \n",
        "def predict(x):\n",
        "  sess = tf.Session()\n",
        "    \n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  saver = tf.train.Saver(tf.global_variables())\n",
        "  ckpt = tf.train.get_checkpoint_state('dbpedia_csv/saves/fasttext/')\n",
        "  saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "  #saver.restore(sess, 'saves/fasttext/checkpoint')\n",
        "    \n",
        "  feed_dict={fasttext.input_x:x}\n",
        "  logits = sess.run(fasttext.logits, feed_dict=feed_dict)\n",
        "  y_pred = np.argmax(logits, 1)\n",
        "    \n",
        "  return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BBx9ZnYA7bUW",
        "colab": {}
      },
      "source": [
        "file_to_read = open('data/dbpedia_train_x_1.pkl', 'rb')\n",
        "\n",
        "dbpedia_train_x = pickle.load(file_to_read)\n",
        "\n",
        "file_to_read.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BValQTgy7bUZ",
        "colab": {}
      },
      "source": [
        "file_to_read = open('data/dbpedia_y_train_1.pkl', 'rb')\n",
        "\n",
        "dbpedia_y_train = pickle.load(file_to_read)\n",
        "\n",
        "file_to_read.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9xcwEV-aLPZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_to_read = open('data/dbpedia_test_x.pkl', 'rb')\n",
        "\n",
        "dbpedia_x_test = pickle.load(file_to_read)\n",
        "\n",
        "file_to_read.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUKOQyAUaLyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_to_read = open('data/dbpedia_test_y.pkl', 'rb')\n",
        "\n",
        "dbpedia_y_test = pickle.load(file_to_read)\n",
        "\n",
        "file_to_read.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xlmXn9ea7bUb",
        "colab": {}
      },
      "source": [
        "dev_sample_index = -1 * int(0.1 * float(len(dbpedia_y_train)))\n",
        "x_train_dbpedia, x_dev_dbpedia = dbpedia_train_x[:dev_sample_index], dbpedia_train_x[dev_sample_index:]\n",
        "dbpedia_y_train_1, dbpedia_y_dev_1 = dbpedia_y_train[:dev_sample_index], dbpedia_y_train[dev_sample_index:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "niWPFTRY7bUd",
        "colab": {}
      },
      "source": [
        "dbpedia_y_dev_2 = np.array(dbpedia_y_dev_1).astype(np.int32).tolist()\n",
        "dbpedia_y_train_2 = np.array(dbpedia_y_train_1).astype(np.int32).tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl9df9Gd9D4I",
        "colab_type": "code",
        "outputId": "c84494c9-17fc-4229-f71f-de5cce2865d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(dbpedia_y_train_1)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "491400"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "12189051-8e55-4600-fed0-30887a863fb6",
        "id": "nTmIlggm7bUg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "fit(x_train_dbpedia, dbpedia_y_train_2, x_dev_dbpedia, dbpedia_y_dev_2,batch_size, epoch)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:0/10, train_steps:100, train_loss:0.0899, trina_acc:0.9810, val_loss:0.0924, val_acc:0.9812\n",
            "--- 118.15000224113464 seconds per epoch ---\n",
            "epoch:1/10, train_steps:200, train_loss:0.0247, trina_acc:0.9946, val_loss:0.0646, val_acc:0.9858\n",
            "--- 118.39497113227844 seconds per epoch ---\n",
            "epoch:2/10, train_steps:300, train_loss:0.0041, trina_acc:1.0000, val_loss:0.0585, val_acc:0.9860\n",
            "--- 120.29153656959534 seconds per epoch ---\n",
            "epoch:3/10, train_steps:400, train_loss:0.0029, trina_acc:0.9998, val_loss:0.0567, val_acc:0.9863\n",
            "--- 114.25055456161499 seconds per epoch ---\n",
            "epoch:4/10, train_steps:500, train_loss:0.0013, trina_acc:1.0000, val_loss:0.0562, val_acc:0.9866\n",
            "epoch:4/10, train_steps:600, train_loss:0.0011, trina_acc:1.0000, val_loss:0.0562, val_acc:0.9865\n",
            "--- 119.35283946990967 seconds per epoch ---\n",
            "epoch:5/10, train_steps:700, train_loss:0.0009, trina_acc:1.0000, val_loss:0.0564, val_acc:0.9866\n",
            "--- 112.45448613166809 seconds per epoch ---\n",
            "epoch:6/10, train_steps:800, train_loss:0.0005, trina_acc:1.0000, val_loss:0.0567, val_acc:0.9866\n",
            "--- 111.07025265693665 seconds per epoch ---\n",
            "epoch:7/10, train_steps:900, train_loss:0.0005, trina_acc:1.0000, val_loss:0.0570, val_acc:0.9867\n",
            "--- 114.29453349113464 seconds per epoch ---\n",
            "epoch:8/10, train_steps:1000, train_loss:0.0004, trina_acc:1.0000, val_loss:0.0572, val_acc:0.9867\n",
            "--- 117.50882029533386 seconds per epoch ---\n",
            "epoch:9/10, train_steps:1100, train_loss:0.0003, trina_acc:1.0000, val_loss:0.0575, val_acc:0.9868\n",
            "epoch:9/10, train_steps:1200, train_loss:0.0003, trina_acc:1.0000, val_loss:0.0578, val_acc:0.9868\n",
            "--- 122.43618702888489 seconds per epoch ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHtSFyeEe4yq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_to_read = open('data/dbpedia_test_x.pkl', 'rb')\n",
        "\n",
        "test_x = pickle.load(file_to_read)\n",
        "\n",
        "file_to_read.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nK4h11Dde4wE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_to_read = open('data/dbpedia_test_y.pkl', 'rb')\n",
        "\n",
        "test_y = pickle.load(file_to_read)\n",
        "\n",
        "file_to_read.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2GDE1ELe4tU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "3d1c07d6-b438-492d-9c92-c5da5ac055d7"
      },
      "source": [
        "y_predict = predict(test_x)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from dbpedia_csv/saves/fasttext/model.ckpt-1200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQVfaF1Re4qz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c_prediction = tf.equal(y_predict, test_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D50PvXn4fWSk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "d7af0d72-3511-4393-9814-a113936ce3de"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(test_y, y_predict)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4822,   28,    9,    3,   11,   20,   50,    2,    0,    2,    1,\n",
              "          11,    5,   36],\n",
              "       [  38, 4926,    1,    1,    6,    1,   25,    1,    0,    0,    0,\n",
              "           0,    0,    1],\n",
              "       [  15,    3, 4880,   10,   56,    0,    3,    0,    1,    0,    0,\n",
              "          13,    4,   15],\n",
              "       [   1,    1,   30, 4952,   11,    0,    1,    0,    0,    2,    0,\n",
              "           1,    1,    0],\n",
              "       [   6,    3,   64,   10, 4906,    3,    3,    0,    0,    0,    1,\n",
              "           1,    0,    3],\n",
              "       [  15,    1,    0,    0,    1, 4973,    5,    0,    1,    2,    0,\n",
              "           0,    2,    0],\n",
              "       [  51,   28,    1,    0,    6,    8, 4877,   16,    7,    1,    0,\n",
              "           2,    0,    3],\n",
              "       [   0,    3,    0,    0,    0,    1,   16, 4974,    6,    0,    0,\n",
              "           0,    0,    0],\n",
              "       [   0,    0,    1,    0,    0,    0,    6,   11, 4981,    1,    0,\n",
              "           0,    0,    0],\n",
              "       [   1,    0,    0,    2,    0,    1,    0,    1,    0, 4976,   19,\n",
              "           0,    0,    0],\n",
              "       [  12,    1,    0,    0,    0,    1,    1,    0,    0,   13, 4972,\n",
              "           0,    0,    0],\n",
              "       [   2,    0,    9,    0,    0,    1,    0,    0,    0,    0,    0,\n",
              "        4973,   10,    5],\n",
              "       [   5,    0,    6,    0,    0,    0,    1,    0,    0,    0,    0,\n",
              "          13, 4951,   24],\n",
              "       [  27,    1,   13,    1,    6,    3,    3,    0,    0,    0,    2,\n",
              "           2,   18, 4924]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3kTUjICfXAf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a3364a7a-a66e-46e5-e303-ba4fae1243eb"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(test_y, y_predict)\n",
        "print('The test accuracy is', accuracy)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The test accuracy is 0.9869571428571429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB3gikJxfazL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiMwTNSYfawj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbhsMhMnfat2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}