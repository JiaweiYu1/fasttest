{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fasttext.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JiaweiYu1/fasttext/blob/master/fasttext.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLhI5k0zVzXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import codecs\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "import csv\n",
        "import html\n",
        "\n",
        "from tensorflow.contrib import slim\n",
        "\n",
        "import argparse\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "import pprint\n",
        "import easydict\n",
        "import random\n",
        "\n",
        "\n",
        "#tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyUwshl225Cb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "5b86e1fa-1840-4eee-f454-c2f0e1ad5fc8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/Colab_Notebooks/fasttext\")\n",
        "cwd = os.getcwd()\n",
        "print(cwd)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "/content/drive/My Drive/Colab_Notebooks/fasttext\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzvJFcENXiU5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "f921d82d-0485-415c-b96a-a53bf9388e2f"
      },
      "source": [
        "!python -m spacy download en_core_web_lg\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_lg')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_lg==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.0.0/en_core_web_lg-2.0.0.tar.gz#egg=en_core_web_lg==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_lg -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en_core_web_lg\n",
            "\n",
            "    You can now load the model via spacy.load('en_core_web_lg')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXqi7haeaG_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_ngram(words, n=2):\n",
        "    # TODO add ngrams up to n\n",
        "    return [' '.join(words[i: i+n]) for i in range(len(words)-(n-1))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPu-gXlLHge0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class load_data:\n",
        "  \n",
        "  def __init__(self, config):\n",
        "    self.config = config\n",
        "    self.class_size = config.class_size\n",
        "    self.max_len = config.max_len\n",
        "    self.n_over_max_len = 0\n",
        "    self.real_max_len = 0\n",
        "    \n",
        "    np.random.seed(config.seed)\n",
        "    \n",
        "    self.ngram2idx = dict()\n",
        "    self.idx2ngram = dict()\n",
        "    self.ngram2idx['PAD'] = 0\n",
        "    self.idx2ngram[0] = 'PAD'\n",
        "    self.ngram2idx['UNK'] = 1\n",
        "    self.idx2ngram[1] = 'UNK'\n",
        "    self.html_tag_re = re.compile(r'<[^>]+>')\n",
        "    self.train_data, self.test_data = self.load_csv()\n",
        "    self.train_data, self.valid_data = \\\n",
        "        self.split_tr_va(n_class_examples=config.valid_size_per_class)\n",
        "    self.count_labels()\n",
        "\n",
        "    print('real_max_len', self.real_max_len)\n",
        "    print('n_over_max_len {}/{} ({:.1f}%)'.\n",
        "          format(self.n_over_max_len, len(self.train_data),\n",
        "          100 * self.n_over_max_len / len(self.train_data)))\n",
        "    \n",
        "  def load_csv(self):\n",
        "    train_data = list()\n",
        "    test_data = list()\n",
        "    \n",
        "    #spacy.prefer_gpu()\n",
        "\n",
        "    # https://spacy.io/usage/facts-figures#benchmarks-models-english\n",
        "    # python3 -m spacy download en_core_web_lg --user\n",
        "    nlp = spacy.load('en_core_web_lg', disable=['parser', 'tagger', 'ner'])\n",
        "    nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
        "    \n",
        "    with open(self.config.train_data_path, 'r', newline='',\n",
        "              encoding='utf-8') as f:\n",
        "      reader = csv.reader(f, quotechar='\"')\n",
        "      for idx, features in enumerate(reader):\n",
        "        y = int(features[0]) -1\n",
        "        assert 0 <= y <= self.class_size, y\n",
        "        x, x_len = self.process_example(features[1], features[2], nlp, is_train=True,\n",
        "                                       padding=args.padding)\n",
        "        train_data.append([x, x_len, y])\n",
        "        \n",
        "        if (idx+1) %10000 ==0:\n",
        "          print(idx+1)\n",
        "    \n",
        "    with open(self.config.test_data_path,'r', newline='',\n",
        "             encoding = 'utf-8') as f:\n",
        "      reader = csv.reader(f, quotechar='\"')\n",
        "      for idx, features in enumerate(reader):\n",
        "        y = int(features[0]) -1\n",
        "        assert 0 <= y <= self.class_size, y\n",
        "        x, x_len = self.process_example(features[1], features[2], nlp, is_train=True,\n",
        "                                       padding=args.padding)\n",
        "        test_data.append([x, x_len, y])\n",
        "        \n",
        "    print('dictionary size ', len(self.ngram2idx))\n",
        "    \n",
        "    return train_data, test_data\n",
        "  \n",
        "  def process_example(self, title, description, nlp, is_train=True, padding=0):\n",
        "    title_desc = title + '. ' + description\n",
        "    \n",
        "    if '\\\\' in title_desc:\n",
        "      title_desc = title_desc.replace('\\\\',' ')\n",
        "      \n",
        "    title_desc = html.unescape(title_desc)\n",
        "    \n",
        "    if '<' in title_desc and '>' in title_desc:\n",
        "      title_desc = self.html_tag_re.sub('', title_desc)\n",
        "      \n",
        "    # create bow and bag-of-ngrams\n",
        "    doc = nlp(title_desc)\n",
        "    b_o_w = [token.text for token in doc]\n",
        "    \n",
        "    \n",
        "    # add tags for ngrams\n",
        "    tagged_title_desc = \\\n",
        "    '<p> ' + ' </s>'.join([s.text for s in doc.sents])+\\\n",
        "    ' <p>'\n",
        "    doc = nlp(tagged_title_desc)\n",
        "    n_gram = get_ngram([token.text for token in doc],\n",
        "                       n = self.config.n_gram)\n",
        "    \n",
        "    b_o_ngrams = b_o_w+n_gram\n",
        "    \n",
        "    if padding >0:\n",
        "      if self.max_len < len(b_o_ngrams):\n",
        "        b_o_ngrams = b_o_ngrams[:self.max_len]\n",
        "        \n",
        "    if is_train:\n",
        "      for ng in b_o_ngrams:\n",
        "        idx = self.ngram2idx.get(ng)\n",
        "        if idx is None:\n",
        "          idx = len(self.ngram2idx)\n",
        "          self.ngram2idx[ng] = idx\n",
        "          self.idx2ngram[idx] = ng\n",
        "          \n",
        "    x = [self.ngram2idx[ng] if ng in self.ngram2idx\n",
        "         else self.ngram2idx['UNK'] for ng in b_o_ngrams]\n",
        "    \n",
        "    x_len = len(x)\n",
        "    \n",
        "    if x_len > self.max_len:\n",
        "      self.n_over_max_len +=1\n",
        "      \n",
        "    if x_len > self.real_max_len:\n",
        "      self.real_max_len = x_len\n",
        "      \n",
        "    if padding >0:\n",
        "      while len(x) < self.max_len:\n",
        "        x.append(self.ngram2idx['PAD'])\n",
        "      assert len(x) == self.max_len\n",
        "    return x, x_len\n",
        "  \n",
        "  def count_labels(self):\n",
        "    def count(data):\n",
        "      count_dict = dict()\n",
        "      for d in data:\n",
        "        if d[-1] not in count_dict:\n",
        "          count_dict[d[-1]] = 1\n",
        "        else:\n",
        "          count_dict[d[-1]] +=1\n",
        "          \n",
        "      return count_dict\n",
        "    print('train', count(self.train_data))\n",
        "    print('test', count(self.test_data))\n",
        "    \n",
        "  def split_tr_va(self, n_class_examples=1900):\n",
        "        count = 0\n",
        "        class_item_set_dict = dict()\n",
        "        item_all = list()\n",
        "\n",
        "        print('Splitting..')\n",
        "        while count < n_class_examples * self.config.class_size:\n",
        "            rand_pick = np.random.randint(len(self.train_data))\n",
        "            # print(rand_pick)\n",
        "            label = self.train_data[rand_pick][-1]\n",
        "            if label in class_item_set_dict:\n",
        "                item_set = class_item_set_dict[label]\n",
        "                if len(item_set) < n_class_examples \\\n",
        "                        and rand_pick not in item_set:\n",
        "                    item_set.add(rand_pick)\n",
        "                    item_all.append(rand_pick)\n",
        "                    count += 1\n",
        "            else:\n",
        "                class_item_set_dict[label] = set()\n",
        "                class_item_set_dict[label].add(rand_pick)\n",
        "                item_all.append(rand_pick)\n",
        "                count += 1\n",
        "\n",
        "        train_data2 = list()\n",
        "        valid_data = list()\n",
        "        for idx, td in enumerate(self.train_data):\n",
        "            if idx in item_all:\n",
        "                valid_data.append(td)\n",
        "            else:\n",
        "                train_data2.append(td)\n",
        "\n",
        "        print(len(train_data2), len(valid_data))\n",
        "        return train_data2, valid_data\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onWwNjYsOgKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = easydict.EasyDict({\n",
        "    \"train_data_path\": 'ag_news_csv/train.csv',\n",
        "    \"test_data_path\": 'ag_news_csv/test.csv',\n",
        "    \"pickle_path\": 'ag.pkl',\n",
        "    \"seed\": 2019,\n",
        "    \"class_size\": 4,\n",
        "    \"valid_size_per_class\":1000,\n",
        "    \"n_gram\": 2,\n",
        "    \"padding\":1,\n",
        "    \"max_len\":467\n",
        "})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a7BM9FSQqLl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "cbff5732-f9ca-43f4-c9de-c84b3237c346"
      },
      "source": [
        "agdata = load_data(args)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000\n",
            "20000\n",
            "30000\n",
            "40000\n",
            "50000\n",
            "60000\n",
            "70000\n",
            "80000\n",
            "90000\n",
            "100000\n",
            "110000\n",
            "120000\n",
            "dictionary size  1439345\n",
            "Splitting..\n",
            "116000 4000\n",
            "train {2: 29000, 3: 29000, 1: 29000, 0: 29000}\n",
            "test {2: 1900, 3: 1900, 1: 1900, 0: 1900}\n",
            "real_max_len 465\n",
            "n_over_max_len 0/116000 (0.0%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRqaPt27dc5j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "0c7c2e0a-3817-4652-f16e-a9721fe1f194"
      },
      "source": [
        "train_data, test_data = agdata.load_csv()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000\n",
            "20000\n",
            "30000\n",
            "40000\n",
            "50000\n",
            "60000\n",
            "70000\n",
            "80000\n",
            "90000\n",
            "100000\n",
            "110000\n",
            "120000\n",
            "dictionary size  1439345\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDdmZWlZehGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x = [e[0] for e in train_data]\n",
        "train_y = [e[2] for e in train_data]\n",
        "train_x_len = [e[1] for e in train_data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4xQ525Jvzb8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = []\n",
        "for i in train_y:\n",
        "  if i==0:\n",
        "    y_train.append([1,0,0,0])\n",
        "  if i==1:\n",
        "    y_train.append([0,1,0,0])\n",
        "  if i==2:\n",
        "    y_train.append([0,0,1,0])\n",
        "    \n",
        "  if i==3:\n",
        "    y_train.append([0,0,0,1])\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6_wd98BQ3f-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class fastTextModel():\n",
        "    \"\"\"\n",
        "    A simple implementation of fasttext for text classification\n",
        "    \"\"\"\n",
        "    def __init__(self, sequence_length, num_classes, vocab_size,\n",
        "                 embedding_size, learning_rate, decay_steps, decay_rate,\n",
        "                 l2_reg_lambda, epoch, is_training=True,\n",
        "                 initializer=tf.random_normal_initializer(stddev=0.1)):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.num_classes = num_classes\n",
        "        self.sequence_length = sequence_length\n",
        "        self.learning_rate = learning_rate\n",
        "        self.decay_steps = decay_steps\n",
        "        self.decay_rate = decay_rate\n",
        "        self.epoch = epoch\n",
        "        self.is_training = is_training\n",
        "        self.l2_reg_lambda = l2_reg_lambda\n",
        "        self.initializer = initializer\n",
        " \n",
        "        self.input_x = tf.placeholder(tf.int32, [None, self.sequence_length], name='input_x')\n",
        "        self.input_y = tf.placeholder(tf.int32, [None, self.num_classes], name='input_y')\n",
        " \n",
        "        self.global_step = tf.Variable(0, trainable=False, name='global_step')\n",
        "        self.instantiate_weight()\n",
        "        self.logits = self.inference()\n",
        "        self.loss_val = self.loss()\n",
        "        self.train_op = self.train()\n",
        " \n",
        "        self.predictions = tf.argmax(self.logits, axis=1, name='predictions')\n",
        "        correct_prediction = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'), name='accuracy')\n",
        " \n",
        "    def instantiate_weight(self):\n",
        "        with tf.variable_scope('weights', reuse=tf.AUTO_REUSE):\n",
        "            self.Embedding = tf.get_variable('Embedding', shape=[self.vocab_size, self.embedding_size],\n",
        "                                             initializer=self.initializer)\n",
        "            self.W_projection = tf.get_variable('W_projection', shape=[self.embedding_size, self.num_classes],\n",
        "                                                initializer=self.initializer)\n",
        "            self.b_projection = tf.get_variable('b_projection', shape=[self.num_classes])\n",
        " \n",
        " \n",
        "    def inference(self):\n",
        "        with tf.name_scope('embedding'):\n",
        "            words_embedding = tf.nn.embedding_lookup(self.Embedding, self.input_x)\n",
        "            self.average_embedding = tf.reduce_mean(words_embedding, axis=1)\n",
        " \n",
        "        logits = tf.matmul(self.average_embedding, self.W_projection) +self.b_projection\n",
        " \n",
        "        return logits\n",
        " \n",
        " \n",
        "    def loss(self):\n",
        "        # loss\n",
        "        with tf.name_scope('loss'):\n",
        "            losses = tf.nn.softmax_cross_entropy_with_logits(labels=self.input_y, logits=self.logits)\n",
        "            data_loss = tf.reduce_mean(losses)\n",
        "            l2_loss = tf.add_n([tf.nn.l2_loss(cand_var) for cand_var in tf.trainable_variables()\n",
        "                                if 'bias' not in cand_var.name]) * self.l2_reg_lambda\n",
        "            data_loss += l2_loss * self.l2_reg_lambda\n",
        "            return data_loss\n",
        " \n",
        "    def train(self):\n",
        "        with tf.name_scope('train'):\n",
        "            learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step,\n",
        "                                                       self.decay_steps, self.decay_rate,\n",
        "                                                       staircase=True)\n",
        " \n",
        "            train_op = tf.contrib.layers.optimize_loss(self.loss_val, global_step=self.global_step,\n",
        "                                                      learning_rate=learning_rate, optimizer='Adam')\n",
        " \n",
        "        return train_op\n",
        "\n",
        "    def fit(self, train_x, train_y, x_dev, y_dev, batch_size):\n",
        "        if not os.path.exists('saves/fasttext'): os.makedirs('saves/fasttext')\n",
        "        if not os.path.exists('train_logs/fasttext'): os.makedirs('train_logs/fasttext')\n",
        "\n",
        "        train_steps = 0\n",
        "        best_val_acc = 0\n",
        "    \n",
        "        tf.summary.scalar('val_loss', self.loss_val)\n",
        "        tf.summary.scalar('val_accuracy', self.accuracy)\n",
        "        merged = tf.summary.merge_all()\n",
        "    \n",
        "        sess = tf.Session()\n",
        "    \n",
        "        writer = tf.summary.FileWriter('train_logs/fasttext', sess.graph)\n",
        "        saver = tf.train.Saver(max_to_keep=10)\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "        for i in range(self.epoch):\n",
        "          batch_train = self.batch_iter(train_x, train_y, batch_size)\n",
        "          for batch_x, batch_y in batch_train:\n",
        "            train_steps +=1\n",
        "            feed_dict = {self.input_x:batch_x, self.input_y:batch_y}\n",
        "            _, train_loss, train_acc = sess.run([self.train_op, self.loss_val, self.accuracy], feed_dict = feed_dict)\n",
        "        \n",
        "            if train_steps % 1000 ==0:\n",
        "              feed_dict = {self.input_x:x_dev, self.input_y:y_dev}\n",
        "              val_loss, val_acc = sess.run([self.loss_val, self.acc],feed_dict=feed_dict)\n",
        "        \n",
        "              summary = sess.run(merged, feed_dict=feed_dict)\n",
        "              writer.add_summary(summary, global_step=train_steps)\n",
        "        \n",
        "              if val_acc >= best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                saver.save(sess, \"saves/fasttext\", global_step=train_steps)\n",
        "              msg = 'epoch:%d/%d, train_steps:%d, train_loss:%.4f, trina_acc:%.4f, val_loss:%.4f, val_acc:%.4f'\n",
        "              print(msg%(i, self.epoch, train_steps, train_loss, train_acc, val_loss, val_acc))\n",
        "        \n",
        "    def batch_iter(self, x, y, batch_size=1, shuffle=True):\n",
        "      #data = np.array(data)\n",
        "      data_len = len(x)\n",
        "      num_batch = int((data_len-1)/batch_size)+1\n",
        "    \n",
        "      if shuffle:\n",
        "        #shuffle_indices = np.random.permutation(np.arange(data_len))\n",
        "        #x_shuffle = x[shuffle_indices]\n",
        "        #y_shuffle= y[shuffle_indices]\n",
        "        random.shuffle(x)\n",
        "        x_shuffle = x\n",
        "        random.shuffle(y)\n",
        "        y_shuffle = y\n",
        "      else:\n",
        "        x_shuffle=x\n",
        "        y_shuffle = y\n",
        "      for i in range(num_batch):\n",
        "        start_index = i*batch_size\n",
        "        end_index = min((i+1)*batch_size, data_len)\n",
        "        yield (x_shuffle[start_index:end_index], y_shuffle[start_index:end_index])\n",
        "      \n",
        "    def predict(self, x):\n",
        "      sess = tf.Session()\n",
        "    \n",
        "      sess.run(tf.global_variables_initializer())\n",
        "      saver = tf.train.Svaer(tf.global_variables())\n",
        "      ckpt = tf.train.get_checkpoint_state('')\n",
        "      saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "    \n",
        "      feed_dict={self.input_x:x}\n",
        "      logits = sess.run(self.logits, feed_dict=feed_dict)\n",
        "      y_pred = np.argmax(logits, 1)\n",
        "    \n",
        "      return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJOg5jb5XlIb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "ef380779-37e2-4398-a59d-cfb87795e555"
      },
      "source": [
        "\n",
        "class_size=4\n",
        "learning_rate=0.01\n",
        "batch_size=32\n",
        "decay_steps=1000\n",
        "decay_rate=0.9\n",
        "sequence_length=467\n",
        "vocab_size = 1439345\n",
        "embedding_dim = 10\n",
        "is_training=True\n",
        "dropout_keep_prob=1\n",
        "epoch = 5000\n",
        "batch_size = 4096\n",
        "l2_reg_lambda = 0.01\n",
        "\n",
        "\n",
        "fasttext = fastTextModel(sequence_length,\n",
        "                      class_size,\n",
        "                      vocab_size,\n",
        "                      embedding_dim,\n",
        "                      learning_rate,\n",
        "                      decay_steps,\n",
        "                      decay_rate,\n",
        "                      l2_reg_lambda,\n",
        "                      epoch,\n",
        "                      is_training=True,\n",
        "                    )"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-12-75a78ff14617>:56: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2BgI3c6Wl7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_iter(x, y, batch_size=1, shuffle=True):\n",
        "  data_x = np.array(x)\n",
        "  data_y = np.array(y)\n",
        "  data_len = len(x)\n",
        "  num_batch = int((data_len-1)/batch_size)+1\n",
        "    \n",
        "  if shuffle:\n",
        "    shuffle_indices = np.random.permutation(np.arange(data_len))\n",
        "    x_shuffle = data_x[shuffle_indices]\n",
        "    y_shuffle= data_y[shuffle_indices]\n",
        "    \n",
        "  else:\n",
        "    x_shuffle=x\n",
        "    y_shuffle = y\n",
        "  for i in range(num_batch):\n",
        "    start_index = i*batch_size\n",
        "    end_index = min((i+1)*batch_size, data_len)\n",
        "    yield (x_shuffle[start_index:end_index], y_shuffle[start_index:end_index])\n",
        "    \n",
        "    \n",
        "    \n",
        "def fit(train_x, train_y, x_dev, y_dev, batch_size, epoch):\n",
        "  if not os.path.exists('saves/fasttext'): os.makedirs('saves/fasttext')\n",
        "  if not os.path.exists('train_logs/fasttext'): os.makedirs('train_logs/fasttext')\n",
        "\n",
        "  train_steps = 0\n",
        "  best_val_acc = 0\n",
        "    \n",
        "  tf.summary.scalar('val_loss', fasttext.loss_val)\n",
        "  tf.summary.scalar('val_accuracy', fasttext.accuracy)\n",
        "  merged = tf.summary.merge_all()\n",
        "    \n",
        "  sess = tf.Session()\n",
        "    \n",
        "  writer = tf.summary.FileWriter('train_logs/fasttext', sess.graph)\n",
        "  saver = tf.train.Saver(max_to_keep=10)\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "  for i in range(epoch):\n",
        "    batch_train = batch_iter(train_x, train_y, batch_size)\n",
        "    for batch_x, batch_y in batch_train:\n",
        "      train_steps +=1\n",
        "      feed_dict = {fasttext.input_x:batch_x, fasttext.input_y:batch_y}\n",
        "      _, train_loss, train_acc = sess.run([fasttext.train_op, fasttext.loss_val, fasttext.accuracy], feed_dict = feed_dict)\n",
        "        \n",
        "      if train_steps % 10 ==0:\n",
        "        feed_dict = {fasttext.input_x:x_dev, fasttext.input_y:y_dev}\n",
        "        val_loss, val_acc = sess.run([fasttext.loss_val, fasttext.accuracy],feed_dict=feed_dict)\n",
        "        \n",
        "        summary = sess.run(merged, feed_dict=feed_dict)\n",
        "        writer.add_summary(summary, global_step=train_steps)\n",
        "        \n",
        "        if val_acc >= best_val_acc:\n",
        "          best_val_acc = val_acc\n",
        "          saver.save(sess, \"saves/fasttext\", global_step=train_steps)\n",
        "        msg = 'epoch:%d/%d, train_steps:%d, train_loss:%.4f, trina_acc:%.4f, val_loss:%.4f, val_acc:%.4f'\n",
        "        print(msg%(i, epoch, train_steps, train_loss, train_acc, val_loss, val_acc))\n",
        "        \n",
        "\n",
        "      \n",
        "def predict(x):\n",
        "  sess = tf.Session()\n",
        "    \n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  saver = tf.train.Svaer(tf.global_variables())\n",
        "  ckpt = tf.train.get_checkpoint_state('')\n",
        "  saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "    \n",
        "  feed_dict={fasttext.input_x:x}\n",
        "  logits = sess.run(fasttext.logits, feed_dict=feed_dict)\n",
        "  y_pred = np.argmax(logits, 1)\n",
        "    \n",
        "  return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOUraTHW0RbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_sample_index = -1 * int(0.1 * float(len(train_y)))\n",
        "x_train, x_dev = train_x[:dev_sample_index], train_x[dev_sample_index:]\n",
        "y_train_1, y_dev_1 = y_train[:dev_sample_index], y_train[dev_sample_index:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iuzo5b4i730S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_dev_2 = np.array(y_dev_1).astype(np.int32).tolist()\n",
        "y_train_2 = np.array(y_train_1).astype(np.int32).tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dy0T6uP-aRQq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 15116
        },
        "outputId": "96d9c6bd-a2d5-464d-b394-b6213b38d5f5"
      },
      "source": [
        "fit(x_train, y_train_2, x_dev, y_dev_2,batch_size, epoch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:0/5000, train_steps:10, train_loss:2.8957, trina_acc:0.2444, val_loss:2.6462, val_acc:0.2442\n",
            "epoch:0/5000, train_steps:20, train_loss:1.7050, trina_acc:0.2639, val_loss:1.6713, val_acc:0.2400\n",
            "epoch:1/5000, train_steps:30, train_loss:1.5127, trina_acc:0.2620, val_loss:1.5029, val_acc:0.2542\n",
            "epoch:1/5000, train_steps:40, train_loss:1.4370, trina_acc:0.2534, val_loss:1.4324, val_acc:0.2442\n",
            "epoch:1/5000, train_steps:50, train_loss:1.4031, trina_acc:0.2727, val_loss:1.4020, val_acc:0.2498\n",
            "epoch:2/5000, train_steps:60, train_loss:1.3905, trina_acc:0.3804, val_loss:1.3906, val_acc:0.3348\n",
            "epoch:2/5000, train_steps:70, train_loss:1.3859, trina_acc:0.2998, val_loss:1.3852, val_acc:0.3306\n",
            "epoch:2/5000, train_steps:80, train_loss:1.3825, trina_acc:0.4385, val_loss:1.3827, val_acc:0.4477\n",
            "epoch:3/5000, train_steps:90, train_loss:1.3806, trina_acc:0.4995, val_loss:1.3804, val_acc:0.4882\n",
            "epoch:3/5000, train_steps:100, train_loss:1.3775, trina_acc:0.5176, val_loss:1.3781, val_acc:0.4562\n",
            "epoch:4/5000, train_steps:110, train_loss:1.3761, trina_acc:0.4324, val_loss:1.3760, val_acc:0.3912\n",
            "epoch:4/5000, train_steps:120, train_loss:1.3711, trina_acc:0.5281, val_loss:1.3719, val_acc:0.5146\n",
            "epoch:4/5000, train_steps:130, train_loss:1.3674, trina_acc:0.4805, val_loss:1.3678, val_acc:0.4887\n",
            "epoch:5/5000, train_steps:140, train_loss:1.3626, trina_acc:0.5242, val_loss:1.3637, val_acc:0.5010\n",
            "epoch:5/5000, train_steps:150, train_loss:1.3593, trina_acc:0.3940, val_loss:1.3581, val_acc:0.4390\n",
            "epoch:5/5000, train_steps:160, train_loss:1.3499, trina_acc:0.4692, val_loss:1.3547, val_acc:0.4448\n",
            "epoch:6/5000, train_steps:170, train_loss:1.3421, trina_acc:0.4492, val_loss:1.3466, val_acc:0.4452\n",
            "epoch:6/5000, train_steps:180, train_loss:1.3391, trina_acc:0.5054, val_loss:1.3416, val_acc:0.5147\n",
            "epoch:7/5000, train_steps:190, train_loss:1.3275, trina_acc:0.5444, val_loss:1.3371, val_acc:0.5203\n",
            "epoch:7/5000, train_steps:200, train_loss:1.3212, trina_acc:0.5374, val_loss:1.3293, val_acc:0.5390\n",
            "epoch:7/5000, train_steps:210, train_loss:1.3168, trina_acc:0.5151, val_loss:1.3221, val_acc:0.5153\n",
            "epoch:8/5000, train_steps:220, train_loss:1.3077, trina_acc:0.5566, val_loss:1.3160, val_acc:0.5457\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "epoch:8/5000, train_steps:230, train_loss:1.3099, trina_acc:0.5518, val_loss:1.3122, val_acc:0.5540\n",
            "epoch:8/5000, train_steps:240, train_loss:1.2938, trina_acc:0.5996, val_loss:1.3047, val_acc:0.5870\n",
            "epoch:9/5000, train_steps:250, train_loss:1.2902, trina_acc:0.5593, val_loss:1.2980, val_acc:0.5461\n",
            "epoch:9/5000, train_steps:260, train_loss:1.2865, trina_acc:0.5952, val_loss:1.2918, val_acc:0.5929\n",
            "epoch:9/5000, train_steps:270, train_loss:1.2876, trina_acc:0.5678, val_loss:1.2876, val_acc:0.5837\n",
            "epoch:10/5000, train_steps:280, train_loss:1.2645, trina_acc:0.6252, val_loss:1.2798, val_acc:0.5913\n",
            "epoch:10/5000, train_steps:290, train_loss:1.2608, trina_acc:0.6208, val_loss:1.2710, val_acc:0.6132\n",
            "epoch:11/5000, train_steps:300, train_loss:1.2459, trina_acc:0.6538, val_loss:1.2671, val_acc:0.6205\n",
            "epoch:11/5000, train_steps:310, train_loss:1.2417, trina_acc:0.6545, val_loss:1.2603, val_acc:0.6218\n",
            "epoch:11/5000, train_steps:320, train_loss:1.2366, trina_acc:0.6526, val_loss:1.2524, val_acc:0.6313\n",
            "epoch:12/5000, train_steps:330, train_loss:1.2236, trina_acc:0.6729, val_loss:1.2476, val_acc:0.6389\n",
            "epoch:12/5000, train_steps:340, train_loss:1.2255, trina_acc:0.6516, val_loss:1.2398, val_acc:0.6476\n",
            "epoch:12/5000, train_steps:350, train_loss:1.2120, trina_acc:0.6841, val_loss:1.2349, val_acc:0.6510\n",
            "epoch:13/5000, train_steps:360, train_loss:1.2151, trina_acc:0.6663, val_loss:1.2305, val_acc:0.6593\n",
            "epoch:13/5000, train_steps:370, train_loss:1.1991, trina_acc:0.6973, val_loss:1.2228, val_acc:0.6725\n",
            "epoch:14/5000, train_steps:380, train_loss:1.1901, trina_acc:0.6987, val_loss:1.2181, val_acc:0.6739\n",
            "epoch:14/5000, train_steps:390, train_loss:1.1948, trina_acc:0.6831, val_loss:1.2116, val_acc:0.6792\n",
            "epoch:14/5000, train_steps:400, train_loss:1.1799, trina_acc:0.7092, val_loss:1.2054, val_acc:0.6830\n",
            "epoch:15/5000, train_steps:410, train_loss:1.1883, trina_acc:0.6990, val_loss:1.2034, val_acc:0.6869\n",
            "epoch:15/5000, train_steps:420, train_loss:1.1679, trina_acc:0.7197, val_loss:1.1956, val_acc:0.6947\n",
            "epoch:15/5000, train_steps:430, train_loss:1.1680, trina_acc:0.7185, val_loss:1.1911, val_acc:0.6939\n",
            "epoch:16/5000, train_steps:440, train_loss:1.1658, trina_acc:0.7163, val_loss:1.1851, val_acc:0.6998\n",
            "epoch:16/5000, train_steps:450, train_loss:1.1588, trina_acc:0.7158, val_loss:1.1834, val_acc:0.6985\n",
            "epoch:17/5000, train_steps:460, train_loss:1.1510, trina_acc:0.7261, val_loss:1.1775, val_acc:0.7078\n",
            "epoch:17/5000, train_steps:470, train_loss:1.1595, trina_acc:0.7146, val_loss:1.1712, val_acc:0.7112\n",
            "epoch:17/5000, train_steps:480, train_loss:1.1534, trina_acc:0.7217, val_loss:1.1680, val_acc:0.7147\n",
            "epoch:18/5000, train_steps:490, train_loss:1.1455, trina_acc:0.7266, val_loss:1.1670, val_acc:0.7120\n",
            "epoch:18/5000, train_steps:500, train_loss:1.1366, trina_acc:0.7351, val_loss:1.1614, val_acc:0.7187\n",
            "epoch:18/5000, train_steps:510, train_loss:1.1286, trina_acc:0.7488, val_loss:1.1560, val_acc:0.7234\n",
            "epoch:19/5000, train_steps:520, train_loss:1.1316, trina_acc:0.7397, val_loss:1.1530, val_acc:0.7251\n",
            "epoch:19/5000, train_steps:530, train_loss:1.1137, trina_acc:0.7512, val_loss:1.1501, val_acc:0.7251\n",
            "epoch:19/5000, train_steps:540, train_loss:1.1189, trina_acc:0.7507, val_loss:1.1473, val_acc:0.7222\n",
            "epoch:20/5000, train_steps:550, train_loss:1.1092, trina_acc:0.7554, val_loss:1.1425, val_acc:0.7337\n",
            "epoch:20/5000, train_steps:560, train_loss:1.1044, trina_acc:0.7595, val_loss:1.1383, val_acc:0.7345\n",
            "epoch:21/5000, train_steps:570, train_loss:1.1010, trina_acc:0.7666, val_loss:1.1397, val_acc:0.7377\n",
            "epoch:21/5000, train_steps:580, train_loss:1.1068, trina_acc:0.7595, val_loss:1.1331, val_acc:0.7301\n",
            "epoch:21/5000, train_steps:590, train_loss:1.1020, trina_acc:0.7703, val_loss:1.1295, val_acc:0.7441\n",
            "epoch:22/5000, train_steps:600, train_loss:1.1087, trina_acc:0.7546, val_loss:1.1279, val_acc:0.7374\n",
            "epoch:22/5000, train_steps:610, train_loss:1.0892, trina_acc:0.7690, val_loss:1.1235, val_acc:0.7440\n",
            "epoch:22/5000, train_steps:620, train_loss:1.0889, trina_acc:0.7747, val_loss:1.1236, val_acc:0.7480\n",
            "epoch:23/5000, train_steps:630, train_loss:1.1083, trina_acc:0.7651, val_loss:1.1193, val_acc:0.7427\n",
            "epoch:23/5000, train_steps:640, train_loss:1.0887, trina_acc:0.7710, val_loss:1.1177, val_acc:0.7473\n",
            "epoch:24/5000, train_steps:650, train_loss:1.0854, trina_acc:0.7771, val_loss:1.1163, val_acc:0.7556\n",
            "epoch:24/5000, train_steps:660, train_loss:1.0655, trina_acc:0.7749, val_loss:1.1109, val_acc:0.7512\n",
            "epoch:24/5000, train_steps:670, train_loss:1.0799, trina_acc:0.7776, val_loss:1.1083, val_acc:0.7540\n",
            "epoch:25/5000, train_steps:680, train_loss:1.0688, trina_acc:0.7803, val_loss:1.1081, val_acc:0.7590\n",
            "epoch:25/5000, train_steps:690, train_loss:1.0741, trina_acc:0.7778, val_loss:1.1053, val_acc:0.7556\n",
            "epoch:25/5000, train_steps:700, train_loss:1.0795, trina_acc:0.7754, val_loss:1.1035, val_acc:0.7602\n",
            "epoch:26/5000, train_steps:710, train_loss:1.0868, trina_acc:0.7649, val_loss:1.0997, val_acc:0.7630\n",
            "epoch:26/5000, train_steps:720, train_loss:1.0573, trina_acc:0.7922, val_loss:1.0985, val_acc:0.7623\n",
            "epoch:27/5000, train_steps:730, train_loss:1.0646, trina_acc:0.7812, val_loss:1.1000, val_acc:0.7625\n",
            "epoch:27/5000, train_steps:740, train_loss:1.0782, trina_acc:0.7725, val_loss:1.0958, val_acc:0.7671\n",
            "epoch:27/5000, train_steps:750, train_loss:1.0522, trina_acc:0.7878, val_loss:1.0949, val_acc:0.7673\n",
            "epoch:28/5000, train_steps:760, train_loss:1.0606, trina_acc:0.7900, val_loss:1.0979, val_acc:0.7653\n",
            "epoch:28/5000, train_steps:770, train_loss:1.0668, trina_acc:0.7822, val_loss:1.0917, val_acc:0.7699\n",
            "epoch:28/5000, train_steps:780, train_loss:1.0571, trina_acc:0.7878, val_loss:1.0885, val_acc:0.7722\n",
            "epoch:29/5000, train_steps:790, train_loss:1.0555, trina_acc:0.7878, val_loss:1.0864, val_acc:0.7730\n",
            "epoch:29/5000, train_steps:800, train_loss:1.0513, trina_acc:0.7966, val_loss:1.0846, val_acc:0.7735\n",
            "epoch:29/5000, train_steps:810, train_loss:1.0323, trina_acc:0.8185, val_loss:1.0856, val_acc:0.7756\n",
            "epoch:30/5000, train_steps:820, train_loss:1.0461, trina_acc:0.7913, val_loss:1.0807, val_acc:0.7752\n",
            "epoch:30/5000, train_steps:830, train_loss:1.0569, trina_acc:0.7971, val_loss:1.0806, val_acc:0.7803\n",
            "epoch:31/5000, train_steps:840, train_loss:1.0462, trina_acc:0.8015, val_loss:1.0842, val_acc:0.7807\n",
            "epoch:31/5000, train_steps:850, train_loss:1.0517, trina_acc:0.7983, val_loss:1.0789, val_acc:0.7803\n",
            "epoch:31/5000, train_steps:860, train_loss:1.0479, trina_acc:0.7827, val_loss:1.0776, val_acc:0.7816\n",
            "epoch:32/5000, train_steps:870, train_loss:1.0517, trina_acc:0.7905, val_loss:1.0784, val_acc:0.7804\n",
            "epoch:32/5000, train_steps:880, train_loss:1.0453, trina_acc:0.8020, val_loss:1.0744, val_acc:0.7859\n",
            "epoch:32/5000, train_steps:890, train_loss:1.0481, trina_acc:0.7971, val_loss:1.0749, val_acc:0.7803\n",
            "epoch:33/5000, train_steps:900, train_loss:1.0348, trina_acc:0.8044, val_loss:1.0721, val_acc:0.7857\n",
            "epoch:33/5000, train_steps:910, train_loss:1.0518, trina_acc:0.7922, val_loss:1.0705, val_acc:0.7881\n",
            "epoch:34/5000, train_steps:920, train_loss:1.0294, trina_acc:0.8096, val_loss:1.0721, val_acc:0.7882\n",
            "epoch:34/5000, train_steps:930, train_loss:1.0416, trina_acc:0.8037, val_loss:1.0681, val_acc:0.7918\n",
            "epoch:34/5000, train_steps:940, train_loss:1.0405, trina_acc:0.8025, val_loss:1.0677, val_acc:0.7884\n",
            "epoch:35/5000, train_steps:950, train_loss:1.0332, trina_acc:0.8047, val_loss:1.0713, val_acc:0.7863\n",
            "epoch:35/5000, train_steps:960, train_loss:1.0291, trina_acc:0.8052, val_loss:1.0659, val_acc:0.7914\n",
            "epoch:35/5000, train_steps:970, train_loss:1.0357, trina_acc:0.8096, val_loss:1.0646, val_acc:0.7897\n",
            "epoch:36/5000, train_steps:980, train_loss:1.0394, trina_acc:0.8037, val_loss:1.0638, val_acc:0.7932\n",
            "epoch:36/5000, train_steps:990, train_loss:1.0231, trina_acc:0.8162, val_loss:1.0640, val_acc:0.7915\n",
            "epoch:37/5000, train_steps:1000, train_loss:1.0244, trina_acc:0.8123, val_loss:1.0655, val_acc:0.7927\n",
            "epoch:37/5000, train_steps:1010, train_loss:1.0381, trina_acc:0.7957, val_loss:1.0597, val_acc:0.7977\n",
            "epoch:37/5000, train_steps:1020, train_loss:1.0290, trina_acc:0.8076, val_loss:1.0612, val_acc:0.7887\n",
            "epoch:38/5000, train_steps:1030, train_loss:1.0339, trina_acc:0.8081, val_loss:1.0643, val_acc:0.7968\n",
            "epoch:38/5000, train_steps:1040, train_loss:1.0255, trina_acc:0.8149, val_loss:1.0612, val_acc:0.7928\n",
            "epoch:38/5000, train_steps:1050, train_loss:1.0320, trina_acc:0.8054, val_loss:1.0590, val_acc:0.7903\n",
            "epoch:39/5000, train_steps:1060, train_loss:1.0148, trina_acc:0.8259, val_loss:1.0608, val_acc:0.7972\n",
            "epoch:39/5000, train_steps:1070, train_loss:1.0223, trina_acc:0.8147, val_loss:1.0567, val_acc:0.7979\n",
            "epoch:39/5000, train_steps:1080, train_loss:1.0399, trina_acc:0.7866, val_loss:1.0561, val_acc:0.8002\n",
            "epoch:40/5000, train_steps:1090, train_loss:1.0058, trina_acc:0.8220, val_loss:1.0549, val_acc:0.7986\n",
            "epoch:40/5000, train_steps:1100, train_loss:1.0088, trina_acc:0.8247, val_loss:1.0563, val_acc:0.7922\n",
            "epoch:41/5000, train_steps:1110, train_loss:1.0211, trina_acc:0.8176, val_loss:1.0584, val_acc:0.8032\n",
            "epoch:41/5000, train_steps:1120, train_loss:1.0231, trina_acc:0.8157, val_loss:1.0537, val_acc:0.8021\n",
            "epoch:41/5000, train_steps:1130, train_loss:1.0345, trina_acc:0.8088, val_loss:1.0529, val_acc:0.8051\n",
            "epoch:42/5000, train_steps:1140, train_loss:1.0173, trina_acc:0.8127, val_loss:1.0557, val_acc:0.7993\n",
            "epoch:42/5000, train_steps:1150, train_loss:1.0271, trina_acc:0.8088, val_loss:1.0548, val_acc:0.7984\n",
            "epoch:42/5000, train_steps:1160, train_loss:1.0300, trina_acc:0.8088, val_loss:1.0531, val_acc:0.7972\n",
            "epoch:43/5000, train_steps:1170, train_loss:1.0108, trina_acc:0.8311, val_loss:1.0519, val_acc:0.8026\n",
            "epoch:43/5000, train_steps:1180, train_loss:1.0296, trina_acc:0.8164, val_loss:1.0536, val_acc:0.8034\n",
            "epoch:44/5000, train_steps:1190, train_loss:1.0141, trina_acc:0.8230, val_loss:1.0539, val_acc:0.8023\n",
            "epoch:44/5000, train_steps:1200, train_loss:1.0157, trina_acc:0.8196, val_loss:1.0509, val_acc:0.8032\n",
            "epoch:44/5000, train_steps:1210, train_loss:1.0291, trina_acc:0.8208, val_loss:1.0513, val_acc:0.7993\n",
            "epoch:45/5000, train_steps:1220, train_loss:1.0346, trina_acc:0.8142, val_loss:1.0530, val_acc:0.8054\n",
            "epoch:45/5000, train_steps:1230, train_loss:1.0218, trina_acc:0.8164, val_loss:1.0486, val_acc:0.8094\n",
            "epoch:45/5000, train_steps:1240, train_loss:1.0189, trina_acc:0.8215, val_loss:1.0478, val_acc:0.8073\n",
            "epoch:46/5000, train_steps:1250, train_loss:1.0062, trina_acc:0.8323, val_loss:1.0487, val_acc:0.8107\n",
            "epoch:46/5000, train_steps:1260, train_loss:1.0172, trina_acc:0.8218, val_loss:1.0481, val_acc:0.8050\n",
            "epoch:47/5000, train_steps:1270, train_loss:1.0170, trina_acc:0.8223, val_loss:1.0515, val_acc:0.8018\n",
            "epoch:47/5000, train_steps:1280, train_loss:1.0164, trina_acc:0.8140, val_loss:1.0500, val_acc:0.8037\n",
            "epoch:47/5000, train_steps:1290, train_loss:1.0149, trina_acc:0.8308, val_loss:1.0476, val_acc:0.8108\n",
            "epoch:48/5000, train_steps:1300, train_loss:1.0032, trina_acc:0.8284, val_loss:1.0506, val_acc:0.8080\n",
            "epoch:48/5000, train_steps:1310, train_loss:1.0137, trina_acc:0.8203, val_loss:1.0455, val_acc:0.8117\n",
            "epoch:48/5000, train_steps:1320, train_loss:1.0200, trina_acc:0.8252, val_loss:1.0504, val_acc:0.8018\n",
            "epoch:49/5000, train_steps:1330, train_loss:1.0196, trina_acc:0.8218, val_loss:1.0492, val_acc:0.8094\n",
            "epoch:49/5000, train_steps:1340, train_loss:1.0076, trina_acc:0.8328, val_loss:1.0464, val_acc:0.8079\n",
            "epoch:49/5000, train_steps:1350, train_loss:1.0150, trina_acc:0.8218, val_loss:1.0461, val_acc:0.8125\n",
            "epoch:50/5000, train_steps:1360, train_loss:1.0019, trina_acc:0.8367, val_loss:1.0455, val_acc:0.8125\n",
            "epoch:50/5000, train_steps:1370, train_loss:1.0174, trina_acc:0.8311, val_loss:1.0456, val_acc:0.8063\n",
            "epoch:51/5000, train_steps:1380, train_loss:0.9994, trina_acc:0.8362, val_loss:1.0486, val_acc:0.8130\n",
            "epoch:51/5000, train_steps:1390, train_loss:1.0098, trina_acc:0.8262, val_loss:1.0445, val_acc:0.8121\n",
            "epoch:51/5000, train_steps:1400, train_loss:1.0012, trina_acc:0.8313, val_loss:1.0469, val_acc:0.8099\n",
            "epoch:52/5000, train_steps:1410, train_loss:1.0132, trina_acc:0.8279, val_loss:1.0509, val_acc:0.8042\n",
            "epoch:52/5000, train_steps:1420, train_loss:1.0181, trina_acc:0.8206, val_loss:1.0444, val_acc:0.8139\n",
            "epoch:52/5000, train_steps:1430, train_loss:1.0066, trina_acc:0.8396, val_loss:1.0471, val_acc:0.8092\n",
            "epoch:53/5000, train_steps:1440, train_loss:1.0146, trina_acc:0.8264, val_loss:1.0423, val_acc:0.8146\n",
            "epoch:53/5000, train_steps:1450, train_loss:1.0075, trina_acc:0.8330, val_loss:1.0421, val_acc:0.8168\n",
            "epoch:54/5000, train_steps:1460, train_loss:1.0040, trina_acc:0.8311, val_loss:1.0461, val_acc:0.8141\n",
            "epoch:54/5000, train_steps:1470, train_loss:1.0036, trina_acc:0.8347, val_loss:1.0414, val_acc:0.8160\n",
            "epoch:54/5000, train_steps:1480, train_loss:0.9983, trina_acc:0.8315, val_loss:1.0436, val_acc:0.8145\n",
            "epoch:55/5000, train_steps:1490, train_loss:1.0088, trina_acc:0.8350, val_loss:1.0458, val_acc:0.8163\n",
            "epoch:55/5000, train_steps:1500, train_loss:1.0160, trina_acc:0.8210, val_loss:1.0431, val_acc:0.8142\n",
            "epoch:55/5000, train_steps:1510, train_loss:1.0135, trina_acc:0.8247, val_loss:1.0421, val_acc:0.8151\n",
            "epoch:56/5000, train_steps:1520, train_loss:0.9984, trina_acc:0.8394, val_loss:1.0424, val_acc:0.8142\n",
            "epoch:56/5000, train_steps:1530, train_loss:0.9977, trina_acc:0.8350, val_loss:1.0405, val_acc:0.8176\n",
            "epoch:57/5000, train_steps:1540, train_loss:1.0092, trina_acc:0.8301, val_loss:1.0429, val_acc:0.8175\n",
            "epoch:57/5000, train_steps:1550, train_loss:1.0232, trina_acc:0.8176, val_loss:1.0420, val_acc:0.8173\n",
            "epoch:57/5000, train_steps:1560, train_loss:1.0106, trina_acc:0.8247, val_loss:1.0421, val_acc:0.8173\n",
            "epoch:58/5000, train_steps:1570, train_loss:1.0092, trina_acc:0.8340, val_loss:1.0453, val_acc:0.8159\n",
            "epoch:58/5000, train_steps:1580, train_loss:1.0151, trina_acc:0.8203, val_loss:1.0426, val_acc:0.8157\n",
            "epoch:58/5000, train_steps:1590, train_loss:1.0091, trina_acc:0.8325, val_loss:1.0420, val_acc:0.8178\n",
            "epoch:59/5000, train_steps:1600, train_loss:1.0158, trina_acc:0.8262, val_loss:1.0422, val_acc:0.8196\n",
            "epoch:59/5000, train_steps:1610, train_loss:1.0071, trina_acc:0.8320, val_loss:1.0430, val_acc:0.8135\n",
            "epoch:59/5000, train_steps:1620, train_loss:1.0211, trina_acc:0.8245, val_loss:1.0429, val_acc:0.8173\n",
            "epoch:60/5000, train_steps:1630, train_loss:1.0138, trina_acc:0.8218, val_loss:1.0441, val_acc:0.8112\n",
            "epoch:60/5000, train_steps:1640, train_loss:1.0089, trina_acc:0.8298, val_loss:1.0441, val_acc:0.8111\n",
            "epoch:61/5000, train_steps:1650, train_loss:1.0051, trina_acc:0.8384, val_loss:1.0457, val_acc:0.8197\n",
            "epoch:61/5000, train_steps:1660, train_loss:1.0049, trina_acc:0.8311, val_loss:1.0425, val_acc:0.8146\n",
            "epoch:61/5000, train_steps:1670, train_loss:1.0020, trina_acc:0.8333, val_loss:1.0410, val_acc:0.8148\n",
            "epoch:62/5000, train_steps:1680, train_loss:1.0126, trina_acc:0.8340, val_loss:1.0438, val_acc:0.8183\n",
            "epoch:62/5000, train_steps:1690, train_loss:1.0055, trina_acc:0.8367, val_loss:1.0385, val_acc:0.8205\n",
            "epoch:62/5000, train_steps:1700, train_loss:1.0031, trina_acc:0.8401, val_loss:1.0407, val_acc:0.8152\n",
            "epoch:63/5000, train_steps:1710, train_loss:0.9980, trina_acc:0.8325, val_loss:1.0480, val_acc:0.8106\n",
            "epoch:63/5000, train_steps:1720, train_loss:1.0233, trina_acc:0.8237, val_loss:1.0427, val_acc:0.8122\n",
            "epoch:64/5000, train_steps:1730, train_loss:1.0009, trina_acc:0.8381, val_loss:1.0454, val_acc:0.8136\n",
            "epoch:64/5000, train_steps:1740, train_loss:1.0096, trina_acc:0.8289, val_loss:1.0380, val_acc:0.8203\n",
            "epoch:64/5000, train_steps:1750, train_loss:1.0166, trina_acc:0.8267, val_loss:1.0395, val_acc:0.8220\n",
            "epoch:65/5000, train_steps:1760, train_loss:0.9942, trina_acc:0.8433, val_loss:1.0445, val_acc:0.8178\n",
            "epoch:65/5000, train_steps:1770, train_loss:0.9959, trina_acc:0.8435, val_loss:1.0391, val_acc:0.8205\n",
            "epoch:65/5000, train_steps:1780, train_loss:1.0045, trina_acc:0.8306, val_loss:1.0436, val_acc:0.8175\n",
            "epoch:66/5000, train_steps:1790, train_loss:1.0008, trina_acc:0.8330, val_loss:1.0398, val_acc:0.8203\n",
            "epoch:66/5000, train_steps:1800, train_loss:0.9996, trina_acc:0.8413, val_loss:1.0398, val_acc:0.8213\n",
            "epoch:67/5000, train_steps:1810, train_loss:1.0032, trina_acc:0.8459, val_loss:1.0420, val_acc:0.8160\n",
            "epoch:67/5000, train_steps:1820, train_loss:0.9898, trina_acc:0.8464, val_loss:1.0390, val_acc:0.8202\n",
            "epoch:67/5000, train_steps:1830, train_loss:1.0072, trina_acc:0.8357, val_loss:1.0376, val_acc:0.8240\n",
            "epoch:68/5000, train_steps:1840, train_loss:0.9979, trina_acc:0.8433, val_loss:1.0455, val_acc:0.8200\n",
            "epoch:68/5000, train_steps:1850, train_loss:1.0097, trina_acc:0.8271, val_loss:1.0385, val_acc:0.8209\n",
            "epoch:68/5000, train_steps:1860, train_loss:1.0048, trina_acc:0.8403, val_loss:1.0379, val_acc:0.8223\n",
            "epoch:69/5000, train_steps:1870, train_loss:1.0031, trina_acc:0.8396, val_loss:1.0411, val_acc:0.8219\n",
            "epoch:69/5000, train_steps:1880, train_loss:1.0168, trina_acc:0.8335, val_loss:1.0381, val_acc:0.8240\n",
            "epoch:69/5000, train_steps:1890, train_loss:0.9807, trina_acc:0.8544, val_loss:1.0393, val_acc:0.8225\n",
            "epoch:70/5000, train_steps:1900, train_loss:1.0064, trina_acc:0.8381, val_loss:1.0381, val_acc:0.8224\n",
            "epoch:70/5000, train_steps:1910, train_loss:1.0079, trina_acc:0.8374, val_loss:1.0390, val_acc:0.8205\n",
            "epoch:71/5000, train_steps:1920, train_loss:0.9997, trina_acc:0.8428, val_loss:1.0437, val_acc:0.8220\n",
            "epoch:71/5000, train_steps:1930, train_loss:1.0086, trina_acc:0.8350, val_loss:1.0378, val_acc:0.8235\n",
            "epoch:71/5000, train_steps:1940, train_loss:1.0042, trina_acc:0.8406, val_loss:1.0379, val_acc:0.8231\n",
            "epoch:72/5000, train_steps:1950, train_loss:1.0129, trina_acc:0.8340, val_loss:1.0411, val_acc:0.8232\n",
            "epoch:72/5000, train_steps:1960, train_loss:1.0089, trina_acc:0.8250, val_loss:1.0380, val_acc:0.8215\n",
            "epoch:72/5000, train_steps:1970, train_loss:1.0119, trina_acc:0.8352, val_loss:1.0391, val_acc:0.8185\n",
            "epoch:73/5000, train_steps:1980, train_loss:1.0187, trina_acc:0.8359, val_loss:1.0389, val_acc:0.8235\n",
            "epoch:73/5000, train_steps:1990, train_loss:0.9885, trina_acc:0.8447, val_loss:1.0395, val_acc:0.8192\n",
            "epoch:74/5000, train_steps:2000, train_loss:1.0057, trina_acc:0.8323, val_loss:1.0421, val_acc:0.8237\n",
            "epoch:74/5000, train_steps:2010, train_loss:1.0118, trina_acc:0.8264, val_loss:1.0379, val_acc:0.8201\n",
            "epoch:74/5000, train_steps:2020, train_loss:1.0053, trina_acc:0.8340, val_loss:1.0370, val_acc:0.8232\n",
            "epoch:75/5000, train_steps:2030, train_loss:1.0035, trina_acc:0.8396, val_loss:1.0400, val_acc:0.8256\n",
            "epoch:75/5000, train_steps:2040, train_loss:1.0031, trina_acc:0.8411, val_loss:1.0353, val_acc:0.8245\n",
            "epoch:75/5000, train_steps:2050, train_loss:1.0030, trina_acc:0.8333, val_loss:1.0363, val_acc:0.8245\n",
            "epoch:76/5000, train_steps:2060, train_loss:0.9981, trina_acc:0.8467, val_loss:1.0380, val_acc:0.8217\n",
            "epoch:76/5000, train_steps:2070, train_loss:0.9963, trina_acc:0.8440, val_loss:1.0361, val_acc:0.8241\n",
            "epoch:77/5000, train_steps:2080, train_loss:0.9926, trina_acc:0.8481, val_loss:1.0362, val_acc:0.8232\n",
            "epoch:77/5000, train_steps:2090, train_loss:0.9975, trina_acc:0.8374, val_loss:1.0372, val_acc:0.8238\n",
            "epoch:77/5000, train_steps:2100, train_loss:0.9995, trina_acc:0.8333, val_loss:1.0368, val_acc:0.8239\n",
            "epoch:78/5000, train_steps:2110, train_loss:1.0139, trina_acc:0.8328, val_loss:1.0387, val_acc:0.8248\n",
            "epoch:78/5000, train_steps:2120, train_loss:0.9864, trina_acc:0.8562, val_loss:1.0368, val_acc:0.8246\n",
            "epoch:78/5000, train_steps:2130, train_loss:1.0099, trina_acc:0.8318, val_loss:1.0365, val_acc:0.8210\n",
            "epoch:79/5000, train_steps:2140, train_loss:0.9979, trina_acc:0.8450, val_loss:1.0383, val_acc:0.8218\n",
            "epoch:79/5000, train_steps:2150, train_loss:1.0051, trina_acc:0.8347, val_loss:1.0356, val_acc:0.8233\n",
            "epoch:79/5000, train_steps:2160, train_loss:1.0137, trina_acc:0.8285, val_loss:1.0359, val_acc:0.8248\n",
            "epoch:80/5000, train_steps:2170, train_loss:0.9880, trina_acc:0.8411, val_loss:1.0356, val_acc:0.8245\n",
            "epoch:80/5000, train_steps:2180, train_loss:1.0013, trina_acc:0.8452, val_loss:1.0377, val_acc:0.8223\n",
            "epoch:81/5000, train_steps:2190, train_loss:1.0050, trina_acc:0.8384, val_loss:1.0411, val_acc:0.8193\n",
            "epoch:81/5000, train_steps:2200, train_loss:1.0061, trina_acc:0.8406, val_loss:1.0395, val_acc:0.8230\n",
            "epoch:81/5000, train_steps:2210, train_loss:0.9974, trina_acc:0.8374, val_loss:1.0396, val_acc:0.8147\n",
            "epoch:82/5000, train_steps:2220, train_loss:1.0050, trina_acc:0.8301, val_loss:1.0403, val_acc:0.8243\n",
            "epoch:82/5000, train_steps:2230, train_loss:0.9997, trina_acc:0.8364, val_loss:1.0406, val_acc:0.8144\n",
            "epoch:82/5000, train_steps:2240, train_loss:0.9957, trina_acc:0.8433, val_loss:1.0348, val_acc:0.8266\n",
            "epoch:83/5000, train_steps:2250, train_loss:1.0028, trina_acc:0.8357, val_loss:1.0392, val_acc:0.8185\n",
            "epoch:83/5000, train_steps:2260, train_loss:0.9882, trina_acc:0.8501, val_loss:1.0337, val_acc:0.8256\n",
            "epoch:84/5000, train_steps:2270, train_loss:0.9920, trina_acc:0.8452, val_loss:1.0377, val_acc:0.8255\n",
            "epoch:84/5000, train_steps:2280, train_loss:1.0126, trina_acc:0.8296, val_loss:1.0344, val_acc:0.8277\n",
            "epoch:84/5000, train_steps:2290, train_loss:1.0028, trina_acc:0.8499, val_loss:1.0360, val_acc:0.8259\n",
            "epoch:85/5000, train_steps:2300, train_loss:0.9979, trina_acc:0.8384, val_loss:1.0434, val_acc:0.8152\n",
            "epoch:85/5000, train_steps:2310, train_loss:1.0069, trina_acc:0.8362, val_loss:1.0363, val_acc:0.8249\n",
            "epoch:85/5000, train_steps:2320, train_loss:1.0057, trina_acc:0.8328, val_loss:1.0370, val_acc:0.8253\n",
            "epoch:86/5000, train_steps:2330, train_loss:1.0025, trina_acc:0.8345, val_loss:1.0393, val_acc:0.8192\n",
            "epoch:86/5000, train_steps:2340, train_loss:1.0077, trina_acc:0.8291, val_loss:1.0368, val_acc:0.8253\n",
            "epoch:87/5000, train_steps:2350, train_loss:0.9982, trina_acc:0.8438, val_loss:1.0385, val_acc:0.8217\n",
            "epoch:87/5000, train_steps:2360, train_loss:0.9990, trina_acc:0.8357, val_loss:1.0360, val_acc:0.8261\n",
            "epoch:87/5000, train_steps:2370, train_loss:1.0051, trina_acc:0.8381, val_loss:1.0361, val_acc:0.8232\n",
            "epoch:88/5000, train_steps:2380, train_loss:1.0045, trina_acc:0.8372, val_loss:1.0394, val_acc:0.8262\n",
            "epoch:88/5000, train_steps:2390, train_loss:1.0090, trina_acc:0.8362, val_loss:1.0350, val_acc:0.8273\n",
            "epoch:88/5000, train_steps:2400, train_loss:1.0062, trina_acc:0.8323, val_loss:1.0347, val_acc:0.8259\n",
            "epoch:89/5000, train_steps:2410, train_loss:1.0190, trina_acc:0.8386, val_loss:1.0373, val_acc:0.8268\n",
            "epoch:89/5000, train_steps:2420, train_loss:1.0040, trina_acc:0.8257, val_loss:1.0348, val_acc:0.8266\n",
            "epoch:89/5000, train_steps:2430, train_loss:1.0074, trina_acc:0.8271, val_loss:1.0375, val_acc:0.8208\n",
            "epoch:90/5000, train_steps:2440, train_loss:1.0002, trina_acc:0.8464, val_loss:1.0405, val_acc:0.8154\n",
            "epoch:90/5000, train_steps:2450, train_loss:0.9864, trina_acc:0.8523, val_loss:1.0359, val_acc:0.8263\n",
            "epoch:91/5000, train_steps:2460, train_loss:0.9986, trina_acc:0.8418, val_loss:1.0392, val_acc:0.8278\n",
            "epoch:91/5000, train_steps:2470, train_loss:0.9992, trina_acc:0.8462, val_loss:1.0349, val_acc:0.8261\n",
            "epoch:91/5000, train_steps:2480, train_loss:1.0026, trina_acc:0.8401, val_loss:1.0371, val_acc:0.8230\n",
            "epoch:92/5000, train_steps:2490, train_loss:0.9959, trina_acc:0.8467, val_loss:1.0386, val_acc:0.8257\n",
            "epoch:92/5000, train_steps:2500, train_loss:1.0025, trina_acc:0.8391, val_loss:1.0360, val_acc:0.8231\n",
            "epoch:92/5000, train_steps:2510, train_loss:1.0023, trina_acc:0.8367, val_loss:1.0386, val_acc:0.8198\n",
            "epoch:93/5000, train_steps:2520, train_loss:1.0002, trina_acc:0.8425, val_loss:1.0355, val_acc:0.8272\n",
            "epoch:93/5000, train_steps:2530, train_loss:0.9949, trina_acc:0.8503, val_loss:1.0367, val_acc:0.8231\n",
            "epoch:94/5000, train_steps:2540, train_loss:1.0022, trina_acc:0.8401, val_loss:1.0380, val_acc:0.8254\n",
            "epoch:94/5000, train_steps:2550, train_loss:0.9923, trina_acc:0.8484, val_loss:1.0357, val_acc:0.8255\n",
            "epoch:94/5000, train_steps:2560, train_loss:0.9951, trina_acc:0.8469, val_loss:1.0371, val_acc:0.8230\n",
            "epoch:95/5000, train_steps:2570, train_loss:0.9873, trina_acc:0.8538, val_loss:1.0377, val_acc:0.8283\n",
            "epoch:95/5000, train_steps:2580, train_loss:1.0066, trina_acc:0.8354, val_loss:1.0341, val_acc:0.8282\n",
            "epoch:95/5000, train_steps:2590, train_loss:0.9975, trina_acc:0.8477, val_loss:1.0340, val_acc:0.8281\n",
            "epoch:96/5000, train_steps:2600, train_loss:0.9980, trina_acc:0.8386, val_loss:1.0360, val_acc:0.8280\n",
            "epoch:96/5000, train_steps:2610, train_loss:1.0041, trina_acc:0.8345, val_loss:1.0340, val_acc:0.8277\n",
            "epoch:97/5000, train_steps:2620, train_loss:0.9868, trina_acc:0.8484, val_loss:1.0409, val_acc:0.8218\n",
            "epoch:97/5000, train_steps:2630, train_loss:0.9994, trina_acc:0.8452, val_loss:1.0347, val_acc:0.8259\n",
            "epoch:97/5000, train_steps:2640, train_loss:1.0073, trina_acc:0.8459, val_loss:1.0364, val_acc:0.8253\n",
            "epoch:98/5000, train_steps:2650, train_loss:0.9988, trina_acc:0.8494, val_loss:1.0384, val_acc:0.8273\n",
            "epoch:98/5000, train_steps:2660, train_loss:1.0044, trina_acc:0.8386, val_loss:1.0342, val_acc:0.8276\n",
            "epoch:98/5000, train_steps:2670, train_loss:1.0000, trina_acc:0.8342, val_loss:1.0346, val_acc:0.8264\n",
            "epoch:99/5000, train_steps:2680, train_loss:0.9866, trina_acc:0.8542, val_loss:1.0379, val_acc:0.8268\n",
            "epoch:99/5000, train_steps:2690, train_loss:1.0060, trina_acc:0.8318, val_loss:1.0379, val_acc:0.8211\n",
            "epoch:99/5000, train_steps:2700, train_loss:0.9927, trina_acc:0.8491, val_loss:1.0377, val_acc:0.8267\n",
            "epoch:100/5000, train_steps:2710, train_loss:0.9868, trina_acc:0.8479, val_loss:1.0369, val_acc:0.8215\n",
            "epoch:100/5000, train_steps:2720, train_loss:1.0152, trina_acc:0.8291, val_loss:1.0364, val_acc:0.8234\n",
            "epoch:101/5000, train_steps:2730, train_loss:0.9904, trina_acc:0.8423, val_loss:1.0376, val_acc:0.8278\n",
            "epoch:101/5000, train_steps:2740, train_loss:0.9950, trina_acc:0.8418, val_loss:1.0359, val_acc:0.8235\n",
            "epoch:101/5000, train_steps:2750, train_loss:1.0060, trina_acc:0.8423, val_loss:1.0337, val_acc:0.8293\n",
            "epoch:102/5000, train_steps:2760, train_loss:0.9939, trina_acc:0.8420, val_loss:1.0405, val_acc:0.8245\n",
            "epoch:102/5000, train_steps:2770, train_loss:1.0055, trina_acc:0.8342, val_loss:1.0371, val_acc:0.8241\n",
            "epoch:102/5000, train_steps:2780, train_loss:0.9901, trina_acc:0.8496, val_loss:1.0343, val_acc:0.8282\n",
            "epoch:103/5000, train_steps:2790, train_loss:1.0016, trina_acc:0.8398, val_loss:1.0353, val_acc:0.8286\n",
            "epoch:103/5000, train_steps:2800, train_loss:1.0066, trina_acc:0.8445, val_loss:1.0344, val_acc:0.8282\n",
            "epoch:104/5000, train_steps:2810, train_loss:0.9919, trina_acc:0.8445, val_loss:1.0363, val_acc:0.8292\n",
            "epoch:104/5000, train_steps:2820, train_loss:0.9959, trina_acc:0.8398, val_loss:1.0370, val_acc:0.8250\n",
            "epoch:104/5000, train_steps:2830, train_loss:1.0084, trina_acc:0.8396, val_loss:1.0348, val_acc:0.8269\n",
            "epoch:105/5000, train_steps:2840, train_loss:1.0023, trina_acc:0.8440, val_loss:1.0405, val_acc:0.8225\n",
            "epoch:105/5000, train_steps:2850, train_loss:1.0127, trina_acc:0.8328, val_loss:1.0350, val_acc:0.8278\n",
            "epoch:105/5000, train_steps:2860, train_loss:1.0104, trina_acc:0.8347, val_loss:1.0372, val_acc:0.8198\n",
            "epoch:106/5000, train_steps:2870, train_loss:1.0046, trina_acc:0.8396, val_loss:1.0376, val_acc:0.8269\n",
            "epoch:106/5000, train_steps:2880, train_loss:0.9943, trina_acc:0.8420, val_loss:1.0340, val_acc:0.8293\n",
            "epoch:107/5000, train_steps:2890, train_loss:0.9999, trina_acc:0.8486, val_loss:1.0356, val_acc:0.8296\n",
            "epoch:107/5000, train_steps:2900, train_loss:0.9880, trina_acc:0.8484, val_loss:1.0359, val_acc:0.8254\n",
            "epoch:107/5000, train_steps:2910, train_loss:1.0116, trina_acc:0.8406, val_loss:1.0352, val_acc:0.8261\n",
            "epoch:108/5000, train_steps:2920, train_loss:1.0054, trina_acc:0.8335, val_loss:1.0398, val_acc:0.8292\n",
            "epoch:108/5000, train_steps:2930, train_loss:1.0160, trina_acc:0.8369, val_loss:1.0353, val_acc:0.8265\n",
            "epoch:108/5000, train_steps:2940, train_loss:1.0080, trina_acc:0.8403, val_loss:1.0353, val_acc:0.8291\n",
            "epoch:109/5000, train_steps:2950, train_loss:1.0045, trina_acc:0.8340, val_loss:1.0370, val_acc:0.8285\n",
            "epoch:109/5000, train_steps:2960, train_loss:0.9991, trina_acc:0.8345, val_loss:1.0362, val_acc:0.8239\n",
            "epoch:109/5000, train_steps:2970, train_loss:0.9907, trina_acc:0.8531, val_loss:1.0371, val_acc:0.8250\n",
            "epoch:110/5000, train_steps:2980, train_loss:0.9978, trina_acc:0.8442, val_loss:1.0355, val_acc:0.8266\n",
            "epoch:110/5000, train_steps:2990, train_loss:1.0033, trina_acc:0.8435, val_loss:1.0378, val_acc:0.8213\n",
            "epoch:111/5000, train_steps:3000, train_loss:0.9936, trina_acc:0.8501, val_loss:1.0402, val_acc:0.8249\n",
            "epoch:111/5000, train_steps:3010, train_loss:1.0060, trina_acc:0.8433, val_loss:1.0342, val_acc:0.8258\n",
            "epoch:111/5000, train_steps:3020, train_loss:0.9913, trina_acc:0.8425, val_loss:1.0376, val_acc:0.8197\n",
            "epoch:112/5000, train_steps:3030, train_loss:0.9997, trina_acc:0.8416, val_loss:1.0382, val_acc:0.8269\n",
            "epoch:112/5000, train_steps:3040, train_loss:0.9914, trina_acc:0.8469, val_loss:1.0349, val_acc:0.8223\n",
            "epoch:112/5000, train_steps:3050, train_loss:0.9969, trina_acc:0.8384, val_loss:1.0352, val_acc:0.8230\n",
            "epoch:113/5000, train_steps:3060, train_loss:0.9896, trina_acc:0.8472, val_loss:1.0342, val_acc:0.8257\n",
            "epoch:113/5000, train_steps:3070, train_loss:1.0112, trina_acc:0.8328, val_loss:1.0327, val_acc:0.8263\n",
            "epoch:114/5000, train_steps:3080, train_loss:1.0025, trina_acc:0.8416, val_loss:1.0380, val_acc:0.8257\n",
            "epoch:114/5000, train_steps:3090, train_loss:0.9935, trina_acc:0.8442, val_loss:1.0343, val_acc:0.8245\n",
            "epoch:114/5000, train_steps:3100, train_loss:0.9953, trina_acc:0.8455, val_loss:1.0341, val_acc:0.8252\n",
            "epoch:115/5000, train_steps:3110, train_loss:0.9959, trina_acc:0.8467, val_loss:1.0365, val_acc:0.8295\n",
            "epoch:115/5000, train_steps:3120, train_loss:1.0066, trina_acc:0.8367, val_loss:1.0339, val_acc:0.8226\n",
            "epoch:115/5000, train_steps:3130, train_loss:0.9978, trina_acc:0.8391, val_loss:1.0361, val_acc:0.8232\n",
            "epoch:116/5000, train_steps:3140, train_loss:0.9973, trina_acc:0.8430, val_loss:1.0366, val_acc:0.8248\n",
            "epoch:116/5000, train_steps:3150, train_loss:1.0004, trina_acc:0.8391, val_loss:1.0330, val_acc:0.8254\n",
            "epoch:117/5000, train_steps:3160, train_loss:0.9870, trina_acc:0.8506, val_loss:1.0356, val_acc:0.8279\n",
            "epoch:117/5000, train_steps:3170, train_loss:0.9910, trina_acc:0.8450, val_loss:1.0338, val_acc:0.8257\n",
            "epoch:117/5000, train_steps:3180, train_loss:0.9924, trina_acc:0.8430, val_loss:1.0360, val_acc:0.8253\n",
            "epoch:118/5000, train_steps:3190, train_loss:0.9924, trina_acc:0.8538, val_loss:1.0378, val_acc:0.8254\n",
            "epoch:118/5000, train_steps:3200, train_loss:0.9979, trina_acc:0.8367, val_loss:1.0359, val_acc:0.8227\n",
            "epoch:118/5000, train_steps:3210, train_loss:0.9986, trina_acc:0.8477, val_loss:1.0342, val_acc:0.8253\n",
            "epoch:119/5000, train_steps:3220, train_loss:0.9997, trina_acc:0.8501, val_loss:1.0382, val_acc:0.8205\n",
            "epoch:119/5000, train_steps:3230, train_loss:0.9951, trina_acc:0.8394, val_loss:1.0340, val_acc:0.8253\n",
            "epoch:119/5000, train_steps:3240, train_loss:1.0036, trina_acc:0.8391, val_loss:1.0343, val_acc:0.8287\n",
            "epoch:120/5000, train_steps:3250, train_loss:0.9890, trina_acc:0.8552, val_loss:1.0338, val_acc:0.8277\n",
            "epoch:120/5000, train_steps:3260, train_loss:0.9981, trina_acc:0.8457, val_loss:1.0329, val_acc:0.8281\n",
            "epoch:121/5000, train_steps:3270, train_loss:0.9859, trina_acc:0.8555, val_loss:1.0358, val_acc:0.8287\n",
            "epoch:121/5000, train_steps:3280, train_loss:0.9976, trina_acc:0.8459, val_loss:1.0346, val_acc:0.8247\n",
            "epoch:121/5000, train_steps:3290, train_loss:0.9822, trina_acc:0.8486, val_loss:1.0334, val_acc:0.8252\n",
            "epoch:122/5000, train_steps:3300, train_loss:0.9857, trina_acc:0.8528, val_loss:1.0376, val_acc:0.8252\n",
            "epoch:122/5000, train_steps:3310, train_loss:0.9912, trina_acc:0.8501, val_loss:1.0338, val_acc:0.8271\n",
            "epoch:122/5000, train_steps:3320, train_loss:1.0127, trina_acc:0.8308, val_loss:1.0329, val_acc:0.8269\n",
            "epoch:123/5000, train_steps:3330, train_loss:1.0047, trina_acc:0.8455, val_loss:1.0340, val_acc:0.8287\n",
            "epoch:123/5000, train_steps:3340, train_loss:0.9932, trina_acc:0.8418, val_loss:1.0320, val_acc:0.8280\n",
            "epoch:124/5000, train_steps:3350, train_loss:0.9939, trina_acc:0.8484, val_loss:1.0358, val_acc:0.8285\n",
            "epoch:124/5000, train_steps:3360, train_loss:0.9946, trina_acc:0.8477, val_loss:1.0329, val_acc:0.8271\n",
            "epoch:124/5000, train_steps:3370, train_loss:1.0001, trina_acc:0.8420, val_loss:1.0358, val_acc:0.8218\n",
            "epoch:125/5000, train_steps:3380, train_loss:0.9991, trina_acc:0.8491, val_loss:1.0362, val_acc:0.8284\n",
            "epoch:125/5000, train_steps:3390, train_loss:0.9863, trina_acc:0.8491, val_loss:1.0352, val_acc:0.8277\n",
            "epoch:125/5000, train_steps:3400, train_loss:1.0012, trina_acc:0.8350, val_loss:1.0340, val_acc:0.8233\n",
            "epoch:126/5000, train_steps:3410, train_loss:0.9982, trina_acc:0.8418, val_loss:1.0340, val_acc:0.8282\n",
            "epoch:126/5000, train_steps:3420, train_loss:0.9926, trina_acc:0.8408, val_loss:1.0332, val_acc:0.8288\n",
            "epoch:127/5000, train_steps:3430, train_loss:0.9886, trina_acc:0.8567, val_loss:1.0334, val_acc:0.8294\n",
            "epoch:127/5000, train_steps:3440, train_loss:1.0015, trina_acc:0.8330, val_loss:1.0325, val_acc:0.8293\n",
            "epoch:127/5000, train_steps:3450, train_loss:1.0060, trina_acc:0.8359, val_loss:1.0331, val_acc:0.8290\n",
            "epoch:128/5000, train_steps:3460, train_loss:0.9854, trina_acc:0.8445, val_loss:1.0382, val_acc:0.8223\n",
            "epoch:128/5000, train_steps:3470, train_loss:0.9882, trina_acc:0.8442, val_loss:1.0328, val_acc:0.8295\n",
            "epoch:128/5000, train_steps:3480, train_loss:0.9947, trina_acc:0.8442, val_loss:1.0356, val_acc:0.8213\n",
            "epoch:129/5000, train_steps:3490, train_loss:0.9860, trina_acc:0.8533, val_loss:1.0361, val_acc:0.8266\n",
            "epoch:129/5000, train_steps:3500, train_loss:1.0017, trina_acc:0.8413, val_loss:1.0333, val_acc:0.8280\n",
            "epoch:129/5000, train_steps:3510, train_loss:0.9693, trina_acc:0.8610, val_loss:1.0327, val_acc:0.8273\n",
            "epoch:130/5000, train_steps:3520, train_loss:0.9900, trina_acc:0.8457, val_loss:1.0350, val_acc:0.8266\n",
            "epoch:130/5000, train_steps:3530, train_loss:1.0073, trina_acc:0.8425, val_loss:1.0337, val_acc:0.8297\n",
            "epoch:131/5000, train_steps:3540, train_loss:0.9991, trina_acc:0.8442, val_loss:1.0364, val_acc:0.8271\n",
            "epoch:131/5000, train_steps:3550, train_loss:0.9952, trina_acc:0.8428, val_loss:1.0361, val_acc:0.8215\n",
            "epoch:131/5000, train_steps:3560, train_loss:1.0036, trina_acc:0.8411, val_loss:1.0339, val_acc:0.8259\n",
            "epoch:132/5000, train_steps:3570, train_loss:0.9942, trina_acc:0.8413, val_loss:1.0362, val_acc:0.8285\n",
            "epoch:132/5000, train_steps:3580, train_loss:1.0047, trina_acc:0.8340, val_loss:1.0323, val_acc:0.8297\n",
            "epoch:132/5000, train_steps:3590, train_loss:1.0011, trina_acc:0.8420, val_loss:1.0326, val_acc:0.8282\n",
            "epoch:133/5000, train_steps:3600, train_loss:0.9976, trina_acc:0.8394, val_loss:1.0342, val_acc:0.8269\n",
            "epoch:133/5000, train_steps:3610, train_loss:0.9993, trina_acc:0.8477, val_loss:1.0330, val_acc:0.8293\n",
            "epoch:134/5000, train_steps:3620, train_loss:0.9827, trina_acc:0.8569, val_loss:1.0360, val_acc:0.8275\n",
            "epoch:134/5000, train_steps:3630, train_loss:0.9961, trina_acc:0.8459, val_loss:1.0325, val_acc:0.8284\n",
            "epoch:134/5000, train_steps:3640, train_loss:1.0017, trina_acc:0.8391, val_loss:1.0329, val_acc:0.8288\n",
            "epoch:135/5000, train_steps:3650, train_loss:0.9875, trina_acc:0.8503, val_loss:1.0371, val_acc:0.8256\n",
            "epoch:135/5000, train_steps:3660, train_loss:0.9937, trina_acc:0.8450, val_loss:1.0331, val_acc:0.8278\n",
            "epoch:135/5000, train_steps:3670, train_loss:1.0103, trina_acc:0.8311, val_loss:1.0330, val_acc:0.8291\n",
            "epoch:136/5000, train_steps:3680, train_loss:1.0018, trina_acc:0.8337, val_loss:1.0353, val_acc:0.8264\n",
            "epoch:136/5000, train_steps:3690, train_loss:0.9980, trina_acc:0.8455, val_loss:1.0342, val_acc:0.8257\n",
            "epoch:137/5000, train_steps:3700, train_loss:0.9840, trina_acc:0.8506, val_loss:1.0336, val_acc:0.8282\n",
            "epoch:137/5000, train_steps:3710, train_loss:1.0187, trina_acc:0.8232, val_loss:1.0338, val_acc:0.8254\n",
            "epoch:137/5000, train_steps:3720, train_loss:0.9974, trina_acc:0.8391, val_loss:1.0347, val_acc:0.8260\n",
            "epoch:138/5000, train_steps:3730, train_loss:1.0040, trina_acc:0.8411, val_loss:1.0368, val_acc:0.8272\n",
            "epoch:138/5000, train_steps:3740, train_loss:1.0000, trina_acc:0.8403, val_loss:1.0330, val_acc:0.8273\n",
            "epoch:138/5000, train_steps:3750, train_loss:0.9835, trina_acc:0.8447, val_loss:1.0342, val_acc:0.8288\n",
            "epoch:139/5000, train_steps:3760, train_loss:1.0011, trina_acc:0.8372, val_loss:1.0375, val_acc:0.8232\n",
            "epoch:139/5000, train_steps:3770, train_loss:0.9989, trina_acc:0.8386, val_loss:1.0349, val_acc:0.8220\n",
            "epoch:139/5000, train_steps:3780, train_loss:0.9958, trina_acc:0.8477, val_loss:1.0394, val_acc:0.8248\n",
            "epoch:140/5000, train_steps:3790, train_loss:0.9938, trina_acc:0.8418, val_loss:1.0368, val_acc:0.8180\n",
            "epoch:140/5000, train_steps:3800, train_loss:0.9998, trina_acc:0.8345, val_loss:1.0341, val_acc:0.8256\n",
            "epoch:141/5000, train_steps:3810, train_loss:0.9871, trina_acc:0.8486, val_loss:1.0397, val_acc:0.8277\n",
            "epoch:141/5000, train_steps:3820, train_loss:1.0067, trina_acc:0.8318, val_loss:1.0352, val_acc:0.8223\n",
            "epoch:141/5000, train_steps:3830, train_loss:0.9851, trina_acc:0.8457, val_loss:1.0357, val_acc:0.8231\n",
            "epoch:142/5000, train_steps:3840, train_loss:0.9928, trina_acc:0.8438, val_loss:1.0363, val_acc:0.8281\n",
            "epoch:142/5000, train_steps:3850, train_loss:1.0114, trina_acc:0.8381, val_loss:1.0326, val_acc:0.8295\n",
            "epoch:142/5000, train_steps:3860, train_loss:1.0044, trina_acc:0.8408, val_loss:1.0329, val_acc:0.8278\n",
            "epoch:143/5000, train_steps:3870, train_loss:0.9825, trina_acc:0.8508, val_loss:1.0348, val_acc:0.8273\n",
            "epoch:143/5000, train_steps:3880, train_loss:1.0005, trina_acc:0.8323, val_loss:1.0335, val_acc:0.8252\n",
            "epoch:144/5000, train_steps:3890, train_loss:0.9909, trina_acc:0.8467, val_loss:1.0354, val_acc:0.8282\n",
            "epoch:144/5000, train_steps:3900, train_loss:0.9998, trina_acc:0.8489, val_loss:1.0331, val_acc:0.8274\n",
            "epoch:144/5000, train_steps:3910, train_loss:0.9944, trina_acc:0.8413, val_loss:1.0329, val_acc:0.8261\n",
            "epoch:145/5000, train_steps:3920, train_loss:0.9940, trina_acc:0.8474, val_loss:1.0366, val_acc:0.8284\n",
            "epoch:145/5000, train_steps:3930, train_loss:0.9909, trina_acc:0.8474, val_loss:1.0334, val_acc:0.8282\n",
            "epoch:145/5000, train_steps:3940, train_loss:1.0103, trina_acc:0.8398, val_loss:1.0317, val_acc:0.8295\n",
            "epoch:146/5000, train_steps:3950, train_loss:1.0001, trina_acc:0.8440, val_loss:1.0349, val_acc:0.8290\n",
            "epoch:146/5000, train_steps:3960, train_loss:1.0062, trina_acc:0.8354, val_loss:1.0322, val_acc:0.8304\n",
            "epoch:147/5000, train_steps:3970, train_loss:0.9938, trina_acc:0.8525, val_loss:1.0336, val_acc:0.8300\n",
            "epoch:147/5000, train_steps:3980, train_loss:0.9993, trina_acc:0.8398, val_loss:1.0341, val_acc:0.8244\n",
            "epoch:147/5000, train_steps:3990, train_loss:0.9909, trina_acc:0.8438, val_loss:1.0343, val_acc:0.8255\n",
            "epoch:148/5000, train_steps:4000, train_loss:0.9861, trina_acc:0.8557, val_loss:1.0361, val_acc:0.8283\n",
            "epoch:148/5000, train_steps:4010, train_loss:0.9867, trina_acc:0.8401, val_loss:1.0312, val_acc:0.8294\n",
            "epoch:148/5000, train_steps:4020, train_loss:0.9887, trina_acc:0.8445, val_loss:1.0310, val_acc:0.8292\n",
            "epoch:149/5000, train_steps:4030, train_loss:1.0004, trina_acc:0.8442, val_loss:1.0344, val_acc:0.8283\n",
            "epoch:149/5000, train_steps:4040, train_loss:0.9917, trina_acc:0.8438, val_loss:1.0315, val_acc:0.8297\n",
            "epoch:149/5000, train_steps:4050, train_loss:1.0063, trina_acc:0.8391, val_loss:1.0343, val_acc:0.8207\n",
            "epoch:150/5000, train_steps:4060, train_loss:0.9831, trina_acc:0.8538, val_loss:1.0347, val_acc:0.8211\n",
            "epoch:150/5000, train_steps:4070, train_loss:0.9839, trina_acc:0.8486, val_loss:1.0328, val_acc:0.8263\n",
            "epoch:151/5000, train_steps:4080, train_loss:0.9862, trina_acc:0.8462, val_loss:1.0367, val_acc:0.8283\n",
            "epoch:151/5000, train_steps:4090, train_loss:0.9913, trina_acc:0.8542, val_loss:1.0309, val_acc:0.8288\n",
            "epoch:151/5000, train_steps:4100, train_loss:1.0035, trina_acc:0.8430, val_loss:1.0331, val_acc:0.8231\n",
            "epoch:152/5000, train_steps:4110, train_loss:0.9970, trina_acc:0.8484, val_loss:1.0353, val_acc:0.8264\n",
            "epoch:152/5000, train_steps:4120, train_loss:0.9967, trina_acc:0.8428, val_loss:1.0314, val_acc:0.8293\n",
            "epoch:152/5000, train_steps:4130, train_loss:0.9928, trina_acc:0.8452, val_loss:1.0313, val_acc:0.8286\n",
            "epoch:153/5000, train_steps:4140, train_loss:0.9987, trina_acc:0.8430, val_loss:1.0332, val_acc:0.8284\n",
            "epoch:153/5000, train_steps:4150, train_loss:1.0064, trina_acc:0.8342, val_loss:1.0305, val_acc:0.8296\n",
            "epoch:154/5000, train_steps:4160, train_loss:0.9936, trina_acc:0.8481, val_loss:1.0333, val_acc:0.8282\n",
            "epoch:154/5000, train_steps:4170, train_loss:0.9871, trina_acc:0.8499, val_loss:1.0321, val_acc:0.8284\n",
            "epoch:154/5000, train_steps:4180, train_loss:0.9810, trina_acc:0.8506, val_loss:1.0329, val_acc:0.8262\n",
            "epoch:155/5000, train_steps:4190, train_loss:1.0022, trina_acc:0.8369, val_loss:1.0342, val_acc:0.8289\n",
            "epoch:155/5000, train_steps:4200, train_loss:1.0081, trina_acc:0.8337, val_loss:1.0313, val_acc:0.8260\n",
            "epoch:155/5000, train_steps:4210, train_loss:1.0072, trina_acc:0.8389, val_loss:1.0311, val_acc:0.8272\n",
            "epoch:156/5000, train_steps:4220, train_loss:0.9991, trina_acc:0.8442, val_loss:1.0331, val_acc:0.8302\n",
            "epoch:156/5000, train_steps:4230, train_loss:0.9956, trina_acc:0.8403, val_loss:1.0314, val_acc:0.8279\n",
            "epoch:157/5000, train_steps:4240, train_loss:0.9937, trina_acc:0.8445, val_loss:1.0332, val_acc:0.8266\n",
            "epoch:157/5000, train_steps:4250, train_loss:0.9937, trina_acc:0.8418, val_loss:1.0315, val_acc:0.8293\n",
            "epoch:157/5000, train_steps:4260, train_loss:0.9982, trina_acc:0.8337, val_loss:1.0308, val_acc:0.8294\n",
            "epoch:158/5000, train_steps:4270, train_loss:0.9917, trina_acc:0.8481, val_loss:1.0355, val_acc:0.8274\n",
            "epoch:158/5000, train_steps:4280, train_loss:0.9977, trina_acc:0.8408, val_loss:1.0319, val_acc:0.8263\n",
            "epoch:158/5000, train_steps:4290, train_loss:0.9952, trina_acc:0.8418, val_loss:1.0313, val_acc:0.8273\n",
            "epoch:159/5000, train_steps:4300, train_loss:0.9925, trina_acc:0.8418, val_loss:1.0350, val_acc:0.8263\n",
            "epoch:159/5000, train_steps:4310, train_loss:1.0063, trina_acc:0.8345, val_loss:1.0317, val_acc:0.8273\n",
            "epoch:159/5000, train_steps:4320, train_loss:0.9954, trina_acc:0.8391, val_loss:1.0328, val_acc:0.8252\n",
            "epoch:160/5000, train_steps:4330, train_loss:0.9923, trina_acc:0.8462, val_loss:1.0329, val_acc:0.8279\n",
            "epoch:160/5000, train_steps:4340, train_loss:0.9980, trina_acc:0.8411, val_loss:1.0337, val_acc:0.8210\n",
            "epoch:161/5000, train_steps:4350, train_loss:0.9807, trina_acc:0.8557, val_loss:1.0341, val_acc:0.8266\n",
            "epoch:161/5000, train_steps:4360, train_loss:0.9834, trina_acc:0.8518, val_loss:1.0318, val_acc:0.8291\n",
            "epoch:161/5000, train_steps:4370, train_loss:1.0080, trina_acc:0.8403, val_loss:1.0307, val_acc:0.8300\n",
            "epoch:162/5000, train_steps:4380, train_loss:0.9814, trina_acc:0.8577, val_loss:1.0331, val_acc:0.8292\n",
            "epoch:162/5000, train_steps:4390, train_loss:0.9903, trina_acc:0.8479, val_loss:1.0314, val_acc:0.8278\n",
            "epoch:162/5000, train_steps:4400, train_loss:1.0069, trina_acc:0.8359, val_loss:1.0315, val_acc:0.8294\n",
            "epoch:163/5000, train_steps:4410, train_loss:0.9841, trina_acc:0.8518, val_loss:1.0317, val_acc:0.8298\n",
            "epoch:163/5000, train_steps:4420, train_loss:0.9845, trina_acc:0.8494, val_loss:1.0309, val_acc:0.8282\n",
            "epoch:164/5000, train_steps:4430, train_loss:0.9990, trina_acc:0.8396, val_loss:1.0329, val_acc:0.8291\n",
            "epoch:164/5000, train_steps:4440, train_loss:0.9944, trina_acc:0.8464, val_loss:1.0319, val_acc:0.8282\n",
            "epoch:164/5000, train_steps:4450, train_loss:0.9976, trina_acc:0.8391, val_loss:1.0303, val_acc:0.8296\n",
            "epoch:165/5000, train_steps:4460, train_loss:0.9956, trina_acc:0.8403, val_loss:1.0342, val_acc:0.8283\n",
            "epoch:165/5000, train_steps:4470, train_loss:1.0021, trina_acc:0.8396, val_loss:1.0303, val_acc:0.8284\n",
            "epoch:165/5000, train_steps:4480, train_loss:1.0040, trina_acc:0.8352, val_loss:1.0308, val_acc:0.8287\n",
            "epoch:166/5000, train_steps:4490, train_loss:0.9925, trina_acc:0.8508, val_loss:1.0347, val_acc:0.8257\n",
            "epoch:166/5000, train_steps:4500, train_loss:1.0070, trina_acc:0.8311, val_loss:1.0301, val_acc:0.8286\n",
            "epoch:167/5000, train_steps:4510, train_loss:0.9817, trina_acc:0.8518, val_loss:1.0335, val_acc:0.8250\n",
            "epoch:167/5000, train_steps:4520, train_loss:0.9822, trina_acc:0.8525, val_loss:1.0311, val_acc:0.8295\n",
            "epoch:167/5000, train_steps:4530, train_loss:1.0056, trina_acc:0.8384, val_loss:1.0307, val_acc:0.8288\n",
            "epoch:168/5000, train_steps:4540, train_loss:0.9871, trina_acc:0.8528, val_loss:1.0340, val_acc:0.8295\n",
            "epoch:168/5000, train_steps:4550, train_loss:0.9920, trina_acc:0.8423, val_loss:1.0324, val_acc:0.8283\n",
            "epoch:168/5000, train_steps:4560, train_loss:1.0048, trina_acc:0.8423, val_loss:1.0350, val_acc:0.8191\n",
            "epoch:169/5000, train_steps:4570, train_loss:0.9910, trina_acc:0.8477, val_loss:1.0352, val_acc:0.8271\n",
            "epoch:169/5000, train_steps:4580, train_loss:0.9962, trina_acc:0.8469, val_loss:1.0321, val_acc:0.8275\n",
            "epoch:169/5000, train_steps:4590, train_loss:1.0098, trina_acc:0.8471, val_loss:1.0311, val_acc:0.8298\n",
            "epoch:170/5000, train_steps:4600, train_loss:0.9936, trina_acc:0.8491, val_loss:1.0332, val_acc:0.8267\n",
            "epoch:170/5000, train_steps:4610, train_loss:0.9981, trina_acc:0.8438, val_loss:1.0308, val_acc:0.8297\n",
            "epoch:171/5000, train_steps:4620, train_loss:0.9977, trina_acc:0.8413, val_loss:1.0355, val_acc:0.8245\n",
            "epoch:171/5000, train_steps:4630, train_loss:0.9962, trina_acc:0.8425, val_loss:1.0322, val_acc:0.8247\n",
            "epoch:171/5000, train_steps:4640, train_loss:0.9920, trina_acc:0.8479, val_loss:1.0312, val_acc:0.8284\n",
            "epoch:172/5000, train_steps:4650, train_loss:0.9935, trina_acc:0.8491, val_loss:1.0350, val_acc:0.8292\n",
            "epoch:172/5000, train_steps:4660, train_loss:0.9905, trina_acc:0.8457, val_loss:1.0310, val_acc:0.8269\n",
            "epoch:172/5000, train_steps:4670, train_loss:1.0045, trina_acc:0.8357, val_loss:1.0312, val_acc:0.8287\n",
            "epoch:173/5000, train_steps:4680, train_loss:1.0000, trina_acc:0.8357, val_loss:1.0321, val_acc:0.8280\n",
            "epoch:173/5000, train_steps:4690, train_loss:0.9956, trina_acc:0.8430, val_loss:1.0302, val_acc:0.8301\n",
            "epoch:174/5000, train_steps:4700, train_loss:0.9926, trina_acc:0.8486, val_loss:1.0336, val_acc:0.8284\n",
            "epoch:174/5000, train_steps:4710, train_loss:0.9954, trina_acc:0.8401, val_loss:1.0308, val_acc:0.8291\n",
            "epoch:174/5000, train_steps:4720, train_loss:1.0183, trina_acc:0.8301, val_loss:1.0333, val_acc:0.8260\n",
            "epoch:175/5000, train_steps:4730, train_loss:0.9846, trina_acc:0.8416, val_loss:1.0346, val_acc:0.8287\n",
            "epoch:175/5000, train_steps:4740, train_loss:1.0058, trina_acc:0.8345, val_loss:1.0315, val_acc:0.8275\n",
            "epoch:175/5000, train_steps:4750, train_loss:0.9817, trina_acc:0.8503, val_loss:1.0302, val_acc:0.8293\n",
            "epoch:176/5000, train_steps:4760, train_loss:0.9934, trina_acc:0.8489, val_loss:1.0340, val_acc:0.8276\n",
            "epoch:176/5000, train_steps:4770, train_loss:1.0006, trina_acc:0.8389, val_loss:1.0304, val_acc:0.8290\n",
            "epoch:177/5000, train_steps:4780, train_loss:0.9901, trina_acc:0.8464, val_loss:1.0319, val_acc:0.8292\n",
            "epoch:177/5000, train_steps:4790, train_loss:1.0059, trina_acc:0.8394, val_loss:1.0324, val_acc:0.8279\n",
            "epoch:177/5000, train_steps:4800, train_loss:1.0016, trina_acc:0.8477, val_loss:1.0336, val_acc:0.8239\n",
            "epoch:178/5000, train_steps:4810, train_loss:1.0033, trina_acc:0.8440, val_loss:1.0342, val_acc:0.8292\n",
            "epoch:178/5000, train_steps:4820, train_loss:0.9791, trina_acc:0.8540, val_loss:1.0300, val_acc:0.8296\n",
            "epoch:178/5000, train_steps:4830, train_loss:0.9873, trina_acc:0.8503, val_loss:1.0311, val_acc:0.8290\n",
            "epoch:179/5000, train_steps:4840, train_loss:0.9895, trina_acc:0.8506, val_loss:1.0347, val_acc:0.8274\n",
            "epoch:179/5000, train_steps:4850, train_loss:0.9872, trina_acc:0.8372, val_loss:1.0319, val_acc:0.8281\n",
            "epoch:179/5000, train_steps:4860, train_loss:0.9955, trina_acc:0.8438, val_loss:1.0319, val_acc:0.8262\n",
            "epoch:180/5000, train_steps:4870, train_loss:0.9926, trina_acc:0.8469, val_loss:1.0322, val_acc:0.8303\n",
            "epoch:180/5000, train_steps:4880, train_loss:0.9930, trina_acc:0.8394, val_loss:1.0316, val_acc:0.8280\n",
            "epoch:181/5000, train_steps:4890, train_loss:0.9844, trina_acc:0.8474, val_loss:1.0334, val_acc:0.8303\n",
            "epoch:181/5000, train_steps:4900, train_loss:1.0149, trina_acc:0.8267, val_loss:1.0306, val_acc:0.8278\n",
            "epoch:181/5000, train_steps:4910, train_loss:1.0035, trina_acc:0.8372, val_loss:1.0348, val_acc:0.8279\n",
            "epoch:182/5000, train_steps:4920, train_loss:0.9939, trina_acc:0.8511, val_loss:1.0346, val_acc:0.8273\n",
            "epoch:182/5000, train_steps:4930, train_loss:1.0031, trina_acc:0.8462, val_loss:1.0306, val_acc:0.8290\n",
            "epoch:182/5000, train_steps:4940, train_loss:0.9872, trina_acc:0.8440, val_loss:1.0316, val_acc:0.8286\n",
            "epoch:183/5000, train_steps:4950, train_loss:0.9974, trina_acc:0.8428, val_loss:1.0329, val_acc:0.8278\n",
            "epoch:183/5000, train_steps:4960, train_loss:0.9942, trina_acc:0.8418, val_loss:1.0330, val_acc:0.8271\n",
            "epoch:184/5000, train_steps:4970, train_loss:0.9920, trina_acc:0.8472, val_loss:1.0329, val_acc:0.8295\n",
            "epoch:184/5000, train_steps:4980, train_loss:0.9920, trina_acc:0.8379, val_loss:1.0319, val_acc:0.8268\n",
            "epoch:184/5000, train_steps:4990, train_loss:0.9938, trina_acc:0.8416, val_loss:1.0309, val_acc:0.8302\n",
            "epoch:185/5000, train_steps:5000, train_loss:0.9916, trina_acc:0.8562, val_loss:1.0343, val_acc:0.8298\n",
            "epoch:185/5000, train_steps:5010, train_loss:0.9937, trina_acc:0.8328, val_loss:1.0303, val_acc:0.8252\n",
            "epoch:185/5000, train_steps:5020, train_loss:1.0091, trina_acc:0.8276, val_loss:1.0298, val_acc:0.8282\n",
            "epoch:186/5000, train_steps:5030, train_loss:0.9974, trina_acc:0.8416, val_loss:1.0309, val_acc:0.8294\n",
            "epoch:186/5000, train_steps:5040, train_loss:0.9963, trina_acc:0.8445, val_loss:1.0289, val_acc:0.8291\n",
            "epoch:187/5000, train_steps:5050, train_loss:0.9825, trina_acc:0.8518, val_loss:1.0329, val_acc:0.8229\n",
            "epoch:187/5000, train_steps:5060, train_loss:0.9946, trina_acc:0.8403, val_loss:1.0319, val_acc:0.8259\n",
            "epoch:187/5000, train_steps:5070, train_loss:1.0015, trina_acc:0.8413, val_loss:1.0318, val_acc:0.8238\n",
            "epoch:188/5000, train_steps:5080, train_loss:1.0114, trina_acc:0.8369, val_loss:1.0324, val_acc:0.8279\n",
            "epoch:188/5000, train_steps:5090, train_loss:1.0114, trina_acc:0.8308, val_loss:1.0296, val_acc:0.8282\n",
            "epoch:188/5000, train_steps:5100, train_loss:0.9982, trina_acc:0.8386, val_loss:1.0335, val_acc:0.8252\n",
            "epoch:189/5000, train_steps:5110, train_loss:0.9990, trina_acc:0.8416, val_loss:1.0322, val_acc:0.8282\n",
            "epoch:189/5000, train_steps:5120, train_loss:1.0056, trina_acc:0.8462, val_loss:1.0306, val_acc:0.8241\n",
            "epoch:189/5000, train_steps:5130, train_loss:0.9927, trina_acc:0.8418, val_loss:1.0292, val_acc:0.8294\n",
            "epoch:190/5000, train_steps:5140, train_loss:0.9821, trina_acc:0.8496, val_loss:1.0315, val_acc:0.8300\n",
            "epoch:190/5000, train_steps:5150, train_loss:1.0104, trina_acc:0.8389, val_loss:1.0292, val_acc:0.8286\n",
            "epoch:191/5000, train_steps:5160, train_loss:0.9856, trina_acc:0.8472, val_loss:1.0326, val_acc:0.8281\n",
            "epoch:191/5000, train_steps:5170, train_loss:0.9927, trina_acc:0.8501, val_loss:1.0296, val_acc:0.8292\n",
            "epoch:191/5000, train_steps:5180, train_loss:0.9979, trina_acc:0.8401, val_loss:1.0303, val_acc:0.8278\n",
            "epoch:192/5000, train_steps:5190, train_loss:0.9937, trina_acc:0.8435, val_loss:1.0331, val_acc:0.8289\n",
            "epoch:192/5000, train_steps:5200, train_loss:0.9956, trina_acc:0.8464, val_loss:1.0308, val_acc:0.8262\n",
            "epoch:192/5000, train_steps:5210, train_loss:1.0031, trina_acc:0.8340, val_loss:1.0316, val_acc:0.8246\n",
            "epoch:193/5000, train_steps:5220, train_loss:0.9930, trina_acc:0.8450, val_loss:1.0308, val_acc:0.8282\n",
            "epoch:193/5000, train_steps:5230, train_loss:1.0115, trina_acc:0.8293, val_loss:1.0303, val_acc:0.8263\n",
            "epoch:194/5000, train_steps:5240, train_loss:0.9919, trina_acc:0.8464, val_loss:1.0354, val_acc:0.8248\n",
            "epoch:194/5000, train_steps:5250, train_loss:1.0024, trina_acc:0.8335, val_loss:1.0323, val_acc:0.8216\n",
            "epoch:194/5000, train_steps:5260, train_loss:0.9938, trina_acc:0.8455, val_loss:1.0312, val_acc:0.8242\n",
            "epoch:195/5000, train_steps:5270, train_loss:0.9949, trina_acc:0.8420, val_loss:1.0350, val_acc:0.8283\n",
            "epoch:195/5000, train_steps:5280, train_loss:0.9960, trina_acc:0.8462, val_loss:1.0302, val_acc:0.8282\n",
            "epoch:195/5000, train_steps:5290, train_loss:0.9915, trina_acc:0.8420, val_loss:1.0293, val_acc:0.8291\n",
            "epoch:196/5000, train_steps:5300, train_loss:0.9990, trina_acc:0.8435, val_loss:1.0334, val_acc:0.8248\n",
            "epoch:196/5000, train_steps:5310, train_loss:0.9961, trina_acc:0.8428, val_loss:1.0307, val_acc:0.8262\n",
            "epoch:197/5000, train_steps:5320, train_loss:0.9908, trina_acc:0.8467, val_loss:1.0304, val_acc:0.8296\n",
            "epoch:197/5000, train_steps:5330, train_loss:1.0010, trina_acc:0.8345, val_loss:1.0310, val_acc:0.8282\n",
            "epoch:197/5000, train_steps:5340, train_loss:0.9839, trina_acc:0.8555, val_loss:1.0297, val_acc:0.8288\n",
            "epoch:198/5000, train_steps:5350, train_loss:0.9971, trina_acc:0.8486, val_loss:1.0336, val_acc:0.8285\n",
            "epoch:198/5000, train_steps:5360, train_loss:1.0026, trina_acc:0.8362, val_loss:1.0291, val_acc:0.8280\n",
            "epoch:198/5000, train_steps:5370, train_loss:0.9931, trina_acc:0.8420, val_loss:1.0294, val_acc:0.8285\n",
            "epoch:199/5000, train_steps:5380, train_loss:1.0010, trina_acc:0.8418, val_loss:1.0328, val_acc:0.8280\n",
            "epoch:199/5000, train_steps:5390, train_loss:0.9988, trina_acc:0.8398, val_loss:1.0317, val_acc:0.8234\n",
            "epoch:199/5000, train_steps:5400, train_loss:1.0035, trina_acc:0.8424, val_loss:1.0302, val_acc:0.8291\n",
            "epoch:200/5000, train_steps:5410, train_loss:1.0045, trina_acc:0.8438, val_loss:1.0308, val_acc:0.8283\n",
            "epoch:200/5000, train_steps:5420, train_loss:0.9903, trina_acc:0.8440, val_loss:1.0295, val_acc:0.8292\n",
            "epoch:201/5000, train_steps:5430, train_loss:0.9896, trina_acc:0.8420, val_loss:1.0334, val_acc:0.8257\n",
            "epoch:201/5000, train_steps:5440, train_loss:0.9893, trina_acc:0.8484, val_loss:1.0315, val_acc:0.8257\n",
            "epoch:201/5000, train_steps:5450, train_loss:0.9909, trina_acc:0.8508, val_loss:1.0299, val_acc:0.8288\n",
            "epoch:202/5000, train_steps:5460, train_loss:1.0073, trina_acc:0.8318, val_loss:1.0325, val_acc:0.8288\n",
            "epoch:202/5000, train_steps:5470, train_loss:0.9858, trina_acc:0.8481, val_loss:1.0291, val_acc:0.8296\n",
            "epoch:202/5000, train_steps:5480, train_loss:0.9883, trina_acc:0.8562, val_loss:1.0295, val_acc:0.8282\n",
            "epoch:203/5000, train_steps:5490, train_loss:0.9832, trina_acc:0.8542, val_loss:1.0330, val_acc:0.8270\n",
            "epoch:203/5000, train_steps:5500, train_loss:0.9903, trina_acc:0.8435, val_loss:1.0283, val_acc:0.8298\n",
            "epoch:204/5000, train_steps:5510, train_loss:0.9957, trina_acc:0.8416, val_loss:1.0308, val_acc:0.8286\n",
            "epoch:204/5000, train_steps:5520, train_loss:1.0003, trina_acc:0.8408, val_loss:1.0298, val_acc:0.8292\n",
            "epoch:204/5000, train_steps:5530, train_loss:1.0059, trina_acc:0.8386, val_loss:1.0301, val_acc:0.8278\n",
            "epoch:205/5000, train_steps:5540, train_loss:0.9845, trina_acc:0.8503, val_loss:1.0340, val_acc:0.8266\n",
            "epoch:205/5000, train_steps:5550, train_loss:0.9994, trina_acc:0.8445, val_loss:1.0305, val_acc:0.8275\n",
            "epoch:205/5000, train_steps:5560, train_loss:1.0021, trina_acc:0.8384, val_loss:1.0305, val_acc:0.8295\n",
            "epoch:206/5000, train_steps:5570, train_loss:0.9964, trina_acc:0.8491, val_loss:1.0312, val_acc:0.8290\n",
            "epoch:206/5000, train_steps:5580, train_loss:0.9911, trina_acc:0.8452, val_loss:1.0296, val_acc:0.8275\n",
            "epoch:207/5000, train_steps:5590, train_loss:0.9892, trina_acc:0.8442, val_loss:1.0304, val_acc:0.8285\n",
            "epoch:207/5000, train_steps:5600, train_loss:0.9951, trina_acc:0.8418, val_loss:1.0302, val_acc:0.8289\n",
            "epoch:207/5000, train_steps:5610, train_loss:0.9784, trina_acc:0.8591, val_loss:1.0298, val_acc:0.8292\n",
            "epoch:208/5000, train_steps:5620, train_loss:0.9810, trina_acc:0.8560, val_loss:1.0327, val_acc:0.8290\n",
            "epoch:208/5000, train_steps:5630, train_loss:0.9999, trina_acc:0.8350, val_loss:1.0313, val_acc:0.8260\n",
            "epoch:208/5000, train_steps:5640, train_loss:0.9875, trina_acc:0.8445, val_loss:1.0312, val_acc:0.8253\n",
            "epoch:209/5000, train_steps:5650, train_loss:0.9766, trina_acc:0.8586, val_loss:1.0321, val_acc:0.8282\n",
            "epoch:209/5000, train_steps:5660, train_loss:0.9882, trina_acc:0.8430, val_loss:1.0316, val_acc:0.8275\n",
            "epoch:209/5000, train_steps:5670, train_loss:0.9824, trina_acc:0.8597, val_loss:1.0316, val_acc:0.8261\n",
            "epoch:210/5000, train_steps:5680, train_loss:1.0059, trina_acc:0.8333, val_loss:1.0348, val_acc:0.8184\n",
            "epoch:210/5000, train_steps:5690, train_loss:0.9940, trina_acc:0.8408, val_loss:1.0290, val_acc:0.8294\n",
            "epoch:211/5000, train_steps:5700, train_loss:0.9897, trina_acc:0.8513, val_loss:1.0324, val_acc:0.8294\n",
            "epoch:211/5000, train_steps:5710, train_loss:0.9817, trina_acc:0.8486, val_loss:1.0307, val_acc:0.8296\n",
            "epoch:211/5000, train_steps:5720, train_loss:0.9989, trina_acc:0.8438, val_loss:1.0302, val_acc:0.8291\n",
            "epoch:212/5000, train_steps:5730, train_loss:0.9874, trina_acc:0.8455, val_loss:1.0330, val_acc:0.8265\n",
            "epoch:212/5000, train_steps:5740, train_loss:0.9917, trina_acc:0.8430, val_loss:1.0311, val_acc:0.8260\n",
            "epoch:212/5000, train_steps:5750, train_loss:0.9964, trina_acc:0.8411, val_loss:1.0303, val_acc:0.8276\n",
            "epoch:213/5000, train_steps:5760, train_loss:1.0014, trina_acc:0.8401, val_loss:1.0319, val_acc:0.8298\n",
            "epoch:213/5000, train_steps:5770, train_loss:0.9990, trina_acc:0.8381, val_loss:1.0291, val_acc:0.8299\n",
            "epoch:214/5000, train_steps:5780, train_loss:0.9727, trina_acc:0.8599, val_loss:1.0307, val_acc:0.8288\n",
            "epoch:214/5000, train_steps:5790, train_loss:0.9818, trina_acc:0.8484, val_loss:1.0307, val_acc:0.8286\n",
            "epoch:214/5000, train_steps:5800, train_loss:1.0049, trina_acc:0.8391, val_loss:1.0298, val_acc:0.8287\n",
            "epoch:215/5000, train_steps:5810, train_loss:0.9876, trina_acc:0.8499, val_loss:1.0315, val_acc:0.8301\n",
            "epoch:215/5000, train_steps:5820, train_loss:1.0057, trina_acc:0.8376, val_loss:1.0295, val_acc:0.8284\n",
            "epoch:215/5000, train_steps:5830, train_loss:1.0009, trina_acc:0.8296, val_loss:1.0311, val_acc:0.8267\n",
            "epoch:216/5000, train_steps:5840, train_loss:1.0070, trina_acc:0.8362, val_loss:1.0320, val_acc:0.8276\n",
            "epoch:216/5000, train_steps:5850, train_loss:1.0016, trina_acc:0.8435, val_loss:1.0311, val_acc:0.8240\n",
            "epoch:217/5000, train_steps:5860, train_loss:0.9778, trina_acc:0.8477, val_loss:1.0320, val_acc:0.8265\n",
            "epoch:217/5000, train_steps:5870, train_loss:0.9947, trina_acc:0.8535, val_loss:1.0328, val_acc:0.8255\n",
            "epoch:217/5000, train_steps:5880, train_loss:0.9991, trina_acc:0.8411, val_loss:1.0301, val_acc:0.8258\n",
            "epoch:218/5000, train_steps:5890, train_loss:0.9859, trina_acc:0.8503, val_loss:1.0330, val_acc:0.8274\n",
            "epoch:218/5000, train_steps:5900, train_loss:0.9965, trina_acc:0.8459, val_loss:1.0301, val_acc:0.8290\n",
            "epoch:218/5000, train_steps:5910, train_loss:0.9821, trina_acc:0.8499, val_loss:1.0293, val_acc:0.8285\n",
            "epoch:219/5000, train_steps:5920, train_loss:0.9918, trina_acc:0.8450, val_loss:1.0328, val_acc:0.8279\n",
            "epoch:219/5000, train_steps:5930, train_loss:0.9939, trina_acc:0.8435, val_loss:1.0301, val_acc:0.8287\n",
            "epoch:219/5000, train_steps:5940, train_loss:0.9931, trina_acc:0.8384, val_loss:1.0303, val_acc:0.8298\n",
            "epoch:220/5000, train_steps:5950, train_loss:0.9907, trina_acc:0.8469, val_loss:1.0315, val_acc:0.8286\n",
            "epoch:220/5000, train_steps:5960, train_loss:1.0043, trina_acc:0.8342, val_loss:1.0351, val_acc:0.8168\n",
            "epoch:221/5000, train_steps:5970, train_loss:0.9924, trina_acc:0.8408, val_loss:1.0342, val_acc:0.8223\n",
            "epoch:221/5000, train_steps:5980, train_loss:0.9899, trina_acc:0.8496, val_loss:1.0308, val_acc:0.8278\n",
            "epoch:221/5000, train_steps:5990, train_loss:0.9941, trina_acc:0.8413, val_loss:1.0331, val_acc:0.8222\n",
            "epoch:222/5000, train_steps:6000, train_loss:1.0058, trina_acc:0.8354, val_loss:1.0337, val_acc:0.8273\n",
            "epoch:222/5000, train_steps:6010, train_loss:0.9844, trina_acc:0.8455, val_loss:1.0290, val_acc:0.8295\n",
            "epoch:222/5000, train_steps:6020, train_loss:0.9975, trina_acc:0.8420, val_loss:1.0294, val_acc:0.8290\n",
            "epoch:223/5000, train_steps:6030, train_loss:0.9912, trina_acc:0.8455, val_loss:1.0332, val_acc:0.8230\n",
            "epoch:223/5000, train_steps:6040, train_loss:0.9939, trina_acc:0.8376, val_loss:1.0292, val_acc:0.8253\n",
            "epoch:224/5000, train_steps:6050, train_loss:0.9782, trina_acc:0.8499, val_loss:1.0311, val_acc:0.8272\n",
            "epoch:224/5000, train_steps:6060, train_loss:0.9971, trina_acc:0.8474, val_loss:1.0305, val_acc:0.8279\n",
            "epoch:224/5000, train_steps:6070, train_loss:0.9922, trina_acc:0.8477, val_loss:1.0281, val_acc:0.8288\n",
            "epoch:225/5000, train_steps:6080, train_loss:0.9849, trina_acc:0.8550, val_loss:1.0306, val_acc:0.8299\n",
            "epoch:225/5000, train_steps:6090, train_loss:0.9964, trina_acc:0.8457, val_loss:1.0274, val_acc:0.8297\n",
            "epoch:225/5000, train_steps:6100, train_loss:0.9930, trina_acc:0.8398, val_loss:1.0293, val_acc:0.8273\n",
            "epoch:226/5000, train_steps:6110, train_loss:0.9978, trina_acc:0.8491, val_loss:1.0324, val_acc:0.8278\n",
            "epoch:226/5000, train_steps:6120, train_loss:0.9937, trina_acc:0.8381, val_loss:1.0298, val_acc:0.8248\n",
            "epoch:227/5000, train_steps:6130, train_loss:0.9901, trina_acc:0.8459, val_loss:1.0299, val_acc:0.8255\n",
            "epoch:227/5000, train_steps:6140, train_loss:0.9832, trina_acc:0.8486, val_loss:1.0292, val_acc:0.8275\n",
            "epoch:227/5000, train_steps:6150, train_loss:1.0123, trina_acc:0.8328, val_loss:1.0277, val_acc:0.8284\n",
            "epoch:228/5000, train_steps:6160, train_loss:0.9910, trina_acc:0.8459, val_loss:1.0329, val_acc:0.8255\n",
            "epoch:228/5000, train_steps:6170, train_loss:0.9750, trina_acc:0.8506, val_loss:1.0300, val_acc:0.8257\n",
            "epoch:228/5000, train_steps:6180, train_loss:0.9891, trina_acc:0.8496, val_loss:1.0281, val_acc:0.8301\n",
            "epoch:229/5000, train_steps:6190, train_loss:0.9922, trina_acc:0.8389, val_loss:1.0302, val_acc:0.8288\n",
            "epoch:229/5000, train_steps:6200, train_loss:0.9871, trina_acc:0.8489, val_loss:1.0273, val_acc:0.8296\n",
            "epoch:229/5000, train_steps:6210, train_loss:1.0072, trina_acc:0.8371, val_loss:1.0294, val_acc:0.8285\n",
            "epoch:230/5000, train_steps:6220, train_loss:0.9884, trina_acc:0.8438, val_loss:1.0313, val_acc:0.8232\n",
            "epoch:230/5000, train_steps:6230, train_loss:1.0039, trina_acc:0.8342, val_loss:1.0300, val_acc:0.8244\n",
            "epoch:231/5000, train_steps:6240, train_loss:0.9669, trina_acc:0.8608, val_loss:1.0307, val_acc:0.8287\n",
            "epoch:231/5000, train_steps:6250, train_loss:0.9973, trina_acc:0.8364, val_loss:1.0305, val_acc:0.8257\n",
            "epoch:231/5000, train_steps:6260, train_loss:0.9989, trina_acc:0.8403, val_loss:1.0276, val_acc:0.8303\n",
            "epoch:232/5000, train_steps:6270, train_loss:0.9817, trina_acc:0.8552, val_loss:1.0308, val_acc:0.8292\n",
            "epoch:232/5000, train_steps:6280, train_loss:0.9934, trina_acc:0.8423, val_loss:1.0278, val_acc:0.8296\n",
            "epoch:232/5000, train_steps:6290, train_loss:1.0090, trina_acc:0.8352, val_loss:1.0301, val_acc:0.8261\n",
            "epoch:233/5000, train_steps:6300, train_loss:0.9855, trina_acc:0.8511, val_loss:1.0302, val_acc:0.8290\n",
            "epoch:233/5000, train_steps:6310, train_loss:0.9974, trina_acc:0.8372, val_loss:1.0286, val_acc:0.8279\n",
            "epoch:234/5000, train_steps:6320, train_loss:0.9750, trina_acc:0.8555, val_loss:1.0300, val_acc:0.8286\n",
            "epoch:234/5000, train_steps:6330, train_loss:0.9855, trina_acc:0.8528, val_loss:1.0287, val_acc:0.8284\n",
            "epoch:234/5000, train_steps:6340, train_loss:0.9960, trina_acc:0.8438, val_loss:1.0276, val_acc:0.8300\n",
            "epoch:235/5000, train_steps:6350, train_loss:0.9795, trina_acc:0.8533, val_loss:1.0312, val_acc:0.8292\n",
            "epoch:235/5000, train_steps:6360, train_loss:0.9890, trina_acc:0.8472, val_loss:1.0279, val_acc:0.8292\n",
            "epoch:235/5000, train_steps:6370, train_loss:0.9981, trina_acc:0.8403, val_loss:1.0274, val_acc:0.8291\n",
            "epoch:236/5000, train_steps:6380, train_loss:0.9972, trina_acc:0.8403, val_loss:1.0299, val_acc:0.8297\n",
            "epoch:236/5000, train_steps:6390, train_loss:1.0002, trina_acc:0.8394, val_loss:1.0278, val_acc:0.8301\n",
            "epoch:237/5000, train_steps:6400, train_loss:0.9858, trina_acc:0.8469, val_loss:1.0296, val_acc:0.8286\n",
            "epoch:237/5000, train_steps:6410, train_loss:0.9934, trina_acc:0.8457, val_loss:1.0302, val_acc:0.8276\n",
            "epoch:237/5000, train_steps:6420, train_loss:0.9973, trina_acc:0.8362, val_loss:1.0298, val_acc:0.8266\n",
            "epoch:238/5000, train_steps:6430, train_loss:0.9987, trina_acc:0.8420, val_loss:1.0313, val_acc:0.8285\n",
            "epoch:238/5000, train_steps:6440, train_loss:0.9966, trina_acc:0.8394, val_loss:1.0290, val_acc:0.8257\n",
            "epoch:238/5000, train_steps:6450, train_loss:0.9878, trina_acc:0.8481, val_loss:1.0279, val_acc:0.8286\n",
            "epoch:239/5000, train_steps:6460, train_loss:0.9945, trina_acc:0.8374, val_loss:1.0316, val_acc:0.8285\n",
            "epoch:239/5000, train_steps:6470, train_loss:0.9922, trina_acc:0.8508, val_loss:1.0304, val_acc:0.8283\n",
            "epoch:239/5000, train_steps:6480, train_loss:1.0026, trina_acc:0.8418, val_loss:1.0285, val_acc:0.8288\n",
            "epoch:240/5000, train_steps:6490, train_loss:0.9755, trina_acc:0.8599, val_loss:1.0313, val_acc:0.8261\n",
            "epoch:240/5000, train_steps:6500, train_loss:0.9873, trina_acc:0.8406, val_loss:1.0292, val_acc:0.8273\n",
            "epoch:241/5000, train_steps:6510, train_loss:0.9833, trina_acc:0.8506, val_loss:1.0308, val_acc:0.8280\n",
            "epoch:241/5000, train_steps:6520, train_loss:0.9989, trina_acc:0.8423, val_loss:1.0286, val_acc:0.8279\n",
            "epoch:241/5000, train_steps:6530, train_loss:0.9913, trina_acc:0.8374, val_loss:1.0295, val_acc:0.8277\n",
            "epoch:242/5000, train_steps:6540, train_loss:1.0081, trina_acc:0.8391, val_loss:1.0336, val_acc:0.8242\n",
            "epoch:242/5000, train_steps:6550, train_loss:0.9796, trina_acc:0.8496, val_loss:1.0280, val_acc:0.8287\n",
            "epoch:242/5000, train_steps:6560, train_loss:0.9868, trina_acc:0.8477, val_loss:1.0285, val_acc:0.8290\n",
            "epoch:243/5000, train_steps:6570, train_loss:0.9860, trina_acc:0.8560, val_loss:1.0303, val_acc:0.8282\n",
            "epoch:243/5000, train_steps:6580, train_loss:0.9903, trina_acc:0.8467, val_loss:1.0281, val_acc:0.8306\n",
            "epoch:244/5000, train_steps:6590, train_loss:0.9863, trina_acc:0.8447, val_loss:1.0341, val_acc:0.8207\n",
            "epoch:244/5000, train_steps:6600, train_loss:0.9919, trina_acc:0.8403, val_loss:1.0316, val_acc:0.8246\n",
            "epoch:244/5000, train_steps:6610, train_loss:1.0009, trina_acc:0.8328, val_loss:1.0286, val_acc:0.8281\n",
            "epoch:245/5000, train_steps:6620, train_loss:0.9884, trina_acc:0.8516, val_loss:1.0303, val_acc:0.8300\n",
            "epoch:245/5000, train_steps:6630, train_loss:0.9858, trina_acc:0.8499, val_loss:1.0280, val_acc:0.8286\n",
            "epoch:245/5000, train_steps:6640, train_loss:0.9961, trina_acc:0.8345, val_loss:1.0286, val_acc:0.8298\n",
            "epoch:246/5000, train_steps:6650, train_loss:0.9981, trina_acc:0.8455, val_loss:1.0305, val_acc:0.8283\n",
            "epoch:246/5000, train_steps:6660, train_loss:0.9854, trina_acc:0.8433, val_loss:1.0287, val_acc:0.8296\n",
            "epoch:247/5000, train_steps:6670, train_loss:0.9869, trina_acc:0.8545, val_loss:1.0305, val_acc:0.8273\n",
            "epoch:247/5000, train_steps:6680, train_loss:0.9987, trina_acc:0.8467, val_loss:1.0313, val_acc:0.8241\n",
            "epoch:247/5000, train_steps:6690, train_loss:0.9951, trina_acc:0.8472, val_loss:1.0305, val_acc:0.8250\n",
            "epoch:248/5000, train_steps:6700, train_loss:0.9994, trina_acc:0.8484, val_loss:1.0338, val_acc:0.8268\n",
            "epoch:248/5000, train_steps:6710, train_loss:0.9769, trina_acc:0.8569, val_loss:1.0286, val_acc:0.8288\n",
            "epoch:248/5000, train_steps:6720, train_loss:0.9832, trina_acc:0.8462, val_loss:1.0285, val_acc:0.8294\n",
            "epoch:249/5000, train_steps:6730, train_loss:0.9806, trina_acc:0.8489, val_loss:1.0330, val_acc:0.8262\n",
            "epoch:249/5000, train_steps:6740, train_loss:0.9994, trina_acc:0.8413, val_loss:1.0279, val_acc:0.8292\n",
            "epoch:249/5000, train_steps:6750, train_loss:1.0055, trina_acc:0.8464, val_loss:1.0279, val_acc:0.8291\n",
            "epoch:250/5000, train_steps:6760, train_loss:0.9945, trina_acc:0.8420, val_loss:1.0295, val_acc:0.8291\n",
            "epoch:250/5000, train_steps:6770, train_loss:0.9911, trina_acc:0.8413, val_loss:1.0292, val_acc:0.8272\n",
            "epoch:251/5000, train_steps:6780, train_loss:0.9793, trina_acc:0.8547, val_loss:1.0322, val_acc:0.8253\n",
            "epoch:251/5000, train_steps:6790, train_loss:0.9889, trina_acc:0.8459, val_loss:1.0287, val_acc:0.8285\n",
            "epoch:251/5000, train_steps:6800, train_loss:1.0016, trina_acc:0.8384, val_loss:1.0286, val_acc:0.8282\n",
            "epoch:252/5000, train_steps:6810, train_loss:1.0013, trina_acc:0.8438, val_loss:1.0312, val_acc:0.8288\n",
            "epoch:252/5000, train_steps:6820, train_loss:0.9887, trina_acc:0.8418, val_loss:1.0283, val_acc:0.8298\n",
            "epoch:252/5000, train_steps:6830, train_loss:1.0017, trina_acc:0.8374, val_loss:1.0293, val_acc:0.8259\n",
            "epoch:253/5000, train_steps:6840, train_loss:0.9966, trina_acc:0.8435, val_loss:1.0311, val_acc:0.8277\n",
            "epoch:253/5000, train_steps:6850, train_loss:0.9964, trina_acc:0.8416, val_loss:1.0316, val_acc:0.8211\n",
            "epoch:254/5000, train_steps:6860, train_loss:0.9761, trina_acc:0.8481, val_loss:1.0315, val_acc:0.8253\n",
            "epoch:254/5000, train_steps:6870, train_loss:1.0029, trina_acc:0.8372, val_loss:1.0304, val_acc:0.8273\n",
            "epoch:254/5000, train_steps:6880, train_loss:0.9869, trina_acc:0.8459, val_loss:1.0285, val_acc:0.8279\n",
            "epoch:255/5000, train_steps:6890, train_loss:0.9975, trina_acc:0.8379, val_loss:1.0320, val_acc:0.8284\n",
            "epoch:255/5000, train_steps:6900, train_loss:0.9992, trina_acc:0.8425, val_loss:1.0299, val_acc:0.8273\n",
            "epoch:255/5000, train_steps:6910, train_loss:1.0019, trina_acc:0.8381, val_loss:1.0286, val_acc:0.8288\n",
            "epoch:256/5000, train_steps:6920, train_loss:0.9881, trina_acc:0.8486, val_loss:1.0307, val_acc:0.8286\n",
            "epoch:256/5000, train_steps:6930, train_loss:0.9878, trina_acc:0.8521, val_loss:1.0275, val_acc:0.8295\n",
            "epoch:257/5000, train_steps:6940, train_loss:0.9852, trina_acc:0.8486, val_loss:1.0289, val_acc:0.8292\n",
            "epoch:257/5000, train_steps:6950, train_loss:0.9913, trina_acc:0.8452, val_loss:1.0293, val_acc:0.8286\n",
            "epoch:257/5000, train_steps:6960, train_loss:0.9899, trina_acc:0.8389, val_loss:1.0296, val_acc:0.8256\n",
            "epoch:258/5000, train_steps:6970, train_loss:0.9804, trina_acc:0.8555, val_loss:1.0314, val_acc:0.8294\n",
            "epoch:258/5000, train_steps:6980, train_loss:0.9915, trina_acc:0.8425, val_loss:1.0288, val_acc:0.8282\n",
            "epoch:258/5000, train_steps:6990, train_loss:0.9924, trina_acc:0.8452, val_loss:1.0305, val_acc:0.8280\n",
            "epoch:259/5000, train_steps:7000, train_loss:0.9921, trina_acc:0.8542, val_loss:1.0308, val_acc:0.8299\n",
            "epoch:259/5000, train_steps:7010, train_loss:0.9814, trina_acc:0.8528, val_loss:1.0278, val_acc:0.8289\n",
            "epoch:259/5000, train_steps:7020, train_loss:1.0106, trina_acc:0.8351, val_loss:1.0294, val_acc:0.8253\n",
            "epoch:260/5000, train_steps:7030, train_loss:0.9807, trina_acc:0.8494, val_loss:1.0298, val_acc:0.8267\n",
            "epoch:260/5000, train_steps:7040, train_loss:0.9897, trina_acc:0.8398, val_loss:1.0292, val_acc:0.8257\n",
            "epoch:261/5000, train_steps:7050, train_loss:0.9842, trina_acc:0.8540, val_loss:1.0311, val_acc:0.8246\n",
            "epoch:261/5000, train_steps:7060, train_loss:1.0011, trina_acc:0.8369, val_loss:1.0284, val_acc:0.8288\n",
            "epoch:261/5000, train_steps:7070, train_loss:0.9864, trina_acc:0.8494, val_loss:1.0268, val_acc:0.8292\n",
            "epoch:262/5000, train_steps:7080, train_loss:0.9877, trina_acc:0.8477, val_loss:1.0292, val_acc:0.8303\n",
            "epoch:262/5000, train_steps:7090, train_loss:0.9986, trina_acc:0.8398, val_loss:1.0265, val_acc:0.8296\n",
            "epoch:262/5000, train_steps:7100, train_loss:1.0030, trina_acc:0.8411, val_loss:1.0276, val_acc:0.8274\n",
            "epoch:263/5000, train_steps:7110, train_loss:0.9935, trina_acc:0.8486, val_loss:1.0290, val_acc:0.8295\n",
            "epoch:263/5000, train_steps:7120, train_loss:0.9774, trina_acc:0.8528, val_loss:1.0267, val_acc:0.8300\n",
            "epoch:264/5000, train_steps:7130, train_loss:0.9913, trina_acc:0.8389, val_loss:1.0311, val_acc:0.8211\n",
            "epoch:264/5000, train_steps:7140, train_loss:0.9980, trina_acc:0.8430, val_loss:1.0316, val_acc:0.8225\n",
            "epoch:264/5000, train_steps:7150, train_loss:0.9937, trina_acc:0.8389, val_loss:1.0284, val_acc:0.8263\n",
            "epoch:265/5000, train_steps:7160, train_loss:0.9899, trina_acc:0.8442, val_loss:1.0294, val_acc:0.8305\n",
            "epoch:265/5000, train_steps:7170, train_loss:0.9882, trina_acc:0.8518, val_loss:1.0286, val_acc:0.8285\n",
            "epoch:265/5000, train_steps:7180, train_loss:1.0044, trina_acc:0.8379, val_loss:1.0278, val_acc:0.8285\n",
            "epoch:266/5000, train_steps:7190, train_loss:0.9899, trina_acc:0.8386, val_loss:1.0316, val_acc:0.8219\n",
            "epoch:266/5000, train_steps:7200, train_loss:0.9854, trina_acc:0.8457, val_loss:1.0276, val_acc:0.8272\n",
            "epoch:267/5000, train_steps:7210, train_loss:0.9820, trina_acc:0.8501, val_loss:1.0288, val_acc:0.8248\n",
            "epoch:267/5000, train_steps:7220, train_loss:0.9838, trina_acc:0.8494, val_loss:1.0279, val_acc:0.8278\n",
            "epoch:267/5000, train_steps:7230, train_loss:0.9936, trina_acc:0.8430, val_loss:1.0287, val_acc:0.8268\n",
            "epoch:268/5000, train_steps:7240, train_loss:0.9933, trina_acc:0.8440, val_loss:1.0304, val_acc:0.8274\n",
            "epoch:268/5000, train_steps:7250, train_loss:0.9939, trina_acc:0.8406, val_loss:1.0281, val_acc:0.8281\n",
            "epoch:268/5000, train_steps:7260, train_loss:1.0066, trina_acc:0.8369, val_loss:1.0276, val_acc:0.8282\n",
            "epoch:269/5000, train_steps:7270, train_loss:0.9990, trina_acc:0.8394, val_loss:1.0315, val_acc:0.8240\n",
            "epoch:269/5000, train_steps:7280, train_loss:0.9845, trina_acc:0.8430, val_loss:1.0295, val_acc:0.8222\n",
            "epoch:269/5000, train_steps:7290, train_loss:1.0075, trina_acc:0.8298, val_loss:1.0282, val_acc:0.8289\n",
            "epoch:270/5000, train_steps:7300, train_loss:0.9997, trina_acc:0.8408, val_loss:1.0292, val_acc:0.8293\n",
            "epoch:270/5000, train_steps:7310, train_loss:0.9923, trina_acc:0.8420, val_loss:1.0273, val_acc:0.8288\n",
            "epoch:271/5000, train_steps:7320, train_loss:0.9909, trina_acc:0.8472, val_loss:1.0285, val_acc:0.8294\n",
            "epoch:271/5000, train_steps:7330, train_loss:0.9844, trina_acc:0.8457, val_loss:1.0269, val_acc:0.8299\n",
            "epoch:271/5000, train_steps:7340, train_loss:0.9896, trina_acc:0.8479, val_loss:1.0279, val_acc:0.8273\n",
            "epoch:272/5000, train_steps:7350, train_loss:0.9872, trina_acc:0.8416, val_loss:1.0302, val_acc:0.8273\n",
            "epoch:272/5000, train_steps:7360, train_loss:0.9848, trina_acc:0.8496, val_loss:1.0274, val_acc:0.8302\n",
            "epoch:272/5000, train_steps:7370, train_loss:0.9887, trina_acc:0.8496, val_loss:1.0276, val_acc:0.8294\n",
            "epoch:273/5000, train_steps:7380, train_loss:0.9831, trina_acc:0.8486, val_loss:1.0304, val_acc:0.8289\n",
            "epoch:273/5000, train_steps:7390, train_loss:0.9942, trina_acc:0.8384, val_loss:1.0279, val_acc:0.8278\n",
            "epoch:274/5000, train_steps:7400, train_loss:0.9761, trina_acc:0.8538, val_loss:1.0288, val_acc:0.8287\n",
            "epoch:274/5000, train_steps:7410, train_loss:0.9866, trina_acc:0.8442, val_loss:1.0276, val_acc:0.8279\n",
            "epoch:274/5000, train_steps:7420, train_loss:0.9956, trina_acc:0.8352, val_loss:1.0278, val_acc:0.8281\n",
            "epoch:275/5000, train_steps:7430, train_loss:0.9905, trina_acc:0.8479, val_loss:1.0311, val_acc:0.8280\n",
            "epoch:275/5000, train_steps:7440, train_loss:0.9894, trina_acc:0.8469, val_loss:1.0278, val_acc:0.8284\n",
            "epoch:275/5000, train_steps:7450, train_loss:0.9897, trina_acc:0.8396, val_loss:1.0275, val_acc:0.8288\n",
            "epoch:276/5000, train_steps:7460, train_loss:0.9915, trina_acc:0.8450, val_loss:1.0292, val_acc:0.8292\n",
            "epoch:276/5000, train_steps:7470, train_loss:0.9967, trina_acc:0.8362, val_loss:1.0271, val_acc:0.8297\n",
            "epoch:277/5000, train_steps:7480, train_loss:0.9880, trina_acc:0.8442, val_loss:1.0272, val_acc:0.8294\n",
            "epoch:277/5000, train_steps:7490, train_loss:0.9905, trina_acc:0.8428, val_loss:1.0286, val_acc:0.8261\n",
            "epoch:277/5000, train_steps:7500, train_loss:1.0046, trina_acc:0.8381, val_loss:1.0280, val_acc:0.8277\n",
            "epoch:278/5000, train_steps:7510, train_loss:0.9911, trina_acc:0.8457, val_loss:1.0306, val_acc:0.8285\n",
            "epoch:278/5000, train_steps:7520, train_loss:0.9987, trina_acc:0.8413, val_loss:1.0283, val_acc:0.8292\n",
            "epoch:278/5000, train_steps:7530, train_loss:0.9936, trina_acc:0.8362, val_loss:1.0291, val_acc:0.8233\n",
            "epoch:279/5000, train_steps:7540, train_loss:0.9826, trina_acc:0.8508, val_loss:1.0293, val_acc:0.8302\n",
            "epoch:279/5000, train_steps:7550, train_loss:1.0021, trina_acc:0.8442, val_loss:1.0264, val_acc:0.8298\n",
            "epoch:279/5000, train_steps:7560, train_loss:0.9786, trina_acc:0.8531, val_loss:1.0290, val_acc:0.8294\n",
            "epoch:280/5000, train_steps:7570, train_loss:0.9886, trina_acc:0.8486, val_loss:1.0291, val_acc:0.8273\n",
            "epoch:280/5000, train_steps:7580, train_loss:0.9954, trina_acc:0.8435, val_loss:1.0273, val_acc:0.8288\n",
            "epoch:281/5000, train_steps:7590, train_loss:0.9866, trina_acc:0.8379, val_loss:1.0301, val_acc:0.8253\n",
            "epoch:281/5000, train_steps:7600, train_loss:0.9911, trina_acc:0.8433, val_loss:1.0284, val_acc:0.8299\n",
            "epoch:281/5000, train_steps:7610, train_loss:0.9857, trina_acc:0.8413, val_loss:1.0289, val_acc:0.8268\n",
            "epoch:282/5000, train_steps:7620, train_loss:0.9953, trina_acc:0.8462, val_loss:1.0329, val_acc:0.8196\n",
            "epoch:282/5000, train_steps:7630, train_loss:0.9949, trina_acc:0.8386, val_loss:1.0280, val_acc:0.8267\n",
            "epoch:282/5000, train_steps:7640, train_loss:0.9958, trina_acc:0.8391, val_loss:1.0273, val_acc:0.8292\n",
            "epoch:283/5000, train_steps:7650, train_loss:0.9955, trina_acc:0.8428, val_loss:1.0288, val_acc:0.8294\n",
            "epoch:283/5000, train_steps:7660, train_loss:0.9871, trina_acc:0.8418, val_loss:1.0282, val_acc:0.8277\n",
            "epoch:284/5000, train_steps:7670, train_loss:0.9902, trina_acc:0.8462, val_loss:1.0293, val_acc:0.8269\n",
            "epoch:284/5000, train_steps:7680, train_loss:0.9941, trina_acc:0.8467, val_loss:1.0298, val_acc:0.8278\n",
            "epoch:284/5000, train_steps:7690, train_loss:0.9954, trina_acc:0.8408, val_loss:1.0286, val_acc:0.8263\n",
            "epoch:285/5000, train_steps:7700, train_loss:0.9899, trina_acc:0.8499, val_loss:1.0329, val_acc:0.8248\n",
            "epoch:285/5000, train_steps:7710, train_loss:1.0099, trina_acc:0.8352, val_loss:1.0286, val_acc:0.8288\n",
            "epoch:285/5000, train_steps:7720, train_loss:0.9760, trina_acc:0.8462, val_loss:1.0270, val_acc:0.8300\n",
            "epoch:286/5000, train_steps:7730, train_loss:0.9857, trina_acc:0.8528, val_loss:1.0285, val_acc:0.8297\n",
            "epoch:286/5000, train_steps:7740, train_loss:1.0080, trina_acc:0.8298, val_loss:1.0270, val_acc:0.8290\n",
            "epoch:287/5000, train_steps:7750, train_loss:0.9858, trina_acc:0.8425, val_loss:1.0300, val_acc:0.8291\n",
            "epoch:287/5000, train_steps:7760, train_loss:0.9956, trina_acc:0.8406, val_loss:1.0285, val_acc:0.8288\n",
            "epoch:287/5000, train_steps:7770, train_loss:0.9910, trina_acc:0.8452, val_loss:1.0270, val_acc:0.8298\n",
            "epoch:288/5000, train_steps:7780, train_loss:0.9752, trina_acc:0.8547, val_loss:1.0303, val_acc:0.8298\n",
            "epoch:288/5000, train_steps:7790, train_loss:0.9859, trina_acc:0.8489, val_loss:1.0276, val_acc:0.8282\n",
            "epoch:288/5000, train_steps:7800, train_loss:0.9947, trina_acc:0.8423, val_loss:1.0276, val_acc:0.8272\n",
            "epoch:289/5000, train_steps:7810, train_loss:1.0045, trina_acc:0.8328, val_loss:1.0321, val_acc:0.8231\n",
            "epoch:289/5000, train_steps:7820, train_loss:0.9933, trina_acc:0.8440, val_loss:1.0274, val_acc:0.8282\n",
            "epoch:289/5000, train_steps:7830, train_loss:0.9693, trina_acc:0.8517, val_loss:1.0271, val_acc:0.8289\n",
            "epoch:290/5000, train_steps:7840, train_loss:1.0047, trina_acc:0.8320, val_loss:1.0278, val_acc:0.8292\n",
            "epoch:290/5000, train_steps:7850, train_loss:0.9878, trina_acc:0.8418, val_loss:1.0283, val_acc:0.8272\n",
            "epoch:291/5000, train_steps:7860, train_loss:0.9824, trina_acc:0.8516, val_loss:1.0309, val_acc:0.8271\n",
            "epoch:291/5000, train_steps:7870, train_loss:0.9863, trina_acc:0.8452, val_loss:1.0287, val_acc:0.8269\n",
            "epoch:291/5000, train_steps:7880, train_loss:0.9855, trina_acc:0.8484, val_loss:1.0275, val_acc:0.8282\n",
            "epoch:292/5000, train_steps:7890, train_loss:0.9858, trina_acc:0.8489, val_loss:1.0300, val_acc:0.8278\n",
            "epoch:292/5000, train_steps:7900, train_loss:0.9943, trina_acc:0.8394, val_loss:1.0275, val_acc:0.8288\n",
            "epoch:292/5000, train_steps:7910, train_loss:1.0043, trina_acc:0.8369, val_loss:1.0276, val_acc:0.8293\n",
            "epoch:293/5000, train_steps:7920, train_loss:0.9854, trina_acc:0.8435, val_loss:1.0320, val_acc:0.8235\n",
            "epoch:293/5000, train_steps:7930, train_loss:0.9972, trina_acc:0.8347, val_loss:1.0304, val_acc:0.8225\n",
            "epoch:294/5000, train_steps:7940, train_loss:0.9793, trina_acc:0.8496, val_loss:1.0298, val_acc:0.8247\n",
            "epoch:294/5000, train_steps:7950, train_loss:0.9834, trina_acc:0.8489, val_loss:1.0278, val_acc:0.8298\n",
            "epoch:294/5000, train_steps:7960, train_loss:0.9978, trina_acc:0.8396, val_loss:1.0275, val_acc:0.8298\n",
            "epoch:295/5000, train_steps:7970, train_loss:0.9895, trina_acc:0.8428, val_loss:1.0294, val_acc:0.8289\n",
            "epoch:295/5000, train_steps:7980, train_loss:0.9924, trina_acc:0.8425, val_loss:1.0271, val_acc:0.8291\n",
            "epoch:295/5000, train_steps:7990, train_loss:0.9942, trina_acc:0.8464, val_loss:1.0278, val_acc:0.8287\n",
            "epoch:296/5000, train_steps:8000, train_loss:1.0051, trina_acc:0.8416, val_loss:1.0293, val_acc:0.8286\n",
            "epoch:296/5000, train_steps:8010, train_loss:0.9838, trina_acc:0.8540, val_loss:1.0270, val_acc:0.8283\n",
            "epoch:297/5000, train_steps:8020, train_loss:0.9738, trina_acc:0.8579, val_loss:1.0284, val_acc:0.8283\n",
            "epoch:297/5000, train_steps:8030, train_loss:0.9879, trina_acc:0.8425, val_loss:1.0287, val_acc:0.8289\n",
            "epoch:297/5000, train_steps:8040, train_loss:0.9915, trina_acc:0.8398, val_loss:1.0260, val_acc:0.8294\n",
            "epoch:298/5000, train_steps:8050, train_loss:0.9888, trina_acc:0.8450, val_loss:1.0278, val_acc:0.8299\n",
            "epoch:298/5000, train_steps:8060, train_loss:0.9917, trina_acc:0.8425, val_loss:1.0270, val_acc:0.8291\n",
            "epoch:298/5000, train_steps:8070, train_loss:0.9941, trina_acc:0.8394, val_loss:1.0279, val_acc:0.8247\n",
            "epoch:299/5000, train_steps:8080, train_loss:0.9810, trina_acc:0.8445, val_loss:1.0295, val_acc:0.8264\n",
            "epoch:299/5000, train_steps:8090, train_loss:1.0051, trina_acc:0.8342, val_loss:1.0268, val_acc:0.8282\n",
            "epoch:299/5000, train_steps:8100, train_loss:1.0085, trina_acc:0.8444, val_loss:1.0265, val_acc:0.8287\n",
            "epoch:300/5000, train_steps:8110, train_loss:1.0015, trina_acc:0.8347, val_loss:1.0290, val_acc:0.8277\n",
            "epoch:300/5000, train_steps:8120, train_loss:1.0010, trina_acc:0.8372, val_loss:1.0281, val_acc:0.8291\n",
            "epoch:301/5000, train_steps:8130, train_loss:0.9783, trina_acc:0.8555, val_loss:1.0280, val_acc:0.8300\n",
            "epoch:301/5000, train_steps:8140, train_loss:0.9902, trina_acc:0.8457, val_loss:1.0274, val_acc:0.8287\n",
            "epoch:301/5000, train_steps:8150, train_loss:0.9809, trina_acc:0.8457, val_loss:1.0259, val_acc:0.8296\n",
            "epoch:302/5000, train_steps:8160, train_loss:0.9959, trina_acc:0.8416, val_loss:1.0286, val_acc:0.8293\n",
            "epoch:302/5000, train_steps:8170, train_loss:0.9760, trina_acc:0.8538, val_loss:1.0255, val_acc:0.8290\n",
            "epoch:302/5000, train_steps:8180, train_loss:0.9941, trina_acc:0.8381, val_loss:1.0255, val_acc:0.8297\n",
            "epoch:303/5000, train_steps:8190, train_loss:0.9819, trina_acc:0.8491, val_loss:1.0288, val_acc:0.8290\n",
            "epoch:303/5000, train_steps:8200, train_loss:0.9915, trina_acc:0.8435, val_loss:1.0268, val_acc:0.8289\n",
            "epoch:304/5000, train_steps:8210, train_loss:0.9736, trina_acc:0.8462, val_loss:1.0293, val_acc:0.8263\n",
            "epoch:304/5000, train_steps:8220, train_loss:0.9925, trina_acc:0.8455, val_loss:1.0279, val_acc:0.8263\n",
            "epoch:304/5000, train_steps:8230, train_loss:1.0051, trina_acc:0.8301, val_loss:1.0275, val_acc:0.8250\n",
            "epoch:305/5000, train_steps:8240, train_loss:0.9881, trina_acc:0.8457, val_loss:1.0280, val_acc:0.8303\n",
            "epoch:305/5000, train_steps:8250, train_loss:0.9988, trina_acc:0.8433, val_loss:1.0267, val_acc:0.8291\n",
            "epoch:305/5000, train_steps:8260, train_loss:1.0028, trina_acc:0.8396, val_loss:1.0268, val_acc:0.8278\n",
            "epoch:306/5000, train_steps:8270, train_loss:1.0010, trina_acc:0.8333, val_loss:1.0285, val_acc:0.8288\n",
            "epoch:306/5000, train_steps:8280, train_loss:0.9796, trina_acc:0.8464, val_loss:1.0265, val_acc:0.8280\n",
            "epoch:307/5000, train_steps:8290, train_loss:0.9767, trina_acc:0.8542, val_loss:1.0272, val_acc:0.8287\n",
            "epoch:307/5000, train_steps:8300, train_loss:0.9941, trina_acc:0.8357, val_loss:1.0273, val_acc:0.8295\n",
            "epoch:307/5000, train_steps:8310, train_loss:0.9965, trina_acc:0.8401, val_loss:1.0257, val_acc:0.8294\n",
            "epoch:308/5000, train_steps:8320, train_loss:0.9770, trina_acc:0.8481, val_loss:1.0283, val_acc:0.8276\n",
            "epoch:308/5000, train_steps:8330, train_loss:0.9894, trina_acc:0.8423, val_loss:1.0260, val_acc:0.8288\n",
            "epoch:308/5000, train_steps:8340, train_loss:0.9824, trina_acc:0.8501, val_loss:1.0261, val_acc:0.8291\n",
            "epoch:309/5000, train_steps:8350, train_loss:0.9885, trina_acc:0.8433, val_loss:1.0298, val_acc:0.8282\n",
            "epoch:309/5000, train_steps:8360, train_loss:0.9871, trina_acc:0.8459, val_loss:1.0276, val_acc:0.8267\n",
            "epoch:309/5000, train_steps:8370, train_loss:0.9817, trina_acc:0.8431, val_loss:1.0279, val_acc:0.8244\n",
            "epoch:310/5000, train_steps:8380, train_loss:0.9980, trina_acc:0.8376, val_loss:1.0276, val_acc:0.8291\n",
            "epoch:310/5000, train_steps:8390, train_loss:0.9979, trina_acc:0.8440, val_loss:1.0258, val_acc:0.8299\n",
            "epoch:311/5000, train_steps:8400, train_loss:0.9793, trina_acc:0.8535, val_loss:1.0279, val_acc:0.8294\n",
            "epoch:311/5000, train_steps:8410, train_loss:1.0012, trina_acc:0.8423, val_loss:1.0271, val_acc:0.8286\n",
            "epoch:311/5000, train_steps:8420, train_loss:0.9922, trina_acc:0.8345, val_loss:1.0283, val_acc:0.8221\n",
            "epoch:312/5000, train_steps:8430, train_loss:0.9956, trina_acc:0.8496, val_loss:1.0292, val_acc:0.8275\n",
            "epoch:312/5000, train_steps:8440, train_loss:1.0045, trina_acc:0.8428, val_loss:1.0257, val_acc:0.8293\n",
            "epoch:312/5000, train_steps:8450, train_loss:1.0032, trina_acc:0.8357, val_loss:1.0256, val_acc:0.8302\n",
            "epoch:313/5000, train_steps:8460, train_loss:0.9993, trina_acc:0.8428, val_loss:1.0290, val_acc:0.8278\n",
            "epoch:313/5000, train_steps:8470, train_loss:0.9873, trina_acc:0.8416, val_loss:1.0274, val_acc:0.8259\n",
            "epoch:314/5000, train_steps:8480, train_loss:0.9815, trina_acc:0.8535, val_loss:1.0269, val_acc:0.8284\n",
            "epoch:314/5000, train_steps:8490, train_loss:0.9940, trina_acc:0.8428, val_loss:1.0264, val_acc:0.8292\n",
            "epoch:314/5000, train_steps:8500, train_loss:0.9928, trina_acc:0.8408, val_loss:1.0276, val_acc:0.8273\n",
            "epoch:315/5000, train_steps:8510, train_loss:0.9907, trina_acc:0.8438, val_loss:1.0300, val_acc:0.8267\n",
            "epoch:315/5000, train_steps:8520, train_loss:0.9842, trina_acc:0.8503, val_loss:1.0279, val_acc:0.8253\n",
            "epoch:315/5000, train_steps:8530, train_loss:0.9853, trina_acc:0.8481, val_loss:1.0263, val_acc:0.8283\n",
            "epoch:316/5000, train_steps:8540, train_loss:0.9822, trina_acc:0.8508, val_loss:1.0300, val_acc:0.8271\n",
            "epoch:316/5000, train_steps:8550, train_loss:0.9830, trina_acc:0.8428, val_loss:1.0270, val_acc:0.8283\n",
            "epoch:317/5000, train_steps:8560, train_loss:0.9718, trina_acc:0.8586, val_loss:1.0282, val_acc:0.8247\n",
            "epoch:317/5000, train_steps:8570, train_loss:1.0068, trina_acc:0.8325, val_loss:1.0281, val_acc:0.8290\n",
            "epoch:317/5000, train_steps:8580, train_loss:0.9932, trina_acc:0.8420, val_loss:1.0262, val_acc:0.8293\n",
            "epoch:318/5000, train_steps:8590, train_loss:0.9846, trina_acc:0.8438, val_loss:1.0281, val_acc:0.8299\n",
            "epoch:318/5000, train_steps:8600, train_loss:0.9796, trina_acc:0.8484, val_loss:1.0273, val_acc:0.8294\n",
            "epoch:318/5000, train_steps:8610, train_loss:0.9856, trina_acc:0.8481, val_loss:1.0262, val_acc:0.8292\n",
            "epoch:319/5000, train_steps:8620, train_loss:0.9859, trina_acc:0.8384, val_loss:1.0300, val_acc:0.8260\n",
            "epoch:319/5000, train_steps:8630, train_loss:0.9897, trina_acc:0.8477, val_loss:1.0262, val_acc:0.8286\n",
            "epoch:319/5000, train_steps:8640, train_loss:0.9757, trina_acc:0.8504, val_loss:1.0265, val_acc:0.8279\n",
            "epoch:320/5000, train_steps:8650, train_loss:0.9946, trina_acc:0.8416, val_loss:1.0271, val_acc:0.8307\n",
            "epoch:320/5000, train_steps:8660, train_loss:0.9932, trina_acc:0.8435, val_loss:1.0265, val_acc:0.8292\n",
            "epoch:321/5000, train_steps:8670, train_loss:0.9818, trina_acc:0.8535, val_loss:1.0288, val_acc:0.8289\n",
            "epoch:321/5000, train_steps:8680, train_loss:0.9759, trina_acc:0.8591, val_loss:1.0280, val_acc:0.8277\n",
            "epoch:321/5000, train_steps:8690, train_loss:1.0052, trina_acc:0.8279, val_loss:1.0267, val_acc:0.8273\n",
            "epoch:322/5000, train_steps:8700, train_loss:0.9797, trina_acc:0.8479, val_loss:1.0276, val_acc:0.8298\n",
            "epoch:322/5000, train_steps:8710, train_loss:0.9790, trina_acc:0.8464, val_loss:1.0257, val_acc:0.8289\n",
            "epoch:322/5000, train_steps:8720, train_loss:0.9922, trina_acc:0.8481, val_loss:1.0258, val_acc:0.8301\n",
            "epoch:323/5000, train_steps:8730, train_loss:0.9833, trina_acc:0.8513, val_loss:1.0286, val_acc:0.8301\n",
            "epoch:323/5000, train_steps:8740, train_loss:0.9910, trina_acc:0.8374, val_loss:1.0261, val_acc:0.8286\n",
            "epoch:324/5000, train_steps:8750, train_loss:1.0079, trina_acc:0.8318, val_loss:1.0281, val_acc:0.8276\n",
            "epoch:324/5000, train_steps:8760, train_loss:0.9950, trina_acc:0.8403, val_loss:1.0282, val_acc:0.8281\n",
            "epoch:324/5000, train_steps:8770, train_loss:0.9890, trina_acc:0.8503, val_loss:1.0268, val_acc:0.8291\n",
            "epoch:325/5000, train_steps:8780, train_loss:0.9789, trina_acc:0.8545, val_loss:1.0284, val_acc:0.8292\n",
            "epoch:325/5000, train_steps:8790, train_loss:0.9799, trina_acc:0.8523, val_loss:1.0263, val_acc:0.8291\n",
            "epoch:325/5000, train_steps:8800, train_loss:0.9990, trina_acc:0.8372, val_loss:1.0275, val_acc:0.8269\n",
            "epoch:326/5000, train_steps:8810, train_loss:1.0001, trina_acc:0.8396, val_loss:1.0293, val_acc:0.8285\n",
            "epoch:326/5000, train_steps:8820, train_loss:0.9890, trina_acc:0.8423, val_loss:1.0266, val_acc:0.8297\n",
            "epoch:327/5000, train_steps:8830, train_loss:0.9802, trina_acc:0.8599, val_loss:1.0274, val_acc:0.8294\n",
            "epoch:327/5000, train_steps:8840, train_loss:0.9885, trina_acc:0.8477, val_loss:1.0285, val_acc:0.8277\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pjrAY1AzxgU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FastText:\n",
        "  \n",
        "  def __init__(self,\n",
        "               class_size, sentence_length, vocab_size,\n",
        "               embedding_dim, learning_rate, num_sampled, epoch,\n",
        "               l2_reg_lambda, initializer=tf.random_normal_initializer(stddev=0.1)):\n",
        "    self.class_size = class_size\n",
        "    self.sentence_length = sentence_length\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.learning_rate = learning_rate\n",
        "    self.num_sampled = num_sampled\n",
        "    self.epoch = epoch\n",
        "    self.l2_reg_lambda = l2_reg_lambda\n",
        "    self.initializer = initializer\n",
        "    \n",
        "    self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "    self.input_x = tf.placeholder(tf.int32, [None, self.sentence_length], name= 'input_x')\n",
        "    self.input_y = tf.placeholder(tf.int32, [None, self.class_size], name='input_y')\n",
        "    #self.loss = tf.constant(0.0)\n",
        "    #self.global_step = tf.Variable(0, trainable=False, name=\"Global_Step\")\n",
        "    \n",
        "    self.model()\n",
        "    #self.instantiate_weights()\n",
        "    #self.logits = self.inference()\n",
        "    \n",
        "    #self.loss_val = self.loss()\n",
        "    #self.train_op = self.train()\n",
        "    \n",
        "    #self.accuracy = self.get_accuracy()\n",
        "    #self.predictions = tf.argmax(self.logits, axis=1, name=\"predictions\")  # shape:[None,]\n",
        "    #correct_prediction = tf.equal(tf.cast(self.predictions, tf.int32), self.input_y)\n",
        "    #self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"Accuracy\")\n",
        "    \n",
        "  \n",
        "  def model(self):\n",
        "    \n",
        "    with tf.variable_scope('weights', reuse=tf.AUTO_REUSE):\n",
        "      self.Embedding = tf.get_variable('Embedding',[self.vocab_size, self.embedding_dim],\n",
        "                                       initializer=self.initializer)\n",
        "      self.W = tf.get_variable('W', [self.embedding_dim, self.class_size],\n",
        "                               initializer=self.initializer)\n",
        "      self.b = tf.get_variable('b',[self.class_size])\n",
        "    \n",
        "    with tf.name_scope('embedding'):\n",
        "      self.embedding_chars = tf.nn.embedding_lookup(self.Embedding, self.input_x)\n",
        "      \n",
        "      self.sentence_embeddings = tf.reduce_mean(self.embedding_chars, axis=1)\n",
        "    \n",
        "    self.logits = tf.matmul(self.sentence_embeddings, self.W) + self.b\n",
        "    \n",
        "    with tf.name_scope('loss'):\n",
        "      self.loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits,\n",
        "                                                                 labels=self.input_y)\n",
        "      l2_loss = tf.nn.l2_loss(self.W)\n",
        "      l2_loss += tf.nn.l2_loss(self.b)\n",
        "      self.loss += l2_loss * self.l2_reg_lambda\n",
        "      #input_y = tf.reshape(self.input_y, [-1])\n",
        "      #input_y = tf.expand_dims(input_y, 1)\n",
        "      #self.loss = tf.reduce_mean(\n",
        "      #           tf.nn.nce_loss(weights=tf.transpose(self.W),\n",
        "      #                         biases=self.b,\n",
        "      #                         labels=input_y,\n",
        "      #                         inputs=self.sentence_embeddings,\n",
        "      #                         num_sampled=self.num_sampled,\n",
        "      #                         num_classes=self.class_size,\n",
        "      #                         partition_strategy=\"div\"))\n",
        "\n",
        "    #with tf.name_scope('opt'):\n",
        "      #optimizer= tf.train.AdamOptimizer(learning_rate)\n",
        "      #update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "      #self.train_op = slim.learning.create_train_op(total_loss=self.loss, optimizer=optimizer,update_ops=update_ops)\n",
        "    self.train_op = tf.contrib.layers.optimize_loss(self.loss, global_step=self.global_step, learning_rate=self.learning_rate, optimizer=\"Adam\")\n",
        "      \n",
        "    with tf.name_scope('accuracy'):\n",
        "      self.predictions = tf.argmax(self.logits, axis = 1, name='predictions')\n",
        "      correct_prediction = tf.equal(tf.cast(self.predictions, tf.int32), self.input_y)\n",
        "      self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='Accuracy')\n",
        "      \n",
        "  def fit(self, train_x, train_y, batch_size):\n",
        "    if not os.path.exists('saves/fasttext'): os.makedirs('saves/fasttext')\n",
        "    if not os.path.exists('train_logs/fasttext'): os.makedirs('train_logs/fasttext')\n",
        "\n",
        "    train_steps = 0\n",
        "    best_val_acc = 0\n",
        "    \n",
        "    tf.summary.scalar('val_loss', self.loss_val)\n",
        "    tf.summary.scalar('val_accuracy', self.accuracy)\n",
        "    merged = tf.summary.merge_all()\n",
        "    \n",
        "    sess = tf.Session()\n",
        "    \n",
        "    writer = tf.summary.FileWriter('train_logs/fasttext', sess.graph)\n",
        "    saver = tf.train.Saver(max_to_keep=10)\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    for i in range(self.epoch):\n",
        "      batch_train = self.batch_iter(train_x, train_y, batch_size)\n",
        "      for batch_x, batch_y in batch_train:\n",
        "        train_steps +=1\n",
        "        feed_dict = {self.input_x:batch_x, self.input_y:batch_y}\n",
        "        _, train_loss, train_acc = sess.run([self.train_op, self.loss, self.accuracy], feed_dict = feed_dict)\n",
        "        \n",
        "        if train_steps % 1000 ==0:\n",
        "          #feed_dict = {self.input_x:val_x, self.input_y:val_y}\n",
        "          val_loss, val_acc = sess.run([self.loss, self.acc],feed_dict=feed_dict)\n",
        "        \n",
        "          summary = sess.run(merged, feed_dict=feed_dict)\n",
        "          writer.add_summary(summary, global_step=train_steps)\n",
        "        \n",
        "          if val_acc >= best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            saver.save(sess, \"saves/fasttext\", global_step=train_steps)\n",
        "          msg = 'epoch:%d/%d, train_steps:%d, train_loss:%.4f, trina_acc:%.4f, val_loss:%.4f, val_acc:%.4f'\n",
        "          print(msg%(i, self.epoch, train_steps, train_loss, train_acc, val_loss, val_acc))\n",
        "        \n",
        "  def batch_iter(self, x, y, batch_size=1, shuffle=True):\n",
        "    data = np.array(data)\n",
        "    data_len = len(x)\n",
        "    num_batch = int((data_len-1)/batch_size)+1\n",
        "    \n",
        "    if shuffle:\n",
        "      shuffle_indices = np.random.permutation(np.arange(data_len))\n",
        "      x_shuffle = x[shuffle_indices]\n",
        "      y_shuffle= y[shuffle_indices]\n",
        "      #random.shuffle(x)\n",
        "      #x_shuffle = x\n",
        "      #random.shuffle(y)\n",
        "      #y_shuffle = y\n",
        "    else:\n",
        "      x_shuffle=x\n",
        "      y_shuffle = y\n",
        "    for i in range(num_batch):\n",
        "      start_index = i*batch_size\n",
        "      end_index = min((i+1)*batch_size, data_len)\n",
        "      yield (x_shuffle[start_index:end_index], y_shuffle[start_index:end_index])\n",
        "      \n",
        "  def predict(self, x):\n",
        "    sess = tf.Session()\n",
        "    \n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    saver = tf.train.Svaer(tf.global_variables())\n",
        "    ckpt = tf.train.get_checkpoint_state('')\n",
        "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "    \n",
        "    feed_dict={self.input_x:x}\n",
        "    logits = sess.run(self.logits, feed_dict=feed_dict)\n",
        "    y_pred = np.argmax(logits, 1)\n",
        "    \n",
        "    return y_pred\n",
        "  #def instantiate_weights(self):\n",
        "  #  with tf.name_scope('Embedding'):\n",
        "  #    self.Embedding = tf.Variable([self.vocab_size, self.embedding_dim], name='Embedding')\n",
        "  #    self.W = tf.get_variable('W', [self.embedding_dim, self.class_size])\n",
        "  #    self.b = tf.get_variable('b', [self.class_size])\n",
        "    \n",
        "  #def inference(self):\n",
        "  #  embedding_chars = tf.nn.embedding_lookup(self.Embedding, self.input_x)\n",
        "  #  self.sentence_embeddings = tf.reduce_mean(embedding_chars, axis =1)\n",
        "  #  logits = tf.matmul(self.sentence_embeddings, self.W)+self.b\n",
        "  #  return logits\n",
        "  \n",
        "  #def loss(self):\n",
        "  #  input_y = tf.reshape(self.input_y, [-1])\n",
        "  #  input_y = tf.expand_dims(input_y, 1)\n",
        "  #  loss = tf.reduce_mean(\n",
        "  #         tf.nn.nce_loss(weights=tf.transpose(self.W),\n",
        "  #                             biases=self.b,\n",
        "  #                             labels=input_y,\n",
        "  #                             inputs=self.sentence_embeddings,\n",
        "  #                             num_sampled=10,\n",
        "  #                             num_classes=self.class_size,\n",
        "  #                             partition_strategy=\"div\"))\n",
        "  #  return loss\n",
        "  \n",
        "  #def train(self):\n",
        "  #      train_op = tf.contrib.layers.optimize_loss(self.loss_val, global_step=self.global_step, learning_rate=self.learning_rate, optimizer=\"Adam\")\n",
        "  #      return train_op\n",
        "    \n",
        "  #def get_accuracy(self):\n",
        "  #  self.predictions = tf.argmax(self.logits, axis =1, name='predictions')\n",
        "  #  correct_preidctions = tf.equal(self.predictions, self.input_y)\n",
        "  #  accruacy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name='accuracy')\n",
        "    \n",
        "  #  return accuracy\n",
        "      \n",
        "      \n",
        "      \n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hR7LKknST_jz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dYTd2mkvsUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NthpWlP3vsRb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYTewLLmTCcA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIy8TlFxlQ6x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3epwpTylLpz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqTeEBRmzxm5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mWmkxDDzxqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXwmm5nQzxs8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DO7gzRpWzxv2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}