{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fasttext.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JiaweiYu1/fasttext/blob/master/fasttext.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLhI5k0zVzXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import codecs\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "import csv\n",
        "import html\n",
        "\n",
        "from tensorflow.contrib import slim\n",
        "\n",
        "import argparse\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "import pprint\n",
        "import easydict\n",
        "import random\n",
        "\n",
        "\n",
        "#tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyUwshl225Cb",
        "colab_type": "code",
        "outputId": "dab6750c-a854-45ca-d771-57aecc69ada7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/Colab_Notebooks/fasttext\")\n",
        "cwd = os.getcwd()\n",
        "print(cwd)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "/content/drive/My Drive/Colab_Notebooks/fasttext\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pjrAY1AzxgU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FastText:\n",
        "  \n",
        "  def __init__(self,\n",
        "               class_size, sentence_length, vocab_size,\n",
        "               embedding_dim, learning_rate, num_sampled, epoch):\n",
        "    self.class_size = class_size\n",
        "    self.sentence_length = sentence_length\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.learning_rate = learning_rate\n",
        "    self.num_sampled = num_sampled\n",
        "    self.epoch = epoch\n",
        "    \n",
        "    self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "    self.input_x = tf.placeholder(tf.int32, [None, self.sentence_length], name= 'input_x')\n",
        "    self.input_y = tf.placeholder(tf.int32, [None], name='input_y')\n",
        "    #self.loss = tf.constant(0.0)\n",
        "    #self.global_step = tf.Variable(0, trainable=False, name=\"Global_Step\")\n",
        "    \n",
        "    self.model()\n",
        "    #self.instantiate_weights()\n",
        "    #self.logits = self.inference()\n",
        "    \n",
        "    #self.loss_val = self.loss()\n",
        "    #self.train_op = self.train()\n",
        "    \n",
        "    #self.accuracy = self.get_accuracy()\n",
        "    #self.predictions = tf.argmax(self.logits, axis=1, name=\"predictions\")  # shape:[None,]\n",
        "    #correct_prediction = tf.equal(tf.cast(self.predictions, tf.int32), self.input_y)\n",
        "    #self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"Accuracy\")\n",
        "    \n",
        "  \n",
        "  def model(self):\n",
        "    \n",
        "    with tf.variable_scope('embedding', reuse=tf.AUTO_REUSE):\n",
        "      self.Embedding = tf.get_variable('embedding',[self.vocab_size, self.embedding_dim])\n",
        "      self.W = tf.get_variable('W', [self.embedding_dim, self.class_size])\n",
        "      self.b = tf.get_variable('b',[self.class_size])\n",
        "      \n",
        "      self.embedding_chars = tf.nn.embedding_lookup(self.Embedding, self.input_x)\n",
        "      \n",
        "      self.sentence_embeddings = tf.reduce_mean(self.embedding_chars, axis=1)\n",
        "      self.logits = tf.matmul(self.sentence_embeddings, self.W) + self.b\n",
        "      \n",
        "      input_y = tf.reshape(self.input_y, [-1])\n",
        "      input_y = tf.expand_dims(input_y, 1)\n",
        "      self.loss = tf.reduce_mean(\n",
        "                 tf.nn.nce_loss(weights=tf.transpose(self.W),\n",
        "                               biases=self.b,\n",
        "                               labels=input_y,\n",
        "                               inputs=self.sentence_embeddings,\n",
        "                               num_sampled=self.num_sampled,\n",
        "                               num_classes=self.class_size,\n",
        "                               partition_strategy=\"div\"))\n",
        "\n",
        "    #with tf.name_scope('opt'):\n",
        "      #optimizer= tf.train.AdamOptimizer(learning_rate)\n",
        "      #update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "      #self.train_op = slim.learning.create_train_op(total_loss=self.loss, optimizer=optimizer,update_ops=update_ops)\n",
        "      self.train_op = tf.contrib.layers.optimize_loss(self.loss, global_step=self.global_step, learning_rate=self.learning_rate, optimizer=\"Adam\")\n",
        "      \n",
        "    #with tf.name_scope('accuracy'):\n",
        "      self.predictions = tf.argmax(self.logits, axis = 1, name='predictions')\n",
        "      correct_prediction = tf.equal(tf.cast(self.predictions, tf.int32), self.input_y)\n",
        "      self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='Accuracy')\n",
        "      \n",
        "  def fit(self, train_x, train_y, batch_size):\n",
        "    if not os.path.exists('saves/fasttext'): os.makedirs('saves/fasttext')\n",
        "    if not os.path.exists('train_logs/fasttext'): os.makedirs('train_logs/fasttext')\n",
        "\n",
        "    train_steps = 0\n",
        "    best_val_acc = 0\n",
        "    \n",
        "    tf.summary.scalar('val_loss', self.loss)\n",
        "    tf.summary.scalar('val_accuracy', self.accuracy)\n",
        "    merged = tf.summary.merge_all()\n",
        "    \n",
        "    sess = tf.Session()\n",
        "    \n",
        "    writer = tf.summary.FileWriter('train_logs/fasttext', sess.graph)\n",
        "    saver = tf.train.Saver(max_to_keep=10)\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    for i in range(self.epoch):\n",
        "      batch_train = self.batch_iter(train_x, train_y, batch_size)\n",
        "      for batch_x, batch_y in batch_train:\n",
        "        train_steps +=1\n",
        "        feed_dict = {self.input_x:batch_x, self.input_y:batch_y}\n",
        "        _, train_loss, train_acc = sess.run([self.train_op, self.loss, self.accuracy], feed_dict = feed_dict)\n",
        "        \n",
        "        if train_steps % 1000 ==0:\n",
        "          #feed_dict = {self.input_x:val_x, self.input_y:val_y}\n",
        "          val_loss, val_acc = sess.run([self.loss, self.acc],feed_dict=feed_dict)\n",
        "        \n",
        "          summary = sess.run(merged, feed_dict=feed_dict)\n",
        "          writer.add_summary(summary, global_step=train_steps)\n",
        "        \n",
        "          if val_acc >= best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            saver.save(sess, \"saves/fasttext\", global_step=train_steps)\n",
        "          msg = 'epoch:%d/%d, train_steps:%d, train_loss:%.4f, trina_acc:%.4f, val_loss:%.4f, val_acc:%.4f'\n",
        "          print(msg%(i, self.epoch, train_steps, train_loss, train_acc, val_loss, val_acc))\n",
        "        \n",
        "  def batch_iter(self, x, y, batch_size=1, shuffle=True):\n",
        "    data_len = len(x)\n",
        "    num_batch = int((data_len-1)/batch_size)+1\n",
        "    \n",
        "    if shuffle:\n",
        "      #shuffle_indices = np.random.permutation(np.arange(data_len))\n",
        "      #x_shuffle = x[shuffle_indices]\n",
        "      #y_shuffle= y[shuffle_indices]\n",
        "      random.shuffle(x)\n",
        "      x_shuffle = x\n",
        "      random.shuffle(y)\n",
        "      y_shuffle = y\n",
        "    else:\n",
        "      x_shuffle=x\n",
        "      y_shuffle = y\n",
        "    for i in range(num_batch):\n",
        "      start_index = i*batch_size\n",
        "      end_index = min((i+1)*batch_size, data_len)\n",
        "      yield (x_shuffle[start_index:end_index], y_shuffle[start_index:end_index])\n",
        "      \n",
        "  def predict(self, x):\n",
        "    sess = tf.Session()\n",
        "    \n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    saver = tf.train.Svaer(tf.global_variables())\n",
        "    ckpt = tf.train.get_checkpoint_state('')\n",
        "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "    \n",
        "    feed_dict={self.input_x:x}\n",
        "    logits = sess.run(self.logits, feed_dict=feed_dict)\n",
        "    y_pred = np.argmax(logits, 1)\n",
        "    \n",
        "    return y_pred\n",
        "  #def instantiate_weights(self):\n",
        "  #  with tf.name_scope('Embedding'):\n",
        "  #    self.Embedding = tf.Variable([self.vocab_size, self.embedding_dim], name='Embedding')\n",
        "  #    self.W = tf.get_variable('W', [self.embedding_dim, self.class_size])\n",
        "  #    self.b = tf.get_variable('b', [self.class_size])\n",
        "    \n",
        "  #def inference(self):\n",
        "  #  embedding_chars = tf.nn.embedding_lookup(self.Embedding, self.input_x)\n",
        "  #  self.sentence_embeddings = tf.reduce_mean(embedding_chars, axis =1)\n",
        "  #  logits = tf.matmul(self.sentence_embeddings, self.W)+self.b\n",
        "  #  return logits\n",
        "  \n",
        "  #def loss(self):\n",
        "  #  input_y = tf.reshape(self.input_y, [-1])\n",
        "  #  input_y = tf.expand_dims(input_y, 1)\n",
        "  #  loss = tf.reduce_mean(\n",
        "  #         tf.nn.nce_loss(weights=tf.transpose(self.W),\n",
        "  #                             biases=self.b,\n",
        "  #                             labels=input_y,\n",
        "  #                             inputs=self.sentence_embeddings,\n",
        "  #                             num_sampled=10,\n",
        "  #                             num_classes=self.class_size,\n",
        "  #                             partition_strategy=\"div\"))\n",
        "  #  return loss\n",
        "  \n",
        "  #def train(self):\n",
        "  #      train_op = tf.contrib.layers.optimize_loss(self.loss_val, global_step=self.global_step, learning_rate=self.learning_rate, optimizer=\"Adam\")\n",
        "  #      return train_op\n",
        "    \n",
        "  #def get_accuracy(self):\n",
        "  #  self.predictions = tf.argmax(self.logits, axis =1, name='predictions')\n",
        "  #  correct_preidctions = tf.equal(self.predictions, self.input_y)\n",
        "  #  accruacy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name='accuracy')\n",
        "    \n",
        "  #  return accuracy\n",
        "      \n",
        "      \n",
        "      \n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVJQOpCARIJ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.0.0/en_core_web_lg-2.0.0.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzvJFcENXiU5",
        "colab_type": "code",
        "outputId": "ae987636-47f3-4445-f3f9-05b9147d16ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        }
      },
      "source": [
        "!python -m spacy download en_core_web_lg\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_lg')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_lg==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.0.0/en_core_web_lg-2.0.0.tar.gz#egg=en_core_web_lg==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_lg -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en_core_web_lg\n",
            "\n",
            "    You can now load the model via spacy.load('en_core_web_lg')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXqi7haeaG_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_ngram(words, n=2):\n",
        "    # TODO add ngrams up to n\n",
        "    return [' '.join(words[i: i+n]) for i in range(len(words)-(n-1))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPu-gXlLHge0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class load_data:\n",
        "  \n",
        "  def __init__(self, config):\n",
        "    self.config = config\n",
        "    self.class_size = config.class_size\n",
        "    self.max_len = config.max_len\n",
        "    self.n_over_max_len = 0\n",
        "    self.real_max_len = 0\n",
        "    \n",
        "    np.random.seed(config.seed)\n",
        "    \n",
        "    self.ngram2idx = dict()\n",
        "    self.idx2ngram = dict()\n",
        "    self.ngram2idx['PAD'] = 0\n",
        "    self.idx2ngram[0] = 'PAD'\n",
        "    self.ngram2idx['UNK'] = 1\n",
        "    self.idx2ngram[1] = 'UNK'\n",
        "    self.html_tag_re = re.compile(r'<[^>]+>')\n",
        "    self.train_data, self.test_data = self.load_csv()\n",
        "    self.train_data, self.valid_data = \\\n",
        "        self.split_tr_va(n_class_examples=config.valid_size_per_class)\n",
        "    self.count_labels()\n",
        "\n",
        "    print('real_max_len', self.real_max_len)\n",
        "    print('n_over_max_len {}/{} ({:.1f}%)'.\n",
        "          format(self.n_over_max_len, len(self.train_data),\n",
        "          100 * self.n_over_max_len / len(self.train_data)))\n",
        "    \n",
        "  def load_csv(self):\n",
        "    train_data = list()\n",
        "    test_data = list()\n",
        "    \n",
        "    #spacy.prefer_gpu()\n",
        "\n",
        "    # https://spacy.io/usage/facts-figures#benchmarks-models-english\n",
        "    # python3 -m spacy download en_core_web_lg --user\n",
        "    nlp = spacy.load('en_core_web_lg', disable=['parser', 'tagger', 'ner'])\n",
        "    nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
        "    \n",
        "    with open(self.config.train_data_path, 'r', newline='',\n",
        "              encoding='utf-8') as f:\n",
        "      reader = csv.reader(f, quotechar='\"')\n",
        "      for idx, features in enumerate(reader):\n",
        "        y = int(features[0]) -1\n",
        "        assert 0 <= y <= self.class_size, y\n",
        "        x, x_len = self.process_example(features[1], features[2], nlp, is_train=True,\n",
        "                                       padding=args.padding)\n",
        "        train_data.append([x, x_len, y])\n",
        "        \n",
        "        if (idx+1) %10000 ==0:\n",
        "          print(idx+1)\n",
        "    \n",
        "    with open(self.config.test_data_path,'r', newline='',\n",
        "             encoding = 'utf-8') as f:\n",
        "      reader = csv.reader(f, quotechar='\"')\n",
        "      for idx, features in enumerate(reader):\n",
        "        y = int(features[0]) -1\n",
        "        assert 0 <= y <= self.class_size, y\n",
        "        x, x_len = self.process_example(features[1], features[2], nlp, is_train=True,\n",
        "                                       padding=args.padding)\n",
        "        test_data.append([x, x_len, y])\n",
        "        \n",
        "    print('dictionary size ', len(self.ngram2idx))\n",
        "    \n",
        "    return train_data, test_data\n",
        "  \n",
        "  def process_example(self, title, description, nlp, is_train=True, padding=0):\n",
        "    title_desc = title + '. ' + description\n",
        "    \n",
        "    if '\\\\' in title_desc:\n",
        "      title_desc = title_desc.replace('\\\\',' ')\n",
        "      \n",
        "    title_desc = html.unescape(title_desc)\n",
        "    \n",
        "    if '<' in title_desc and '>' in title_desc:\n",
        "      title_desc = self.html_tag_re.sub('', title_desc)\n",
        "      \n",
        "    # create bow and bag-of-ngrams\n",
        "    doc = nlp(title_desc)\n",
        "    b_o_w = [token.text for token in doc]\n",
        "    \n",
        "    \n",
        "    # add tags for ngrams\n",
        "    tagged_title_desc = \\\n",
        "    '<p> ' + ' </s>'.join([s.text for s in doc.sents])+\\\n",
        "    ' <p>'\n",
        "    doc = nlp(tagged_title_desc)\n",
        "    n_gram = get_ngram([token.text for token in doc],\n",
        "                       n = self.config.n_gram)\n",
        "    \n",
        "    b_o_ngrams = b_o_w+n_gram\n",
        "    \n",
        "    if padding >0:\n",
        "      if self.max_len < len(b_o_ngrams):\n",
        "        b_o_ngrams = b_o_ngrams[:self.max_len]\n",
        "        \n",
        "    if is_train:\n",
        "      for ng in b_o_ngrams:\n",
        "        idx = self.ngram2idx.get(ng)\n",
        "        if idx is None:\n",
        "          idx = len(self.ngram2idx)\n",
        "          self.ngram2idx[ng] = idx\n",
        "          self.idx2ngram[idx] = ng\n",
        "          \n",
        "    x = [self.ngram2idx[ng] if ng in self.ngram2idx\n",
        "         else self.ngram2idx['UNK'] for ng in b_o_ngrams]\n",
        "    \n",
        "    x_len = len(x)\n",
        "    \n",
        "    if x_len > self.max_len:\n",
        "      self.n_over_max_len +=1\n",
        "      \n",
        "    if x_len > self.real_max_len:\n",
        "      self.real_max_len = x_len\n",
        "      \n",
        "    if padding >0:\n",
        "      while len(x) < self.max_len:\n",
        "        x.append(self.ngram2idx['PAD'])\n",
        "      assert len(x) == self.max_len\n",
        "    return x, x_len\n",
        "  \n",
        "  def count_labels(self):\n",
        "    def count(data):\n",
        "      count_dict = dict()\n",
        "      for d in data:\n",
        "        if d[-1] not in count_dict:\n",
        "          count_dict[d[-1]] = 1\n",
        "        else:\n",
        "          count_dict[d[-1]] +=1\n",
        "          \n",
        "      return count_dict\n",
        "    print('train', count(self.train_data))\n",
        "    print('test', count(self.test_data))\n",
        "    \n",
        "  def split_tr_va(self, n_class_examples=1900):\n",
        "        count = 0\n",
        "        class_item_set_dict = dict()\n",
        "        item_all = list()\n",
        "\n",
        "        print('Splitting..')\n",
        "        while count < n_class_examples * self.config.class_size:\n",
        "            rand_pick = np.random.randint(len(self.train_data))\n",
        "            # print(rand_pick)\n",
        "            label = self.train_data[rand_pick][-1]\n",
        "            if label in class_item_set_dict:\n",
        "                item_set = class_item_set_dict[label]\n",
        "                if len(item_set) < n_class_examples \\\n",
        "                        and rand_pick not in item_set:\n",
        "                    item_set.add(rand_pick)\n",
        "                    item_all.append(rand_pick)\n",
        "                    count += 1\n",
        "            else:\n",
        "                class_item_set_dict[label] = set()\n",
        "                class_item_set_dict[label].add(rand_pick)\n",
        "                item_all.append(rand_pick)\n",
        "                count += 1\n",
        "\n",
        "        train_data2 = list()\n",
        "        valid_data = list()\n",
        "        for idx, td in enumerate(self.train_data):\n",
        "            if idx in item_all:\n",
        "                valid_data.append(td)\n",
        "            else:\n",
        "                train_data2.append(td)\n",
        "\n",
        "        print(len(train_data2), len(valid_data))\n",
        "        return train_data2, valid_data\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onWwNjYsOgKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = easydict.EasyDict({\n",
        "    \"train_data_path\": 'ag_news_csv/train.csv',\n",
        "    \"test_data_path\": 'ag_news_csv/test.csv',\n",
        "    \"pickle_path\": 'ag.pkl',\n",
        "    \"seed\": 2019,\n",
        "    \"class_size\": 4,\n",
        "    \"valid_size_per_class\":1000,\n",
        "    \"n_gram\": 2,\n",
        "    \"padding\":1,\n",
        "    \"max_len\":467\n",
        "})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a7BM9FSQqLl",
        "colab_type": "code",
        "outputId": "a961a4ce-e1d2-4f0b-b9ec-9fc584352aff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "agdata = load_data(args)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000\n",
            "20000\n",
            "30000\n",
            "40000\n",
            "50000\n",
            "60000\n",
            "70000\n",
            "80000\n",
            "90000\n",
            "100000\n",
            "110000\n",
            "120000\n",
            "dictionary size  1439345\n",
            "Splitting..\n",
            "116000 4000\n",
            "train {2: 29000, 3: 29000, 1: 29000, 0: 29000}\n",
            "test {2: 1900, 3: 1900, 1: 1900, 0: 1900}\n",
            "real_max_len 465\n",
            "n_over_max_len 0/116000 (0.0%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRqaPt27dc5j",
        "colab_type": "code",
        "outputId": "adeae237-45d6-4f10-f35a-bbc049a0aa9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "train_data, test_data = agdata.load_csv()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000\n",
            "20000\n",
            "30000\n",
            "40000\n",
            "50000\n",
            "60000\n",
            "70000\n",
            "80000\n",
            "90000\n",
            "100000\n",
            "110000\n",
            "120000\n",
            "dictionary size  1439345\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDdmZWlZehGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x = [e[0] for e in train_data]\n",
        "train_y = [e[2] for e in train_data]\n",
        "train_x_len = [e[1] for e in train_data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0iaCbnwOMbf",
        "colab_type": "code",
        "outputId": "1919051a-e188-47c1-9e47-e34d3aaf7fb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train_x)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "120000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRDt-AcbNx-E",
        "colab_type": "code",
        "outputId": "9551a188-0476-4e34-babc-ac58b3a3c331",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max(train_x_len)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "465"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xyz5UbpqOd8i",
        "colab_type": "code",
        "outputId": "b8851500-c121-40ab-fdd4-ae360174b040",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "class_size=4\n",
        "learning_rate=0.01\n",
        "batch_size=32\n",
        "decay_steps=1000\n",
        "decay_rate=0.9\n",
        "sequence_length=467\n",
        "vocab_size = 1439345\n",
        "embedding_dim = 10\n",
        "is_training=True\n",
        "dropout_keep_prob=1\n",
        "epoch = 5\n",
        "batch_size = 4096\n",
        "fastext=FastText(class_size, sequence_length, vocab_size, embedding_dim, learning_rate, decay_steps, epoch)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5JGSivDBvuG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fastext.fit(train_x, train_y, batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dYTd2mkvsUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NthpWlP3vsRb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYTewLLmTCcA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIy8TlFxlQ6x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3epwpTylLpz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqTeEBRmzxm5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mWmkxDDzxqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXwmm5nQzxs8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DO7gzRpWzxv2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}